% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode}{hyperref}
\PassOptionsToPackage{hyphens}{url}
%
\documentclass[
]{book}
\usepackage{amsmath,amssymb}
\usepackage{lmodern}
\usepackage{iftex}
\ifPDFTeX
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provide euro and other symbols
\else % if luatex or xetex
  \usepackage{unicode-math}
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
\fi
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\makeatletter
\@ifundefined{KOMAClassName}{% if non-KOMA class
  \IfFileExists{parskip.sty}{%
    \usepackage{parskip}
  }{% else
    \setlength{\parindent}{0pt}
    \setlength{\parskip}{6pt plus 2pt minus 1pt}}
}{% if KOMA class
  \KOMAoptions{parskip=half}}
\makeatother
\usepackage{xcolor}
\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\IfFileExists{bookmark.sty}{\usepackage{bookmark}}{\usepackage{hyperref}}
\hypersetup{
  pdftitle={Analyzing single-case data with R and scan},
  pdfauthor={Jürgen Wilbert},
  hidelinks,
  pdfcreator={LaTeX via pandoc}}
\urlstyle{same} % disable monospaced font for URLs
\usepackage[a4paper]{geometry}
\usepackage{color}
\usepackage{fancyvrb}
\newcommand{\VerbBar}{|}
\newcommand{\VERB}{\Verb[commandchars=\\\{\}]}
\DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
% Add ',fontsize=\small' for more characters per line
\usepackage{framed}
\definecolor{shadecolor}{RGB}{248,248,248}
\newenvironment{Shaded}{\begin{snugshade}}{\end{snugshade}}
\newcommand{\AlertTok}[1]{\textcolor[rgb]{0.94,0.16,0.16}{#1}}
\newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.77,0.63,0.00}{#1}}
\newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\BuiltInTok}[1]{#1}
\newcommand{\CharTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\CommentTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textit{#1}}}
\newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
\newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{#1}}
\newcommand{\DecValTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\ErrorTok}[1]{\textcolor[rgb]{0.64,0.00,0.00}{\textbf{#1}}}
\newcommand{\ExtensionTok}[1]{#1}
\newcommand{\FloatTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\ImportTok}[1]{#1}
\newcommand{\InformationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
\newcommand{\NormalTok}[1]{#1}
\newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.81,0.36,0.00}{\textbf{#1}}}
\newcommand{\OtherTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{#1}}
\newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textit{#1}}}
\newcommand{\RegionMarkerTok}[1]{#1}
\newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\StringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\VariableTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\WarningTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\usepackage{longtable,booktabs,array}
\usepackage{calc} % for calculating minipage widths
% Correct order of tables after \paragraph or \subparagraph
\usepackage{etoolbox}
\makeatletter
\patchcmd\longtable{\par}{\if@noskipsec\mbox{}\fi\par}{}{}
\makeatother
% Allow footnotes in longtable head/foot
\IfFileExists{footnotehyper.sty}{\usepackage{footnotehyper}}{\usepackage{footnote}}
\makesavenoteenv{longtable}
\usepackage{graphicx}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
% Set default figure placement to htbp
\makeatletter
\def\fps@figure{htbp}
\makeatother
\setlength{\emergencystretch}{3em} % prevent overfull lines
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
\setcounter{secnumdepth}{5}
\usepackage{booktabs}
\usepackage{amsthm}
\makeatletter
\def\thm@space@setup{%
  \thm@preskip=8pt plus 2pt minus 4pt
  \thm@postskip=\thm@preskip
}
\makeatother


\makeatletter
\newenvironment{kframe}{%
\medskip{}
\setlength{\fboxsep}{.8em}
 \def\at@end@of@kframe{}%
 \ifinner\ifhmode%
  \def\at@end@of@kframe{\end{minipage}}%
  \begin{minipage}{\columnwidth}%
 \fi\fi%
 \def\FrameCommand##1{\hskip\@totalleftmargin \hskip-\fboxsep
 \colorbox{shadecolor}{##1}\hskip-\fboxsep
     % There is no \\@totalrightmargin, so:
     \hskip-\linewidth \hskip-\@totalleftmargin \hskip\columnwidth}%
 \MakeFramed {\advance\hsize-\width
   \@totalleftmargin\z@ \linewidth\hsize
   \@setminipage}}%
 {\par\unskip\endMakeFramed%
 \at@end@of@kframe}
\makeatother

\newenvironment{rmdblock}[1]
  {
  \begin{itemize}
  \renewcommand{\labelitemi}{
    \raisebox{-.7\height}[0pt][0pt]{
      {\setkeys{Gin}{width=3em,keepaspectratio}\includegraphics{images/#1}}
    }
  }
  \setlength{\fboxsep}{1em}
  \begin{kframe}
  \item
  }
  {
  \end{kframe}
  \end{itemize}
  }
  
\newenvironment{rmdnote}
  {\begin{rmdblock}{bulp}}
{\end{rmdblock}}
\usepackage{booktabs}
\usepackage{longtable}
\usepackage{array}
\usepackage{multirow}
\usepackage{wrapfig}
\usepackage{float}
\usepackage{colortbl}
\usepackage{pdflscape}
\usepackage{tabu}
\usepackage{threeparttable}
\usepackage{threeparttablex}
\usepackage[normalem]{ulem}
\usepackage{makecell}
\usepackage{xcolor}
\ifLuaTeX
  \usepackage{selnolig}  % disable illegal ligatures
\fi
\usepackage[]{natbib}
\bibliographystyle{apalike}

\title{Analyzing single-case data with R and scan}
\author{Jürgen Wilbert}
\date{2022-05-18}

\begin{document}
\maketitle

{
\setcounter{tocdepth}{1}
\tableofcontents
}
\hypertarget{welcome}{%
\chapter*{Welcome}\label{welcome}}
\addcontentsline{toc}{chapter}{Welcome}

\includegraphics[width=4.16667in,height=\textheight]{images/cover.png}

Note: The cover has been designed by Tony Wilbert and Henry Ritter.\\
Thanx for that!

\hypertarget{preface}{%
\chapter*{Preface}\label{preface}}
\addcontentsline{toc}{chapter}{Preface}

Hello!

I am glad your found your way to this book as is tells me you are beginning to use the scan package. While \texttt{scan} is quiet thoroughly developed, this book is at an early stage (about 30\% is done). I am continuously working on it and extending it. At this point in time there is no release of this book available. Only this draft which is full of errors (code and typos).\\
If you have any suggestions how to enhance the book or would like to report errors, comments, feedback etc. you can do so by posting an issue to the gitHub repository of this book. You can find the repository at \url{https://github.com/jazznbass/scan-Book}.

Thank you!

Jürgen

18 May 2022

\hypertarget{software-reference}{%
\section*{Software reference}\label{software-reference}}
\addcontentsline{toc}{section}{Software reference}

This book has been created using the \texttt{Rmarkdown} \citep{R-rmarkdown} and \texttt{bookdown} \citep{R-bookdown} packages within the RStudio \citep{RStudio} environment. The analyses have been conducted with the \textbf{R} package \texttt{scan} at version 0.54.3 \citep{R-scan}. R version 4.2.0 (2022-04-22) was used \citep{R-base}.

\hypertarget{introduction}{%
\chapter{Introduction}\label{introduction}}

Single case research has become an important and broadly accepted method for gaining insight into educational processes. Especially the field of special education has adopted single-case research as a proper method for evaluating the effectiveness of an intervention or the developmental processes underlying problems in acquiring academic skills. Single-case studies are also popular among teachers and educators who are interested in evaluating the learning progress of their students. The resulting information of a single-case research design provide helpful information for pedagogical decision processes regarding further teaching processes of an individual student but also help to decide, whether or how to implement certain teaching methods into a classroom.\\
Despite its usefulness, standards on how to conduct single-case studies, how to analyze the data, and how to present the results is less well developed compared to group based research designs. Moreover, while there is ample software helping to analyse data, most of the software is designed towards analyzing group based data sets. Visualizing single-case data sets oftentimes means to tinker with spreadsheet programs and analyzing becomes a cumbersome endeavor. This book addresses this gap. It has been written around a specialized software tool for managing, visualizing, and analyzing single-case data. This tool is an extension package for the software \texttt{R} \citep{R-base} named \texttt{scan}, an acronym for \textbf{single-case analyses}.

\hypertarget{a-teaser}{%
\section{A teaser}\label{a-teaser}}

Before I go into the details on how \texttt{scan} exactly works, I like to provide an example of what you can do with \texttt{scan}. It is meant to be a teaser to get you motivated to tackle the steep learning curve associated with the use of \texttt{R} (but there is a land of milk and honey behind this curve!). So, do not mind if you do not understand every detail of this example, it will all be explained and obvious to you once you get familiar with \texttt{scan}.

Let us set a fictional context. Let us assume you are researching on a method to foster the calculation abilities of struggling fourth grade students. You developed an intervention program named \emph{KUNO}. In a pilot study you like to get some evidence on the effectiveness of that new method and you set up a multi-baseline single-case study comprising three students that take part in the \emph{KUNO} program across a period of ten weeks. Throughout that course you regularly measured the calculation abilities of each student 20 times with a reliable test. You also implemented a follow up after eight weeks with additional five measures. The calculation test gives you the number of correctly solved calculation tasks within ten minutes.\\
Now, I invent some data for this fictitious \emph{KUNO} study as it would be to laborious to conduct a real study and actually to evolve a real intervention method.\\
We use the \texttt{scan} package to code the data. Each case consists of 25 measurements. We have three phases: pre intervention (A), during the intervention (B), and follow-up (C). Phases A and B have different lengths. The cases are named and combined into a single object called \texttt{strange\_study}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{case1 }\OtherTok{\textless{}{-}} \FunctionTok{scdf}\NormalTok{(}
  \FunctionTok{c}\NormalTok{(}\AttributeTok{A =} \DecValTok{3}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{4}\NormalTok{, }\DecValTok{6}\NormalTok{, }\DecValTok{4}\NormalTok{, }\DecValTok{3}\NormalTok{, }
    \AttributeTok{B =} \DecValTok{6}\NormalTok{, }\DecValTok{5}\NormalTok{, }\DecValTok{4}\NormalTok{, }\DecValTok{6}\NormalTok{, }\DecValTok{7}\NormalTok{, }\DecValTok{5}\NormalTok{, }\DecValTok{6}\NormalTok{, }\DecValTok{8}\NormalTok{, }\DecValTok{6}\NormalTok{, }\DecValTok{7}\NormalTok{, }\DecValTok{8}\NormalTok{, }\DecValTok{9}\NormalTok{, }\DecValTok{7}\NormalTok{, }\DecValTok{8}\NormalTok{, }
    \AttributeTok{C =} \DecValTok{6}\NormalTok{, }\DecValTok{6}\NormalTok{, }\DecValTok{8}\NormalTok{, }\DecValTok{5}\NormalTok{, }\DecValTok{7}\NormalTok{), }
  \AttributeTok{name =} \StringTok{"Dustin"}
\NormalTok{)}
\NormalTok{case2 }\OtherTok{\textless{}{-}} \FunctionTok{scdf}\NormalTok{(}
  \FunctionTok{c}\NormalTok{(}\AttributeTok{A =} \DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{3}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{4}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{1}\NormalTok{, }
    \AttributeTok{B =} \DecValTok{2}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{4}\NormalTok{, }\DecValTok{3}\NormalTok{, }\DecValTok{5}\NormalTok{, }\DecValTok{5}\NormalTok{, }\DecValTok{7}\NormalTok{, }\DecValTok{6}\NormalTok{, }\DecValTok{3}\NormalTok{, }\DecValTok{8}\NormalTok{, }\DecValTok{6}\NormalTok{, }\DecValTok{4}\NormalTok{, }\DecValTok{7}\NormalTok{, }
    \AttributeTok{C =} \DecValTok{6}\NormalTok{, }\DecValTok{5}\NormalTok{, }\DecValTok{6}\NormalTok{, }\DecValTok{8}\NormalTok{, }\DecValTok{6}\NormalTok{), }
  \AttributeTok{name =} \StringTok{"Mike"}
\NormalTok{)}
\NormalTok{case3 }\OtherTok{\textless{}{-}} \FunctionTok{scdf}\NormalTok{(}
  \FunctionTok{c}\NormalTok{(}\AttributeTok{A =} \DecValTok{7}\NormalTok{, }\DecValTok{5}\NormalTok{, }\DecValTok{6}\NormalTok{, }\DecValTok{4}\NormalTok{, }\DecValTok{4}\NormalTok{, }\DecValTok{7}\NormalTok{, }\DecValTok{5}\NormalTok{, }\DecValTok{7}\NormalTok{, }\DecValTok{4}\NormalTok{,}
    \AttributeTok{B =} \DecValTok{8}\NormalTok{, }\DecValTok{9}\NormalTok{, }\DecValTok{11}\NormalTok{, }\DecValTok{13}\NormalTok{, }\DecValTok{12}\NormalTok{, }\DecValTok{15}\NormalTok{, }\DecValTok{16}\NormalTok{, }\DecValTok{13}\NormalTok{, }\DecValTok{17}\NormalTok{, }\DecValTok{16}\NormalTok{, }\DecValTok{18}\NormalTok{,}
    \AttributeTok{C =} \DecValTok{17}\NormalTok{, }\DecValTok{20}\NormalTok{, }\DecValTok{22}\NormalTok{, }\DecValTok{18}\NormalTok{, }\DecValTok{20}\NormalTok{), }
  \AttributeTok{name =} \StringTok{"Will"}
\NormalTok{)}
\NormalTok{strange\_study }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(case1, case2, case3)}
\end{Highlighting}
\end{Shaded}

Now we visualize the cases:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{plot}\NormalTok{(}
\NormalTok{  strange\_study,}
  \AttributeTok{ylab =} \StringTok{"Correct"}\NormalTok{,}
  \AttributeTok{xlab =} \StringTok{"Days"}\NormalTok{,}
  \AttributeTok{lines =} \FunctionTok{c}\NormalTok{(}\StringTok{"loreg"}\NormalTok{, }\AttributeTok{col =} \StringTok{"red"}\NormalTok{),}
  \AttributeTok{phase.names =} \FunctionTok{c}\NormalTok{(}\StringTok{"Baseline"}\NormalTok{, }\StringTok{"Intervention"}\NormalTok{, }\StringTok{"Follow{-}up"}\NormalTok{),}
  \AttributeTok{style =} \StringTok{"chart"}\NormalTok{,}
  \AttributeTok{ylim =} \FunctionTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{30}\NormalTok{),}
  \AttributeTok{xinc =} \DecValTok{2}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{scan-book_files/figure-latex/plot-strange-study-1.pdf}

Now we need some descriptive statistics:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{describe}\NormalTok{(strange\_study)}
\end{Highlighting}
\end{Shaded}

\begin{table}[!h]

\caption{\label{tab:describe-strange-study}Descriptive statistics}
\begin{threeparttable}
\begin{tabular}[t]{lccc}
\toprule
Parameter & Dustin & Mike & Will\\
\midrule
Design & A-B-C & A-B-C & A-B-C\\
n A & 6 & 7 & 9\\
n B & 14 & 13 & 11\\
n C & 5 & 5 & 5\\
Missing A & 0 & 0 & 0\\
Missing B & 0 & 0 & 0\\
Missing C & 0 & 0 & 0\\
m A & 3.67 & 1.71 & 5.44\\
m B & 6.57 & 4.69 & 13.45\\
m C & 6.4 & 6.2 & 19.4\\
md A & 3.5 & 1.0 & 5.0\\
md B & 6.5 & 5.0 & 13.0\\
md C & 6 & 6 & 20\\
sd A & 1.37 & 1.38 & 1.33\\
sd B & 1.40 & 2.10 & 3.27\\
sd C & 1.14 & 1.10 & 1.95\\
mad A & 0.74 & 1.48 & 1.48\\
mad B & 1.48 & 2.97 & 4.45\\
mad C & 1.48 & 0.00 & 2.97\\
Min A & 2 & 0 & 4\\
Min B & 4 & 1 & 8\\
Min C & 5 & 5 & 17\\
Max A & 6 & 4 & 7\\
Max B & 9 & 8 & 18\\
Max C & 8 & 8 & 22\\
Trend A & 0.23 & 0.21 & -0.08\\
Trend B & 0.25 & 0.36 & 0.91\\
Trend C & 0.1 & 0.3 & 0.4\\
\bottomrule
\end{tabular}
\begin{tablenotes}
\item \textit{Note: } 
\item n = Number of measurements; Missing = Number of missing values; M = Mean; Median = Median; SD = Standard deviation; MAD = Median average deviation; Min = Minimum; Max = Maximum; Trend = Slope of dependent variable regressed on measurement-time.
\end{tablenotes}
\end{threeparttable}
\end{table}

Single-case data are oftentimes analyzed with overlap indices. Let us get an overview comparing phases A and B:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{overlap}\NormalTok{(strange\_study)}
\end{Highlighting}
\end{Shaded}

\begin{table}[!h]

\caption{\label{tab:overlapAB-strange-study}Overlap indices. Comparing phase 1 against phase 2}
\begin{threeparttable}
\begin{tabular}[t]{llll}
\toprule
  & Dustin & Mike & Will\\
\midrule
Design & A-B-C & A-B-C & A-B-C\\
PND & 50.00 & 53.85 & 100.00\\
PEM & 100.00 & 92.31 & 100.00\\
PET & 71.43 & 61.54 & 100.00\\
NAP & 92.86 & 87.91 & 100.00\\
NAP-R & 85.71 & 75.82 & 100.00\\
PAND & 90 & 80 & 100\\
Tau-U & 0.66 & 0.56 & 0.80\\
Base Tau & 0.60 & 0.55 & 0.74\\
Delta M & 2.90 & 2.98 & 8.01\\
Delta Trend & 0.02 & 0.14 & 0.99\\
SMD & 2.13 & 2.16 & 6.01\\
Hedges g & 2.00 & 1.51 & 2.96\\
\bottomrule
\end{tabular}
\begin{tablenotes}
\item \textit{Note: } 
\item PND = Percentage Non-Overlapping Data; PEM = Percentage Exceeding the Median; PET = Percentage Exceeding the Trend; NAP = Nonoverlap of all pairs; NAP-R = NAP rescaled; PAND = Percentage all nonoverlapping data;Tau U = Parker's Tau-U; Base Tau = Baseline corrected Tau; Delta M = Mean difference between phases; Delta Trend = Trend difference between phases; SMD = Standardized Mean Difference; Hedges g = Corrected SMD.
\end{tablenotes}
\end{threeparttable}
\end{table}

How do the changes hold up against the follow-up? Let us compare phases A and C:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{overlap}\NormalTok{(strange\_study, }\AttributeTok{phases =} \FunctionTok{c}\NormalTok{(}\StringTok{"A"}\NormalTok{, }\StringTok{"C"}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{table}[!h]

\caption{\label{tab:overlapAC-strange-study}Overlap indices. Comparing phase A against phase C}
\begin{threeparttable}
\begin{tabular}[t]{llll}
\toprule
  & Dustin & Mike & Will\\
\midrule
Design & A-B-C & A-B-C & A-B-C\\
PND & 40 & 100 & 100\\
PEM & 100 & 100 & 100\\
PET & 0 & 60 & 100\\
NAP & 93.33 & 100.00 & 100.00\\
NAP-R & 86.67 & 100.00 & 100.00\\
PAND & 81.82 & 100.00 & 100.00\\
Tau-U & 0.46 & 0.51 & 0.61\\
Base Tau & 0.67 & 0.76 & 0.74\\
Delta M & 2.73 & 4.49 & 13.96\\
Delta Trend & -0.13 & 0.09 & 0.48\\
SMD & 2.00 & 3.25 & 10.47\\
Hedges g & 1.97 & 3.25 & 8.34\\
\bottomrule
\end{tabular}
\begin{tablenotes}
\item \textit{Note: } 
\item PND = Percentage Non-Overlapping Data; PEM = Percentage Exceeding the Median; PET = Percentage Exceeding the Trend; NAP = Nonoverlap of all pairs; NAP-R = NAP rescaled; PAND = Percentage all nonoverlapping data;Tau U = Parker's Tau-U; Base Tau = Baseline corrected Tau; Delta M = Mean difference between phases; Delta Trend = Trend difference between phases; SMD = Standardized Mean Difference; Hedges g = Corrected SMD.
\end{tablenotes}
\end{threeparttable}
\end{table}

Finally, we conduct regression analyses for each cases with a piecewise regression model:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{plm}\NormalTok{(strange\_study}\SpecialCharTok{$}\NormalTok{Dustin)}
\FunctionTok{plm}\NormalTok{(strange\_study}\SpecialCharTok{$}\NormalTok{Mike)}
\FunctionTok{plm}\NormalTok{(strange\_study}\SpecialCharTok{$}\NormalTok{Will)}
\end{Highlighting}
\end{Shaded}

\begin{table}[!h]

\caption{\label{tab:plm-strange-study}Piecewise-regression model predicting variable 'values'}
\begin{threeparttable}
\begin{tabular}[t]{lrrrrrrr}
\toprule
\multicolumn{2}{c}{ } & \multicolumn{2}{c}{CI(95\%)} & \multicolumn{4}{c}{ } \\
\cmidrule(l{3pt}r{3pt}){3-4}
Parameter & B & 2.5\% & 97.5\% & SE & t & p & Delta R²\\
\midrule
Intercept & 2.87 & 0.77 & 4.97 & 1.07 & 2.68 & <.05 & \\
Trend mt & 0.23 & -0.31 & 0.77 & 0.28 & 0.83 & .41 & .01\\
Level phase B & 0.49 & -1.58 & 2.56 & 1.06 & 0.46 & .64 & .00\\
Level phase C & -1.34 & -10.59 & 7.91 & 4.72 & -0.28 & .77 & .00\\
Slope phase B & 0.02 & -0.54 & 0.58 & 0.29 & 0.06 & .95 & .00\\
Slope phase C & -0.13 & -1.02 & 0.77 & 0.46 & -0.28 & .78 & .00\\
\bottomrule
\end{tabular}
\begin{tablenotes}
\item \textit{Note: } 
\item F(5, 19) = 7.88; p <.001; R² = 0.675; Adjusted R² = 0.589
\end{tablenotes}
\end{threeparttable}
\end{table}

\begin{table}[!h]

\caption{\label{tab:plm-strange-study}Piecewise-regression model predicting variable 'values'}
\begin{threeparttable}
\begin{tabular}[t]{lrrrrrrr}
\toprule
\multicolumn{2}{c}{ } & \multicolumn{2}{c}{CI(95\%)} & \multicolumn{4}{c}{ } \\
\cmidrule(l{3pt}r{3pt}){3-4}
Parameter & B & 2.5\% & 97.5\% & SE & t & p & Delta R²\\
\midrule
Intercept & 0.86 & -1.65 & 3.37 & 1.28 & 0.67 & .51 & \\
Trend mt & 0.21 & -0.35 & 0.78 & 0.29 & 0.75 & .46 & .01\\
Level phase B & -0.16 & -2.84 & 2.51 & 1.36 & -0.12 & .90 & .00\\
Level phase C & 0.16 & -9.41 & 9.73 & 4.88 & 0.03 & .97 & .00\\
Slope phase B & 0.14 & -0.46 & 0.75 & 0.31 & 0.46 & .64 & .00\\
Slope phase C & 0.09 & -1.01 & 1.18 & 0.56 & 0.15 & .87 & .00\\
\bottomrule
\end{tabular}
\begin{tablenotes}
\item \textit{Note: } 
\item F(5, 19) = 8.00; p <.001; R² = 0.678; Adjusted R² = 0.593
\end{tablenotes}
\end{threeparttable}
\end{table}

\begin{table}[!h]

\caption{\label{tab:plm-strange-study}Piecewise-regression model predicting variable 'values'}
\begin{threeparttable}
\begin{tabular}[t]{lrrrrrrr}
\toprule
\multicolumn{2}{c}{ } & \multicolumn{2}{c}{CI(95\%)} & \multicolumn{4}{c}{ } \\
\cmidrule(l{3pt}r{3pt}){3-4}
Parameter & B & 2.5\% & 97.5\% & SE & t & p & Delta R²\\
\midrule
Intercept & 5.86 & 3.71 & 8.01 & 1.10 & 5.35 & <.001 & \\
Trend mt & -0.08 & -0.46 & 0.30 & 0.19 & -0.43 & .67 & .00\\
Level phase B & 2.89 & 0.25 & 5.53 & 1.35 & 2.15 & <.05 & .01\\
Level phase C & 14.01 & 7.42 & 20.59 & 3.36 & 4.17 & <.001 & .05\\
Slope phase B & 0.99 & 0.52 & 1.47 & 0.24 & 4.10 & <.001 & .05\\
Slope phase C & 0.48 & -0.53 & 1.49 & 0.52 & 0.94 & .35 & .00\\
\bottomrule
\end{tabular}
\begin{tablenotes}
\item \textit{Note: } 
\item F(5, 19) = 68.16; p <.001; R² = 0.947; Adjusted R² = 0.933
\end{tablenotes}
\end{threeparttable}
\end{table}

\hypertarget{some-things-about-r}{%
\chapter{Some things about R}\label{some-things-about-r}}

In this chapter you will get a brief introduction to R. If you are familiar with R you might like to go directly to the next chapter.

\hfill\break
R is a programming language optimized for statistical purposes. It was created in 1992 by Ross Ihaka and Robert Gentleman at the University of Auckland. Since then it has been developed continuously and became one of the leading statistical software programs. R is unmatched in its versatility. It is used for teaching introductory courses into statistics up to doing the most sophisticated mathematical analysis. It has become the defacto standard in many scientific disciplines from the natural to the social sciences.\\
R is completely community driven . That is, it is developed and extended by anybody who likes to participate . It comes at no costs and can be downloaded for free for all major and many minor platforms at \href{http://www.r-project.org}{www.r-project.org}. Yet, it is as reliable as other proprietary software like Mplus, STATA, SPSS etc . You can tell from my writing that is hard not to become an R-fan when you are into statistics :-)\\
R can be used in at least two ways:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  You can use it for applying data analyses. In that way it functions like most other statistical programs. You have to learn the specific syntax of R and it will compute the data analysis you need. For example \texttt{mean(x)} will return the mean of the variable \texttt{x}; \texttt{lm(y\ \textasciitilde{}\ x)} will calculate a linear regression with the criteria \texttt{y} and the predictor \texttt{x} for you or \texttt{plot(x,\ y)} will return a scatter-plot of the variables \texttt{x} and \texttt{y}.
\item
  You can use R to program new statistical procedures, or extend previous ones.
\end{enumerate}

It is the second function that is the origin of R's huge success and versatility. New statistical procedures and functions can be published to be used for everyone in so called packages. A package usually contains several functions, help files and example data-sets. Hundreds of such packages are available to help in all kinds of specialized analyses. The basic installation of R comes with a large variety of packages per installed. New packages can most of the times be easily installed from within R. Admittedly, if you must have the latest developmental version of a new package installation sometimes can get a bit more complex. But with a bit of help and persistence it is not to difficult to accomplish.

The book at hand describes the use of such an additional package named \emph{scan} providing specialized functions for single-case analyses. \emph{scan} comes in two versions: A ``stable'' version and a developmental version. Both versions can be installed directly from within R. The stable version is much older and only provides a limited functionality. Therefore, I will refer to the developmental version in this book.

\hypertarget{basic-r}{%
\section{Basic R}\label{basic-r}}

\emph{R} is a script language. That is, you type in text and let R execute the commands you wrote down. Either you work in a \emph{console} or a \emph{textfile}. In a \emph{console} the command will be executed every time you press the RETURN-key. In a \emph{textfile} you type down your code, mark the part you like to be executed, and run that code (with a click or a certain key). The latter text files can be saved and reused for later R sessions. Therefore, usually you will work in a text file.

A value is assigned to a variable with the \texttt{\textless{}-} operator. Which should be read as an arrow rather than a less sign and a minus sign. A \texttt{\#} is followed by a comment to make your code more understandable. So, what follows a \texttt{\#} is not interpreted by R. A vector is a chain of several values. With a vector you could describe the values of a measurement series. The \texttt{c} function is used to build a vector (e.g., \texttt{c(1,\ 2,\ 3,\ 4)}). If you like to see the content of a variable you could use the \texttt{print} function. \texttt{print(x)} will display the content of the variable \texttt{x}. A shortcut for this is just to type variable name (and press return) \texttt{x}.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# x is assigned the value 10:}
\NormalTok{x }\OtherTok{\textless{}{-}} \DecValTok{10}

\CommentTok{\# See what\textquotesingle{}s inside of x:}
\NormalTok{x}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 10
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# x is assigned a vector with three values:}
\NormalTok{x }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\DecValTok{10}\NormalTok{, }\DecValTok{11}\NormalTok{, }\DecValTok{15}\NormalTok{)}

\CommentTok{\# ... and display the content of x:}
\NormalTok{x}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 10 11 15
\end{verbatim}

Two important concepts in \textbf{R} are \emph{functions} and \emph{arguments}. A \emph{function} is the name for a procedure that does something with the \emph{arguments} that are provided by you. For example, the function \texttt{mean} calculated the mean. \texttt{mean} has an argument \texttt{x} which ``expects'' that you provide a vector (a series of values) from which it will calculate the mean. \texttt{mean(\ x\ =\ c(1,\ 3,\ 5)\ )} will compute the mean of the values 1, 3, and 5 and return the result 3. Some \emph{functions} can take several arguments. \texttt{mean} for example also takes the argument \texttt{trim}. For calculating a trimmed mean. \texttt{mean(\ x\ =\ c(1,\ 1,\ 3,\ 3,\ 5,\ 6,\ 7,\ 8,\ 9,\ 9),\ trim\ =\ 0.1)} will calculate the 10\% trimmed mean of the provided values. The name of the first argument could be dropped. That is, \texttt{mean(\ c(1,\ 3,\ 5)\ )} will be interpreted by \textbf{R} as \texttt{mean(\ x\ =\ c(1,\ 3,\ 5)\ )}. You could also provide a variable to an argument.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{values }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{4}\NormalTok{, }\DecValTok{5}\NormalTok{, }\DecValTok{6}\NormalTok{, }\DecValTok{3}\NormalTok{, }\DecValTok{7}\NormalTok{, }\DecValTok{7}\NormalTok{, }\DecValTok{5}\NormalTok{)}
\FunctionTok{mean}\NormalTok{(}\AttributeTok{x =}\NormalTok{ values)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 4.75
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# or shorter:}
\FunctionTok{mean}\NormalTok{(values)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 4.75
\end{verbatim}

The return value of a function can be assigned to a new variable instead:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{y }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{4}\NormalTok{, }\DecValTok{5}\NormalTok{, }\DecValTok{6}\NormalTok{, }\DecValTok{3}\NormalTok{, }\DecValTok{7}\NormalTok{, }\DecValTok{7}\NormalTok{, }\DecValTok{5}\NormalTok{)}
\NormalTok{res }\OtherTok{\textless{}{-}} \FunctionTok{mean}\NormalTok{(y)}
\CommentTok{\#now res contains the mean of y:}
\NormalTok{res}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 4.75
\end{verbatim}

Every function in R has a help page written by the programmers. You can retrieve these pages with the \texttt{help} function or the short cut \texttt{?}. \texttt{help("mean")} will display the help page for the \texttt{mean} function. The quotation marks are necessary here because you do not provide a variable with the name \emph{mean} but a word `mean'. The shortcut works \texttt{?mean}. A bit confusingly, you do not need the quotation marks here.

\hypertarget{the-scan-package}{%
\chapter{The scan package}\label{the-scan-package}}

\hypertarget{installing-the-scan-package}{%
\section{\texorpdfstring{Installing the \emph{scan} package}{Installing the scan package}}\label{installing-the-scan-package}}

You can use the \texttt{install.packages} function to install \emph{scan}.

\texttt{install.packages("scan")} will install the stable version.

The current stable release is version \texttt{0.54.1}. Please look at Section \emph{Software reference} for which version of \emph{scan} has been used for creating this book and make sure you have this version or a newer one installed.

R contains many packages and it would significantly slow down if all packages would be loaded into the computer memory at the beginning of each R session. Therefore, after installing \emph{scan} it needs to be activated at the beginning of each session you use R. Usually a session starts when you start the R program and ends with closing R.

For activating a package you need the \texttt{library} function. In this case \texttt{library(scan)}. You should get something like

scan 0.54.1 (2022-04-03)\\
Single-Case Data Analysis for Single and Multiple Baseline Designs

indicating that everything went smoothly and \emph{scan} is ready for the job.

\hypertarget{development-version-of-scan}{%
\section{Development version of scan}\label{development-version-of-scan}}

Alternatively, you can compile the development version of \emph{scan} yourself. This might be necessary if the stable version has some bugs or missing functions which has been fixed.

You may need some computer expertise to get the development version running. It is hosted on gitHub at \href{https://github.com/jazznbass/scan}{\textless https://github.com/jazznbass/scan\textgreater{}}.

For installation, you can apply the \texttt{install\_github} function from the \texttt{devtools} package (make sure you have installed the \texttt{devtools} package before):

\texttt{devtools::install\_github("jazznbass/scan",\ dependencies\ =\ TRUE)}

When you are running a Windows operating system you will probably have to install Rtools before. Rtools contains additional programs (e.g.~compilers) that are needed to compile R source packages.

You can find Rtools here: \href{https://cran.r-project.org/bin/windows/Rtools/}{\textless https://cran.r-project.org/bin/windows/Rtools/\textgreater{}}

\hypertarget{reporting-issues-with-scan-and-suggesting-enhancements}{%
\section{\texorpdfstring{Reporting issues with \emph{scan} and suggesting enhancements}{Reporting issues with scan and suggesting enhancements}}\label{reporting-issues-with-scan-and-suggesting-enhancements}}

The \emph{scan} gitHub repository at \href{https://github.com/jazznbass/scan}{\textless https://github.com/jazznbass/scan\textgreater{}} is the ideal place to report bugs, problems, or ideas for enhancing \emph{scan}. Please use the issue tool (direct link: \href{https://github.com/jazznbass/scan/issues}{\textless https://github.com/jazznbass/scan/issues\textgreater{}}).

We are very thankful for any feedback, corrections, or whatever helps to improve \emph{scan}!

\hypertarget{functions-overview}{%
\section{Functions overview}\label{functions-overview}}

The functions of the \emph{scan} package can be divided into the following categories:\\
\emph{Manage data}, \emph{analyze}, \emph{manipulate}, \emph{simulate}, and \emph{depict}.

Table \ref{tab:table-functions} gives an overview of all functions. Furthermore, you can see the current life cycle stage of a function. The life cycle stage categorization is based on the tidyverse package and described in detail here \url{https://lifecycle.r-lib.org/articles/stages.html}.

\begin{table}

\caption{\label{tab:table-functions}Functions in scan.}
\begin{tabular}[t]{>{\raggedright\arraybackslash}p{15em}>{\raggedright\arraybackslash}p{30em}ll}
\toprule
Function & What it does ... & Category & Lifecycle stage\\
\midrule
\textbf{scdf} & Creates a single-case data-frame & Manage data & Stable\\
\textbf{select\_cases} & Selects specific cases of an scdf & Manage data & Stable\\
\textbf{select\_phases} & Selects and/or recombines phases & Manage data & Stable\\
\textbf{subset} & Selects specific measurements or variables of an scdf & Manage data & Stable\\
\textbf{read\_scdf} & Loads external data into an scdf & Manage data & Stable\\
\textbf{write\_scdf} & Writes scdf into an external file & Manage data & Stable\\
\textbf{convert} & Converts an scdf object into R syntax & Manage data & Stable\\
\textbf{set\_var} & (Re)sets dependent, measurement, and phase variable of an scdf & Manage data & Stable\\
\textbf{scdf\_attr} & Gets and sets attributes of an scdf & Manage data & Stable\\
\textbf{add\_l2} & Adds level-two data to an scdf & Manage data & Stable\\
\textbf{as.data.frame/as.data.frame.scdf} & Transforms an scdf into a data frame & Manage data & Stable\\
\textbf{} &  &  \vphantom{3} & \\
\textbf{plot/plot.scdf} & Creates plots of single cases & Depict & Superseded\\
\textbf{style\_plot} & Defines single-case plot graphical styles & Depict & Superseded\\
\textbf{export} & Creates html or latex tables from the output of various can functions & Depict & Experimental\\
\textbf{print/print.sc} & Prints the results of various scan outputs & Depict & Stable\\
\textbf{print/print.scdf} & Prints an scdf & Depict & Stable\\
\textbf{summary/summary.scdf} & Summaizes an scdf & Depict & Stable\\
\textbf{plot\_rand} & Create a distribution plot from a randomization test obejct & Depict & Experimental\\
\textbf{} &  &  \vphantom{2} & \\
\textbf{autocorr} & Autocorrelations for each phase of each case & Analyze & Stable\\
\textbf{corrected\_tau} & Baseline corrected tau & Analyze & Stable\\
\textbf{describe} & Descriptive statistics for each phase of each case & Analyze & Stable\\
\textbf{overlap} & An overview of overlap indeces for each case & Analyze & Stable\\
\textbf{smd} & Various standardized mean differences between phase A and B & Analyze & Stable\\
\textbf{rci} & Reliable change index & Analyze & Experimental\\
\textbf{rand\_test} & Randomization test & Analyze & Stable\\
\textbf{tau\_u} & Tau-U for each case and all cases & Analyze & Stable\\
\textbf{trend} & Trend analyses for each case & Analyze & Stable\\
\textbf{plm} & Piecewise linear regression model & Analyze & Stable\\
\textbf{mplm} & Multivariate piecewise linear regression model & Analyze & Experimental\\
\textbf{hplm} & Hierarchical piecewise linear regression model & Analyze & Stable\\
\textbf{nap} & Non-overlap of all pairs for each case & Analyze & Stable\\
\textbf{pnd} & Percentage of non overlapping data for each case & Analyze & Stable\\
\textbf{pand} & Percentage of all non overlapping data for all cases & Analyze & Stable\\
\textbf{pem} & Percantage exceeding the mean for each case & Analyze & Stable\\
\textbf{pet} & Percentage exceeding the trend for each case & Analyze & Stable\\
\textbf{cdc} & Conservative dual-criterion test & Analyze & Stable\\
\textbf{outlier} & Detect outliers for all cases & Analyse & Stable\\
\textbf{} &  &  \vphantom{1} & \\
\textbf{fill\_missing} & Interpolate missign values or missing measurement times & Manipulate & Stable\\
\textbf{ranks} & Covert data into ranked data across all cases & Manipulate & Stable\\
\textbf{transform} & Change and create new variabes & Manipulate & Stable\\
\textbf{smooth\_cases} & Smoothes time series data & Manipulate & Stable\\
\textbf{truncate\_phase} & Deletes measurements of phases & Manipulate & Stable\\
\textbf{standardize} & Standardizes or centers variables across cases & Manipulate & Stable\\
\textbf{} &  &  & \\
\textbf{design} & Defines a design of one or multiple single-cases & Simulate & Stable\\
\textbf{power\_test} & Calculates power and alpha error of a specific analyzes for a specific single-case design & Simulate & Stable\\
\textbf{estimate\_design} & Extraxt a deisgn template from an existing scdf & Simulate & Experimental\\
\textbf{random\_scdf} & Creats random single-case studies from a single-case design & Simulate & Stable\\
\bottomrule
\end{tabular}
\end{table}

\hypertarget{managing-single-case-data}{%
\chapter{Managing single-case data}\label{managing-single-case-data}}

\hypertarget{a-single-case-data-frame}{%
\section{\texorpdfstring{A \textbf{\emph{single-case data frame}}}{A single-case data frame}}\label{a-single-case-data-frame}}

Scan provides its own data-class for encoding single-case data: the \emph{single-case data frame} (short \emph{scdf}). An \emph{scdf} is an object that contains one or multiple single-case data sets and is optimized for managing and displaying these data. Think of an scdf as a file including a separate datasheet for each single case. Each datasheet is made up of at least three variables: The measured \textbf{values}, the \textbf{phase} identifier for each measured value, and the measurement time (\textbf{mt}) of each measure. Optionally, scdfs could include further variables for each single-case (e.g., control variables), and also a name for each case.

\begin{rmdnote}
Technically, an scdf object is a list containing data frames. It is of
the class \texttt{c("scdf","list")}. Additionally, an \emph{scdf}
entails an attribute \texttt{scdf} with a list with further attributes.
\texttt{var.values}, \texttt{var.phase}, and \texttt{var.mt} contain the
names of the \texttt{values}, \texttt{phase}, and the
\texttt{measurement\ time} variable. By default, these names are set to
\texttt{values}, \texttt{phase}, and \texttt{mt}.
\end{rmdnote}

Several functions are available for creating, transforming, merging, and importing/exporting \emph{scdfs}.

\hypertarget{creating-scdfs}{%
\section{Creating scdfs}\label{creating-scdfs}}

The \texttt{scdf} function is the basic tool for creating a single-case data frame. Basically, you have to provide the measurement \emph{values} and the \emph{phase} structure and a scdf object is build. There are three different ways of defining the phase structure. First, defining the beginning of the B-phase with the \texttt{B\_start} argument, second, defining a design with the \texttt{phase\_design} argument and third, setting parameters in a named vector of the dependent variable.

\begin{Shaded}
\begin{Highlighting}[]
\DocumentationTok{\#\#\# Three ways to code the same scdf}
\FunctionTok{scdf}\NormalTok{(}\AttributeTok{values =} \FunctionTok{c}\NormalTok{(}\AttributeTok{A =} \DecValTok{2}\NormalTok{,}\DecValTok{2}\NormalTok{,}\DecValTok{4}\NormalTok{,}\DecValTok{5}\NormalTok{, }\AttributeTok{B =} \DecValTok{8}\NormalTok{,}\DecValTok{7}\NormalTok{,}\DecValTok{6}\NormalTok{,}\DecValTok{9}\NormalTok{,}\DecValTok{8}\NormalTok{,}\DecValTok{7}\NormalTok{))}
\FunctionTok{scdf}\NormalTok{(}\AttributeTok{values =} \FunctionTok{c}\NormalTok{(}\DecValTok{2}\NormalTok{,}\DecValTok{2}\NormalTok{,}\DecValTok{4}\NormalTok{,}\DecValTok{5}\NormalTok{,}\DecValTok{8}\NormalTok{,}\DecValTok{7}\NormalTok{,}\DecValTok{6}\NormalTok{,}\DecValTok{9}\NormalTok{,}\DecValTok{8}\NormalTok{,}\DecValTok{7}\NormalTok{), }\AttributeTok{B\_start =} \DecValTok{5}\NormalTok{)}
\FunctionTok{scdf}\NormalTok{(}\AttributeTok{values =} \FunctionTok{c}\NormalTok{(}\DecValTok{2}\NormalTok{,}\DecValTok{2}\NormalTok{,}\DecValTok{4}\NormalTok{,}\DecValTok{5}\NormalTok{,}\DecValTok{8}\NormalTok{,}\DecValTok{7}\NormalTok{,}\DecValTok{6}\NormalTok{,}\DecValTok{9}\NormalTok{,}\DecValTok{8}\NormalTok{,}\DecValTok{7}\NormalTok{), }\AttributeTok{phase\_design =} \FunctionTok{c}\NormalTok{(}\AttributeTok{A =} \DecValTok{4}\NormalTok{, }\AttributeTok{B =} \DecValTok{6}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

The \texttt{B\_start} argument is only applicable when the single-case consists of a single A-phase followed by a B-phase. It is a remnant from the time when \texttt{scan} could only handle sign-case designs with two phases. The number assigned to \texttt{B\_start} indicates the measurement-time as defined in the \texttt{mt} argument. That is, assume a vector for the measurement times \texttt{mt\ =\ c(1,3,7,10,15,17,18,20)} and \texttt{B\_start\ =\ 15} then the first measurement of the B-phase will start with the fifth measurement at which \emph{mt} = 15.\\
The \texttt{phase\_design} argument is a named vector with the name and length of each phase. The phase names can be set arbitrary, although I recommend to use capital letters (A, B, C, \ldots) for each phase followed by, when indicated, a number if the phases repeat (A1, B1, A2, B2, \ldots). Although it is possible to give the same name to more than one phase (A, B, A, B) this might lead to some confusion and errors when coding analyzes with \emph{scan}.\\
When the vector of the dependent variable includes named values, a phase\_design structure is created automatically. Each named value sets the beginning of a new phase. For example \texttt{c(A\ =\ 3,2,4,\ B\ =\ 5,4,3,\ C\ =\ 6,7,6,5)} will create an ABC-phase design with 3, 3, and 4 values per phase.\\
Use only one of the three methods at a time and I recommend to use the \texttt{phase\_design} argument or the named vector method as they are the most versatile.\\
If no measurement times are given, scdf automatically adds them numbered sequentially 1, 2, 3, \ldots, \emph{N} where \emph{N} is the number of measurements. in some circumstances it might be useful to define individual measurement times for each measurement. For example, if you want to include the days since the beginning of the study as time intervals between measurements are widely varying you might get more valid results this way when analyzing the data in a regression approach.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# example of a more complex design }
\FunctionTok{scdf}\NormalTok{(}
  \AttributeTok{values =} \FunctionTok{c}\NormalTok{(}\DecValTok{2}\NormalTok{,}\DecValTok{2}\NormalTok{,}\DecValTok{4}\NormalTok{,}\DecValTok{5}\NormalTok{, }\DecValTok{8}\NormalTok{,}\DecValTok{7}\NormalTok{,}\DecValTok{6}\NormalTok{,}\DecValTok{9}\NormalTok{,}\DecValTok{8}\NormalTok{,}\DecValTok{7}\NormalTok{, }\DecValTok{12}\NormalTok{,}\DecValTok{11}\NormalTok{,}\DecValTok{13}\NormalTok{), }
  \AttributeTok{mt =} \FunctionTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{,}\DecValTok{2}\NormalTok{,}\DecValTok{3}\NormalTok{,}\DecValTok{6}\NormalTok{, }\DecValTok{8}\NormalTok{,}\DecValTok{9}\NormalTok{,}\DecValTok{11}\NormalTok{,}\DecValTok{12}\NormalTok{,}\DecValTok{16}\NormalTok{,}\DecValTok{18}\NormalTok{, }\DecValTok{27}\NormalTok{,}\DecValTok{28}\NormalTok{,}\DecValTok{29}\NormalTok{),}
  \AttributeTok{phase\_design =} \FunctionTok{c}\NormalTok{(}\AttributeTok{A =} \DecValTok{4}\NormalTok{, }\AttributeTok{B =} \DecValTok{6}\NormalTok{, }\AttributeTok{C =} \DecValTok{3}\NormalTok{)}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
#A single-case data frame with one case

 Case1: values mt phase
             2  1     A
             2  2     A
             4  3     A
             5  6     A
             8  8     B
             7  9     B
             6 11     B
             9 12     B
             8 16     B
             7 18     B
            12 27     C
            11 28     C
            13 29     C
\end{verbatim}

Missing values could be coded using \texttt{NA} (not available).

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{scdf}\NormalTok{(}\AttributeTok{values =} \FunctionTok{c}\NormalTok{(}\AttributeTok{A =} \DecValTok{2}\NormalTok{,}\DecValTok{2}\NormalTok{,}\ConstantTok{NA}\NormalTok{,}\DecValTok{5}\NormalTok{, }\AttributeTok{B =} \DecValTok{8}\NormalTok{,}\DecValTok{7}\NormalTok{,}\DecValTok{6}\NormalTok{,}\DecValTok{9}\NormalTok{,}\ConstantTok{NA}\NormalTok{,}\DecValTok{7}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

More variables are implemented by adding new variable names with a vector containing the values. Please be aware that a new variable must never have the same name as one of the arguments of the function (i.e.~B\_start, phase\_design, name, dvar, pvar, mvar).

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{scdf}\NormalTok{(}
  \AttributeTok{values =} \FunctionTok{c}\NormalTok{(}\AttributeTok{A =} \DecValTok{2}\NormalTok{,}\DecValTok{2}\NormalTok{,}\DecValTok{3}\NormalTok{,}\DecValTok{5}\NormalTok{, }\AttributeTok{B =} \DecValTok{8}\NormalTok{,}\DecValTok{7}\NormalTok{,}\DecValTok{6}\NormalTok{,}\DecValTok{9}\NormalTok{,}\DecValTok{7}\NormalTok{,}\DecValTok{7}\NormalTok{), }
  \AttributeTok{teacher =} \FunctionTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{,}\DecValTok{0}\NormalTok{,}\DecValTok{1}\NormalTok{,}\DecValTok{1}\NormalTok{,}\DecValTok{0}\NormalTok{,}\DecValTok{1}\NormalTok{,}\DecValTok{1}\NormalTok{,}\DecValTok{1}\NormalTok{,}\DecValTok{0}\NormalTok{,}\DecValTok{1}\NormalTok{), }
  \AttributeTok{hour =} \FunctionTok{c}\NormalTok{(}\DecValTok{2}\NormalTok{,}\DecValTok{3}\NormalTok{,}\DecValTok{4}\NormalTok{,}\DecValTok{3}\NormalTok{,}\DecValTok{3}\NormalTok{,}\DecValTok{1}\NormalTok{,}\DecValTok{6}\NormalTok{,}\DecValTok{5}\NormalTok{,}\DecValTok{2}\NormalTok{,}\DecValTok{2}\NormalTok{)}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
#A single-case data frame with one case

 Case1: values teacher hour mt phase
             2       0    2  1     A
             2       0    3  2     A
             3       1    4  3     A
             5       1    3  4     A
             8       0    3  5     B
             7       1    1  6     B
             6       1    6  7     B
             9       1    5  8     B
             7       0    2  9     B
             7       1    2 10     B
\end{verbatim}

Table \ref{tab:table-scdf} shows a complete list of arguments that could be passed to the function.

\begin{table}

\caption{\label{tab:table-scdf}Arguments of the scdf function}
\begin{tabular}[t]{>{\raggedright\arraybackslash}p{15em}>{\raggedright\arraybackslash}p{30em}}
\toprule
Argument & What it does ...\\
\midrule
\textbf{values} & The default vector with values for the dependent variable. It can be changed with the dvar argument.\\
\textbf{phase} & Usually, this variable is not defined manually and will be created by the function. It is the default vector with values for the phase variable. It can be changed with the pvar argument.\\
\textbf{mt} & The default vector with values for the measurement-time variable. It can be changed with the mvar argument.\\
\textbf{phase\_design} & A vector defining the length and label of each phase.\\
\textbf{B\_start} & The first measurement of phase B (simple coding if design is strictly AB).\\
\textbf{name} & A name for the case.\\
\textbf{dvar} & The name of the dependent variable. By default this is 'values'.\\
\textbf{pvar} & The name of the variable containing the phase information. By default this is 'phase'.\\
\textbf{mvar} & The name of the variable with the measurement-time. The default is 'mt'.\\
\textbf{...} & Any number of variables with a vector asigned to them.\\
\bottomrule
\end{tabular}
\end{table}

If you want to create a data-set comprising several single-cases the easiest way is to first create an scdf for each case and then join them into a new scdf with the \texttt{c} command:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{case1 }\OtherTok{\textless{}{-}} \FunctionTok{scdf}\NormalTok{(}
  \AttributeTok{values =} \FunctionTok{c}\NormalTok{(}\AttributeTok{A =} \DecValTok{5}\NormalTok{, }\DecValTok{7}\NormalTok{, }\DecValTok{10}\NormalTok{, }\DecValTok{5}\NormalTok{, }\DecValTok{12}\NormalTok{, }\AttributeTok{B =} \DecValTok{7}\NormalTok{, }\DecValTok{10}\NormalTok{, }\DecValTok{18}\NormalTok{, }\DecValTok{15}\NormalTok{, }\DecValTok{14}\NormalTok{, }\DecValTok{19}\NormalTok{), }
  \AttributeTok{name =} \StringTok{"Charlotte"}
\NormalTok{)}
\NormalTok{case2 }\OtherTok{\textless{}{-}} \FunctionTok{scdf}\NormalTok{(}
  \AttributeTok{values =} \FunctionTok{c}\NormalTok{(}\AttributeTok{A =} \DecValTok{3}\NormalTok{, }\DecValTok{4}\NormalTok{, }\DecValTok{3}\NormalTok{, }\DecValTok{5}\NormalTok{, }\AttributeTok{B =} \DecValTok{7}\NormalTok{, }\DecValTok{4}\NormalTok{, }\DecValTok{7}\NormalTok{, }\DecValTok{9}\NormalTok{, }\DecValTok{8}\NormalTok{, }\DecValTok{10}\NormalTok{, }\DecValTok{12}\NormalTok{), }
  \AttributeTok{name =} \StringTok{"Theresa"}
\NormalTok{)}
\NormalTok{case3 }\OtherTok{\textless{}{-}} \FunctionTok{scdf}\NormalTok{(}
  \AttributeTok{values =} \FunctionTok{c}\NormalTok{(}\AttributeTok{A =} \DecValTok{9}\NormalTok{, }\DecValTok{8}\NormalTok{, }\DecValTok{8}\NormalTok{, }\DecValTok{7}\NormalTok{, }\DecValTok{5}\NormalTok{, }\DecValTok{7}\NormalTok{, }\AttributeTok{B =} \DecValTok{6}\NormalTok{, }\DecValTok{14}\NormalTok{, }\DecValTok{15}\NormalTok{, }\DecValTok{12}\NormalTok{, }\DecValTok{16}\NormalTok{), }
  \AttributeTok{name =} \StringTok{"Antonia"}
\NormalTok{)}
\NormalTok{mbd }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(case1, case2, case3)}
\end{Highlighting}
\end{Shaded}

If you like to use other than the default variable names (``values'', ``phase'', and ``mt'') you could define these with the \texttt{dvar} (for the dependent variable), \texttt{pvar} (the variable indicating the phase), and \texttt{mvar} (the measurement-time variable) arguments.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Example: Using a different name for the dependent variable}
\NormalTok{case }\OtherTok{\textless{}{-}} \FunctionTok{scdf}\NormalTok{(}
  \AttributeTok{score =} \FunctionTok{c}\NormalTok{(}\AttributeTok{A =} \DecValTok{5}\NormalTok{, }\DecValTok{7}\NormalTok{, }\DecValTok{10}\NormalTok{, }\DecValTok{5}\NormalTok{, }\DecValTok{12}\NormalTok{, }\AttributeTok{B =} \DecValTok{7}\NormalTok{, }\DecValTok{10}\NormalTok{, }\DecValTok{18}\NormalTok{, }\DecValTok{15}\NormalTok{, }\DecValTok{14}\NormalTok{, }\DecValTok{19}\NormalTok{), }
  \AttributeTok{dvar =} \StringTok{"score"}
\NormalTok{)}

\CommentTok{\# Example: Using new names for the dependent and the phase variables}
\NormalTok{case }\OtherTok{\textless{}{-}} \FunctionTok{scdf}\NormalTok{(}
  \AttributeTok{score =} \FunctionTok{c}\NormalTok{(}\AttributeTok{A =} \DecValTok{3}\NormalTok{, }\DecValTok{4}\NormalTok{, }\DecValTok{3}\NormalTok{, }\DecValTok{5}\NormalTok{, }\AttributeTok{B =} \DecValTok{7}\NormalTok{, }\DecValTok{4}\NormalTok{, }\DecValTok{7}\NormalTok{, }\DecValTok{9}\NormalTok{, }\DecValTok{8}\NormalTok{, }\DecValTok{10}\NormalTok{, }\DecValTok{12}\NormalTok{), }
  \AttributeTok{dvar =} \StringTok{"score"}\NormalTok{, }\AttributeTok{pvar =} \StringTok{"section"}
\NormalTok{)}

\CommentTok{\# Example: Using new names for dependent, phase, and measurement{-}time variables}
\NormalTok{case }\OtherTok{\textless{}{-}} \FunctionTok{scdf}\NormalTok{(}
  \AttributeTok{score =} \FunctionTok{c}\NormalTok{(}\AttributeTok{A =} \DecValTok{9}\NormalTok{, }\DecValTok{8}\NormalTok{, }\DecValTok{8}\NormalTok{, }\DecValTok{7}\NormalTok{, }\DecValTok{5}\NormalTok{, }\DecValTok{7}\NormalTok{, }\AttributeTok{B =} \DecValTok{6}\NormalTok{, }\DecValTok{14}\NormalTok{, }\DecValTok{15}\NormalTok{, }\DecValTok{12}\NormalTok{, }\DecValTok{16}\NormalTok{), }
  \AttributeTok{name =} \StringTok{"Antonia"}\NormalTok{, }\AttributeTok{dvar =} \StringTok{"score"}\NormalTok{, }\AttributeTok{pvar =} \StringTok{"section"}\NormalTok{, }\AttributeTok{mvar =} \StringTok{"day"}
\NormalTok{)}

\FunctionTok{summary}\NormalTok{(case)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
#A single-case data frame with one case

         Measurements Design
 Antonia           11    A-B

Variable names:
score <dependent variable>
day <measurement-time variable>
section <phase variable>
\end{verbatim}

\hypertarget{saving-and-reading-single-case-data-frames}{%
\section{\texorpdfstring{Saving and reading \emph{single-case data frames}}{Saving and reading single-case data frames}}\label{saving-and-reading-single-case-data-frames}}

Usually, it is not needed to save an scdf to a separate file on your computer. In most of the cases you could keep the coding of the \emph{scdf} as described above and rerun it every time that you are working with your data. But sometimes it is more convenient to separately save the data to a file for later use or to send them to a colleague.\\
The simplest way is to use the base \emph{R} functions \texttt{saveRDS} and \texttt{readRDS} for this purpose. \texttt{saveRDS} takes at least two arguments: the first is the object you like to save and the second is a file name for the resulting file. If you have an \emph{scdf} with the name \texttt{study1} the line \texttt{saveRDS(study1,\ "study1.rds")} will save the \emph{scdf} to your drive. You could later read this file with \texttt{study1\ \textless{}-\ readRDS("study1.rds")}. \texttt{getwd()} will return the current active folder that you are working in.

\hypertarget{import-and-export-single-case-data-frames}{%
\section{\texorpdfstring{Import and export \emph{single-case data frames}}{Import and export single-case data frames}}\label{import-and-export-single-case-data-frames}}

When you are working with other programs besides \textbf{R} you need to export and import the \emph{scdf} into a common file format. \texttt{read\_scdf} imports a comma-separated-variable (\emph{csv}) file and converts it into an \emph{scdf} object. By default, the csv-file has to contain the columns \emph{case}, \emph{phase}, and \emph{values}. Optionally, a further column named \emph{mt} could be provided. The csv file should be build up like this:

\begin{figure}
\centering
\includegraphics[width=3.125in,height=\textheight]{images/readSC.jpg}
\caption{How to format a single-case file in a spreadsheet program for importing into scan}
\end{figure}

In case your variables names differ from the standard (i.e.~``case'', ``values'', ``phase'', and ``mt'' ), you could set additional arguments to fit your file. \texttt{read\_scdf("example.csv",\ cvar\ =\ "name",\ dvar\ =\ "wellbeing",\ pvar\ =\ "intervention",\ mvar\ =\ "time")} for example will set the variables attributes of the resulting scdf. Cases will be split by the variable \texttt{"name"}, \texttt{"wellbeing"} is set as the dependent variable (default is \emph{values}), phase information are in the variable \texttt{"intervention"}, and measurement times in the variable \texttt{"time"}. You could also reassign the phase names within the phase variable by setting the argument \texttt{phase.names}. Assume for example your file contains the values 0 and 1 to identify the two phases I recommend to set them to ``A'' and ``B'' with \texttt{read\_scdf("example.csv",\ phase.names\ =\ c("A",\ "B"))}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{dat }\OtherTok{\textless{}{-}} \FunctionTok{read\_scdf}\NormalTok{(}
  \StringTok{"example2.xlsx"}\NormalTok{, }\AttributeTok{cvar =} \StringTok{"name"}\NormalTok{, }\AttributeTok{pvar =} \StringTok{"intervention"}\NormalTok{, }
  \AttributeTok{dvar =} \StringTok{"wellbeing"}\NormalTok{, }\AttributeTok{mvar =} \StringTok{"time"}\NormalTok{, }\AttributeTok{phase.names =} \FunctionTok{c}\NormalTok{(}\StringTok{"A"}\NormalTok{,}\StringTok{"B"}\NormalTok{)}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Loaded 20 cases.
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{summary}\NormalTok{(dat)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
#A single-case data frame with 20 cases

          Measurements Design
 Charles            20    A-B
 Kolten             20    A-B
 Annika             20    A-B
 Kaysen             20    A-B
 Urijah             20    A-B
 Leila              20    A-B
 Leia               20    A-B
 Aleigha            20    A-B
 Greta              20    A-B
 Alijah             20    A-B
 Ricardo            20    A-B
 Dallas             20    A-B
 Edith              20    A-B
 Braylee            20    A-B
 Giovanni           20    A-B
 Ismael             20    A-B
 Grady              20    A-B
 Raina              20    A-B
 Cambria            20    A-B
 Lincoln            20    A-B

Variable names:
intervention <phase variable>
wellbeing <dependent variable>
time <measurement-time variable>
age
gender
gym
\end{verbatim}

For some reasons, computer systems with a German (and some other) language setups export csv-files by default with a comma as a decimal point and a semicolon as a separator between values. In these cases you have to set two extra arguments to import the data:

\texttt{read\_scdf("example.csv",\ dec\ =\ ",",\ sep\ =\ ";")}

\texttt{read\_scdf} also allows for directly importing \emph{Microsoft Excel} \texttt{.xlsx} or \texttt{.xls} files. You need to have the library \texttt{readxl} installed in your R setup for this to work. Excel files will be automatically detected by the filename extension \texttt{xls}or \texttt{xlsx} or by explicitly setting the \texttt{type} argument (e.g.~\texttt{type\ =\ "xlsx"}).

\texttt{write\_scdf()} exports an scdf object as a comma-separated-variables file (\emph{csv}) which can be imported into any other software for data analyses (MS OFFICE, Libre Office etc.). The scdf object is converted into a single data frame with a \emph{case} variable identifying the rows for each subject. The first argument of the command identifies the scdf to be exported and the second argument (\texttt{file}) the name of the resulting csv-file. If no file argument is provided, a dialog box is opened to choose a file interactively. By default, writeSC exports into a standard csv-format with a dot as the decimal point and a comma for separating variables. If your system expects a comma instead of a point for decimal numbers you may use the \texttt{dec} and the \texttt{sep} arguments. For example, \texttt{write\_scdf(example,\ file\ =\ "example.csv",\ dec\ =\ ",",\ sep\ =\ ";")} exports a csv variation usually used for example in Germany.

\hypertarget{convert-an-scdf-object-back-to-scan-syntax}{%
\section{Convert an scdf object back to scan syntax}\label{convert-an-scdf-object-back-to-scan-syntax}}

You can also reconvert an scdf object back to ``raw'' scan syntax. This is a convenient way when you imported data from an Excel or csv file and want to keep everything clean and transparent within your R syntax files.

Here is an example:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{convert}\NormalTok{(exampleABC)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
case1 <- scdf(
   values = c(58, 56, 60, 63, 51, 45, 44, 59, 45, 39, 83, 65, 70, 83, 70, 85, 47, 66, 77, 75, 51, 87, 80, 68, 70, 56, 52, 70, 83, 63), 
   phase_design = c(A = 10, B = 10, C = 10),
   name = "Marie"
)

case2 <- scdf(
   values = c(47, 41, 47, 52, 54, 65, 55, 37, 51, 60, 60, 65, 55, 46, 49, 54, 77, 73, 97, 64, 84, 71, 66, 74, 78, 68, 52, 76, 63, 54), 
   phase_design = c(A = 15, B = 8, C = 7),
   name = "Rosalind"
)

case3 <- scdf(
   values = c(50, 45, 63, 53, 66, 57, 35, 45, 74, 63, 47, 45, 47, 36, 51, 55, 35, 66, 59, 55, 73, 60, 85, 62, 79, 69, 87, 76, 90, 48), 
   phase_design = c(A = 20, B = 7, C = 3),
   name = "Lise"
)

study <- c(case1, case2, case3)
\end{verbatim}

Now you can copy and past the output into your R file or you set the \texttt{file} argument to save the output into an R file \texttt{convert(exampleABC,\ file\ =\ "scdf.R")}.

\hypertarget{displaying-scdf-files}{%
\section{Displaying scdf-files}\label{displaying-scdf-files}}

\emph{scdf} are displayed by just typing the name of the object.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#Beretvas2008 is an example scdf included in scan}
\NormalTok{Beretvas2008}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
#A single-case data frame with one case

 Case1: values mt phase
           0.7  1     A
           1.6  2     A
           1.4  3     A
           1.6  4     A
           1.9  5     A
           1.2  6     A
           1.3  7     A
           1.6  8     A
            10  9     B
          10.8 10     B
          11.9 11     B
            11 12     B
            13 13     B
          12.7 14     B
            14 15     B
\end{verbatim}

The \texttt{print} command allows for specifying the output. Some possible arguments are \texttt{cases} (the number of cases to be displayed; Three by default), \texttt{rows} (the maximum number of rows to be displayed; Fifteen by default), and \texttt{digits} (number of digits). \texttt{cases\ =\ \textquotesingle{}all\textquotesingle{}} and \texttt{rows\ =\ \textquotesingle{}all\textquotesingle{}} prints all cases and rows.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#Huber2014 is an example scdf included in scan}
\FunctionTok{print}\NormalTok{(Huber2014, }\AttributeTok{cases =} \DecValTok{2}\NormalTok{, }\AttributeTok{rows =} \DecValTok{10}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
#A single-case data frame with 4 cases

 Adam: mt compliance phase ｜ Berta: mt compliance phase ｜
        1         25     A ｜         1         25     A ｜
        2       20.8     A ｜         2       20.8     A ｜
        3       39.6     A ｜         3       39.6     A ｜
        4         75     A ｜         4         75     A ｜
        5         45     A ｜         5         45     A ｜
        6       39.6     A ｜         6       14.6     A ｜
        7       54.2     A ｜         7       45.8     A ｜
        8         50     A ｜         8       33.3     A ｜
        9       28.1     A ｜         9       31.3     A ｜
       10         40     A ｜        10       32.5     A ｜
# ... up to 66 more rows
#  2 more cases
\end{verbatim}

The argument \texttt{long\ =\ TRUE} prints each case one after the other instead side by side (e.g., \texttt{print(exampleAB,\ long\ =\ TRUE)}).

\texttt{summary()} gives a very concise overview of the \emph{scdf}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{summary}\NormalTok{(Huber2014)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
#A single-case data frame with 4 cases

           Measurements Design
 Adam                37    A-B
 Berta               29    A-B
 Christian           76    A-B
 David               76    A-B

Variable names:
mt <measurement-time variable>
compliance <dependent variable>
phase <phase variable>


Note:  Behavioral data (compliance in percent).
Author of data:  Christian Huber 
\end{verbatim}

\hypertarget{selecting-cases-and-measurements}{%
\section{Selecting cases and measurements}\label{selecting-cases-and-measurements}}

\hypertarget{subsetting-cases-with-base-r-syntax}{%
\subsection{Subsetting cases with base R syntax}\label{subsetting-cases-with-base-r-syntax}}

You can extract one or more single-cases from an \emph{scdf} with multiple cases in two ways. If the case has a name, you can address it with the \texttt{\$} operator.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{Huber2014}\SpecialCharTok{$}\NormalTok{David}
\end{Highlighting}
\end{Shaded}

or you can use squared brackets

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{Huber2014[}\DecValTok{1}\NormalTok{] }\CommentTok{\#extracts case 1}
\NormalTok{Huber2014[}\DecValTok{2}\SpecialCharTok{:}\DecValTok{3}\NormalTok{] }\CommentTok{\#extracts cases 2 and 3}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{new.huber2014 }\OtherTok{\textless{}{-}}\NormalTok{ Huber2014[}\FunctionTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{4}\NormalTok{)] }\CommentTok{\#extracts cases 1 and 4}
\NormalTok{new.huber2014}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
#A single-case data frame with 2 cases

 Adam: mt compliance phase ｜ David: mt compliance phase ｜
        1         25     A ｜         1       65.6     A ｜
        2       20.8     A ｜         2       37.5     A ｜
        3       39.6     A ｜         3       58.3     A ｜
        4         75     A ｜         4       72.9     A ｜
        5         45     A ｜         5       33.3     A ｜
        6       39.6     A ｜         6       59.4     A ｜
        7       54.2     A ｜         7       77.1     A ｜
        8         50     A ｜         8       54.2     A ｜
        9       28.1     A ｜         9       68.8     A ｜
       10         40     A ｜        10       43.8     A ｜
       11       52.1     B ｜        11       62.5     B ｜
       12       31.3     B ｜        12       64.6     B ｜
       13       15.6     B ｜        13       60.4     B ｜
       14       29.2     B ｜        14       81.3     B ｜
       15       43.8     B ｜        15       79.2     B ｜
# ... up to 61 more rows
\end{verbatim}

\hypertarget{select-cases}{%
\subsection{Select cases}\label{select-cases}}

Since version 0.53 scan includes some functions to work with pipe-operators. Therefore, we will provide syntax examples with and without pipe operators.

The \texttt{select\_cases()} function takes case-names and/or numbers for selecting cases:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# With pipes:}
\NormalTok{Huber2014 }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{select\_cases}\NormalTok{(}\StringTok{"Adam"}\NormalTok{, }\StringTok{"Berta"}\NormalTok{, }\DecValTok{4}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{summary}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
#A single-case data frame with 3 cases

       Measurements Design
 Adam            37    A-B
 Berta           29    A-B
 David           76    A-B

Variable names:
mt <measurement-time variable>
compliance <dependent variable>
phase <phase variable>


Note:  Behavioral data (compliance in percent).
Author of data:  Christian Huber 
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Without pipes:}

\CommentTok{\# new\_huber \textless{}{-} select\_cases(Huber2014, "Adam", "Berta", 4)}
\CommentTok{\# summary(new\_huber)}
\end{Highlighting}
\end{Shaded}

\hypertarget{select-measurements}{%
\subsection{Select measurements}\label{select-measurements}}

The \texttt{subset()} function helps with extracting measurements (or rows) by a specific criteria from a scdf.

Subset takes a scdf as its first argument and a logical expression as the second argument (\texttt{filter}). Only measurements for which the logical argument is evaluated to be TRUE are inlcuded in the returning scdf object.

For example, the scdf \texttt{Huber2014} has a variable \texttt{compliance} and we like to keep measurements where \texttt{compliance} is larger than 10 because we assume the others to be outliers:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{Huber2014 }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{subset}\NormalTok{(compliance }\SpecialCharTok{\textgreater{}} \DecValTok{10}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{summary}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
#A single-case data frame with 4 cases

           Measurements Design
 Adam                37    A-B
 Berta               20    A-B
 Christian           76    A-B
 David               76    A-B

Variable names:
mt <measurement-time variable>
compliance <dependent variable>
phase <phase variable>


Note:  Behavioral data (compliance in percent).
Author of data:  Christian Huber 
\end{verbatim}

In an more complex example, we only like to keep values lower than 60 when they are in phase A or values equal or larger than 60 when they are in phase B:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{exampleAB }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{subset}\NormalTok{((values }\SpecialCharTok{\textless{}} \DecValTok{60} \SpecialCharTok{\&}\NormalTok{ phase }\SpecialCharTok{==} \StringTok{"A"}\NormalTok{) }\SpecialCharTok{|}\NormalTok{ (values }\SpecialCharTok{\textgreater{}=} \DecValTok{60} \SpecialCharTok{\&}\NormalTok{ phase }\SpecialCharTok{==} \StringTok{"B"}\NormalTok{)) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{summary}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
#A single-case data frame with 3 cases

          Measurements Design
 Johanna            20    A-B
 Karolina           18    A-B
 Anja               19    A-B

Variable names:
values <dependent variable>
mt <measurement-time variable>
phase <phase variable>


Note:  Randomly created data with normal distributed dependent variable.
\end{verbatim}

\hypertarget{change-and-create-variables}{%
\section{Change and create variables}\label{change-and-create-variables}}

\hypertarget{creating-a-single-case-data-plot}{%
\chapter{Creating a single-case data plot}\label{creating-a-single-case-data-plot}}

Plotting the data is a first important approach of analyzing. After you build an \emph{scdf} the \texttt{plot} command helps to visualize the data. When the \texttt{scdf} includes more than one case a multiple baseline figure is provided. Various arguments can be set to customize the appearance of the plot. Table \ref{tab:table-plot-arguments} gives an overview of all available arguments.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{plot}\NormalTok{(exampleA1B1A2B2\_zvt)}
\end{Highlighting}
\end{Shaded}

\begin{figure}
\centering
\includegraphics{scan-book_files/figure-latex/ex-simple-plot-1.pdf}
\caption{\label{fig:ex-simple-plot}A simple plot does not need much.}
\end{figure}

\hypertarget{plot-axis}{%
\section{Plot axis}\label{plot-axis}}

Labels of the axes and for the phases can be changed with the \texttt{xlab}, \texttt{ylab}, and the \texttt{phase.names} arguments. The x- and y-scaling of the graphs are by default calculated as the minimum and the maximum of all included single cases. The \texttt{xlim} and the \texttt{ylim} argument are used to set specific values. The argument takes a vector of two numbers. The first for the lower and the second for the upper limit of the scale. In case of multiple single cases an \texttt{NA} sets the individual minimum or maximum for each case. Assume for example the study contains three single cases \texttt{ylim\ =\ c(0,\ NA)} will set the lower limit for all three single cases to \texttt{0} and the upper limit individually at the maximum of each case. The argument \texttt{xinc} sets the incremental steps for the x-axis ticks with corresponding values. For example \texttt{xinc\ =\ 1} will set a tick for every measurement time increase of 1 while \texttt{xinc\ =\ 5} will only set every ffith tick.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{plot}\NormalTok{(}
\NormalTok{  exampleABC,}
  \AttributeTok{phase.names =} \FunctionTok{c}\NormalTok{(}\StringTok{"Baseline"}\NormalTok{, }\StringTok{"Intervention"}\NormalTok{, }\StringTok{"Follow{-}Up"}\NormalTok{),}
  \AttributeTok{case.names =} \FunctionTok{c}\NormalTok{(}\StringTok{"First"}\NormalTok{, }\StringTok{"Second"}\NormalTok{, }\StringTok{"Third"}\NormalTok{),}
  \AttributeTok{ylab =} \StringTok{"Frequency"}\NormalTok{,}
  \AttributeTok{xlab =} \StringTok{"Days"}\NormalTok{,}
  \AttributeTok{main =} \StringTok{"An example"}\NormalTok{,}
  \AttributeTok{ylim =} \FunctionTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{120}\NormalTok{),}
  \AttributeTok{xinc =} \DecValTok{2}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}
\centering
\includegraphics{scan-book_files/figure-latex/ex-plot-axis-1.pdf}
\caption{\label{fig:ex-plot-axis}A plot with various axis specidications.}
\end{figure}

\begin{table}

\caption{\label{tab:table-plot-arguments}Arguments of the plot function}
\begin{tabular}[t]{>{\raggedright\arraybackslash}p{15em}>{\raggedright\arraybackslash}p{30em}}
\toprule
Argument & What it does ...\\
\midrule
\textbf{data} & A single-case data frame.\\
\textbf{ylim} & Lower and upper limits of the y-axis\\
\textbf{xlim} & Lower and upper limits of the x-axis.\\
\textbf{style} & A specific design for displaying the plot.\\
\textbf{lines} & A character or list defining one or more lines or curves to be plotted.\\
\textbf{marks} & A list of parameters defining markings of certain data points.\\
\textbf{main} & A figure title\\
\textbf{phase.names} & By default phases are labeled as given in the phase variable. Use this argument to specify different labels: `phase.names = c('Baseline', 'Intervention')`.\\
\textbf{case.names} & Case names. If not provided, names are taken from the scdf or left blank if the scdf does not contain case names.\\
\textbf{xlab} & The label of the x-axis. The default is taken from the name of the measurement variable as provided by the scdf.\\
\textbf{ylab} & The labels of the y-axis. The default is taken from the name of the dependent variable as provided by the scdf.\\
\textbf{xinc} & An integer. Increment of the x-axis. 1 : each mt value will be printed, 2 : every other value, 3 : every third values etc.\\
\bottomrule
\end{tabular}
\end{table}

\hypertarget{adding-lines}{%
\section{Adding lines}\label{adding-lines}}

Extra lines can be added to the plot using the \texttt{lines} argument. The lines argument takes several separate sub-arguments which have to be provided in a list. In its most simple form this list contains one element. \texttt{lines\ =\ list(type\ =\ \textquotesingle{}median\textquotesingle{})} adds a line with the median of each phase to the plot. Additional arguments like \texttt{col} or \texttt{lwd} help to format these lines. For adding red thick median lines use the command \texttt{lines\ =\ list(type\ =\ \textquotesingle{}median\textquotesingle{},\ col\ =\ \textquotesingle{}red\textquotesingle{},\ lwd\ =\ \textquotesingle{}2\textquotesingle{})}.

\begin{table}

\caption{\label{tab:table-lines-arguments}Values of the lines argument}
\begin{tabular}[t]{>{\raggedright\arraybackslash}p{15em}>{\raggedright\arraybackslash}p{30em}}
\toprule
Argument & What it does ...\\
\midrule
\textbf{median} & separate lines for the medians of each phase\\
\textbf{mean} & separate lines for the means of each phase. By default it is 10\%-trimmed. Other trims can be set using a second parameter (e.g., `lines = list(type = 'mean', trim = 0.2)` draws a 20\%-trimmed mean line).\\
\textbf{trend} & Separate lines for the trend of each phase.\\
\textbf{trendA} & Trend line for phase A, extrapolated throughout the other phases\\
\textbf{maxA} & Line at the level of the highest phase A score.\\
\textbf{minA} & Line at the level of the lowest phase A score.\\
\textbf{medianA} & Line at the phase A median score.\\
\textbf{meanA} & Line at the phase A 10\%-trimmed mean score. Apply a different trim, by using the additional argument (e.g., `lines = list(type = 'meanA', trim = 0.2)`).\\
\textbf{movingMean} & Draws a moving mean curve, with a specified lag: `lines = list(type = 'movingMean', lag = 2)`. Default is a lag 1 curve.\\
\textbf{movingMedian} & Draws a moving median curve, with a specified lag: `lines = list(type = 'movingMedian', lag = 3).` Default is a lag 1 curve.\\
\textbf{loreg} & Draws a non-parametric local regression line. The proportion of data influencing each data point can be specified using `lines = list(type = 'loreg', f = 0.66)`. The default is 0.5.\\
\textbf{lty} & Line type. Examples are: 'solid','dashed', 'dotted'.\\
\textbf{lwd} & Line thickness, e.g., `lwd = 4`.\\
\textbf{col} & Line colour, e.g., `col = 'red'`.\\
\bottomrule
\end{tabular}
\end{table}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{plot}\NormalTok{(}
\NormalTok{  exampleAB, }
  \AttributeTok{lines =} \FunctionTok{list}\NormalTok{(}
    \FunctionTok{list}\NormalTok{(}\AttributeTok{type =} \StringTok{"median"}\NormalTok{, }\AttributeTok{col =} \StringTok{"red"}\NormalTok{, }\AttributeTok{lwd =} \FloatTok{0.5}\NormalTok{),}
    \FunctionTok{list}\NormalTok{(}\AttributeTok{type =} \StringTok{"trend"}\NormalTok{, }\AttributeTok{col =} \StringTok{"blue"}\NormalTok{, }\AttributeTok{lty =} \StringTok{"dashed"}\NormalTok{, }\AttributeTok{lwd =} \DecValTok{2}\NormalTok{),}
    \FunctionTok{list}\NormalTok{(}\AttributeTok{type =} \StringTok{"loreg"}\NormalTok{, }\AttributeTok{f =} \FloatTok{0.2}\NormalTok{, }\AttributeTok{col =} \StringTok{"green"}\NormalTok{, }\AttributeTok{lty =} \StringTok{"solid"}\NormalTok{, }\AttributeTok{lwd =} \DecValTok{1}\NormalTok{)}
\NormalTok{  )}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}
\centering
\includegraphics{scan-book_files/figure-latex/ex-plot-lines-1.pdf}
\caption{\label{fig:ex-plot-lines}A plot with various visual aids}
\end{figure}

\hypertarget{mark-data-points}{%
\section{Mark data points}\label{mark-data-points}}

Specific data points can be highlighted using the \texttt{marks} argument. A \texttt{list} defines the measurement times to be marked, the marking color and the size of the marking. \texttt{marks\ =\ list(position\ =\ c(1,5,6))} marks the first, fifth, and sixth measurement time. If the \emph{scdf} contains more than one data-set marking would be the same for all data sets in this example. In case you define a \texttt{list} Containing vectors, marking can be individually defined for each data set. Assume, for example, we have an \emph{scdf} comprising three data sets, then \texttt{marks\ =\ list(position\ =\ list(c(1,2),\ c(3,4),\ c(5,6)))} will highlight measurement times one and two for the first data set, three and four for the second and five and six for the third. \texttt{pch}, \texttt{col} and \texttt{cex} define symbol, colour and size of the markings.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# plot with marks in a red circles 2.5 times larger than the standard symbol }
\CommentTok{\# size. exampleAB is an example scdf included in the scan package}
\NormalTok{marks }\OtherTok{\textless{}{-}} \FunctionTok{list}\NormalTok{(}
  \AttributeTok{positions =} \FunctionTok{list}\NormalTok{( }\FunctionTok{c}\NormalTok{(}\DecValTok{8}\NormalTok{, }\DecValTok{9}\NormalTok{), }\FunctionTok{c}\NormalTok{(}\DecValTok{17}\NormalTok{, }\DecValTok{19}\NormalTok{), }\FunctionTok{c}\NormalTok{(}\DecValTok{7}\NormalTok{, }\DecValTok{18}\NormalTok{) ), }
  \AttributeTok{col =} \StringTok{\textquotesingle{}red\textquotesingle{}}\NormalTok{, }\AttributeTok{cex =} \FloatTok{2.5}\NormalTok{, }\AttributeTok{pch =} \DecValTok{1}
\NormalTok{)}
\FunctionTok{plot}\NormalTok{(exampleAB, }\AttributeTok{marks =}\NormalTok{ marks, }\AttributeTok{style =} \StringTok{"sienna"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics{scan-book_files/figure-latex/marks-example-1} 

}

\caption{A plot with highlighted data-points}\label{fig:marks-example}
\end{figure}

\hypertarget{graphical-styles-of-a-plot}{%
\section{Graphical styles of a plot}\label{graphical-styles-of-a-plot}}

The \texttt{style} argument of the plot function allows to specify a specific design of a plot. By default, the \texttt{grid} style is applied. \texttt{scan} includes some further predefined styles. \texttt{default,\ yaxis,\ tiny,\ small,\ big,\ chart,\ ridge,\ annotate,\ grid,\ grid2,\ dark,\ nodot,\ and\ sienna}. The name of a style is provided as a character string (e.g., \texttt{style\ =\ "grid"}).\\
Some styles only address specific elements (e.g., ``small'' or ``tiny'' just influence text and line sizes). These styles lend themselves to be combined with other styles. This could be achieved by providing several style names to the plot argument: \texttt{style\ =\ c("grid",\ "annotate",\ "small")}. A style overwrites the settings of all previously included style.\\
Beyond predefined styles, styles can be individually modified and created. New styles are provided as a \texttt{list} of several design parameters that are passed to the \texttt{style} argument of the \texttt{plot} function. Table \ref{tab:table-style} shows all design parameter that could be defined.\\
To define a new style, first create a list containing a plain design. The \texttt{style\_plot} function returns such a list with the default values for a plain design (e.g., \texttt{mystyle\ \textless{}-\ style\_plot()}). Single design parameters can now be set by assigning a specific value within the list. For example, \texttt{newstyle\$fill\ \textless{}-\ "grey90"} will set the \texttt{fill} parameter to \texttt{"grey90"}. Alternatively, changes to the plain design can already by defined within the \texttt{style\_plot} function. To set a light-blue background color and also an orange grid, create the style \texttt{style\_plot(fill.bg\ =\ "lightblue",\ grid\ =\ "orange")}.
If you do not want to start with the plain design but a different of the predefined styles, set the \texttt{style} argument. If, for example, you like to have the \texttt{grid} combined with the \texttt{big} style but want to change the color of the grid to orange type \texttt{style\_plot(style\ =\ c("grid",\ "big"),\ col.grid\ =\ "orange")}. \texttt{plot(mydata,\ style\ =\ mystyle)} will apply the new style in a plot. Please note that the new style is not passed in quotation marks.

\begin{table}

\caption{\label{tab:table-style}Arguments of the style plot function}
\begin{tabular}[t]{>{\raggedright\arraybackslash}p{15em}>{\raggedright\arraybackslash}p{30em}}
\toprule
Argument & What it does ...\\
\midrule
\textbf{fill} & If TRUE area under the line is filled.\\
\textbf{col.fill} & Sets the color of the area under the line.\\
\textbf{grid} & If TRUE a grid is included.\\
\textbf{col.grid} & Sets the color of the grid.\\
\textbf{lty.grid} & Sets the line type of the grid.\\
\textbf{lwd.grid} & Sets the line thikness of the grid.\\
\textbf{fill.bg} & If not NA the backgorund of the plot is filled with the given color. If multiple colours are provided, the colours change with phases (e.g., `fill.bg = c('aliceblue', 'mistyrose1', 'honeydew')`\\
\textbf{annotations} & A list of parameters defining annotations to each data point. This adds the score of each MT to your plot. `'pos'` Position of the annotations: 1 = below, 2 = left, 3 = above, 4 = right. `'col'` Color of the annotations. `'cex'` Size of the annotations. `'round'` rounds the values to the specified decimal. `annotations = list(pos = 3, col = 'brown', round = 1)` adds scores rounded to one decimal above the data point in brown color to the plot.\\
\textbf{text.ABlag} & By default a vertical line separates phases A and B in the plot. Alternatively, you could print a character string between the two phases using this argument: `text.ABlag = 'Start'`.\\
\textbf{lwd} & Width of the plot line. Default is `lwd = 2`.\\
\textbf{pch} & Point type. Default is `pch = 17` (triangles). Other options are for example: 16 (filled circles) or 'A' (uses the letter A).\\
\textbf{col.lines} & The color of the lines. If set to an empty string no lines are drawn.\\
\textbf{col.dots} & The color of the dots. If set to an empty string no dots are drawn.\\
\textbf{mai} & Sets the margins of the plot.\\
\textbf{...} & Further arguments passed to the plot command.\\
\bottomrule
\end{tabular}
\end{table}

The width of the lines are set with the \texttt{lwd} argument, \texttt{col} is used to set the line colour and \texttt{pch} sets the symbol for a data point.
The \texttt{pch} argument can take several values for defining the symbol in which data points are plotted.

\begin{figure}
\centering
\includegraphics{scan-book_files/figure-latex/symbols, pch-1.pdf}
\caption{(\#fig:symbols, pch)Some of the possible symbols and their pch values.}
\end{figure}

Here is an example customizing a plot with several additional graphic parameters

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{newstyle }\OtherTok{\textless{}{-}} \FunctionTok{style\_plot}\NormalTok{(}
  \AttributeTok{fill =} \StringTok{"grey95"}\NormalTok{,}
  \AttributeTok{fill.bg =} \FunctionTok{c}\NormalTok{(}\StringTok{\textquotesingle{}aliceblue\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}mistyrose1\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}honeydew\textquotesingle{}}\NormalTok{),}
  \AttributeTok{names =} \FunctionTok{list}\NormalTok{(}\AttributeTok{col =} \StringTok{"brown"}\NormalTok{, }\AttributeTok{cex =} \DecValTok{2}\NormalTok{, }\AttributeTok{font =} \DecValTok{3}\NormalTok{, }\AttributeTok{side =} \DecValTok{3}\NormalTok{),}
  \AttributeTok{annotations =} \FunctionTok{list}\NormalTok{(}\AttributeTok{col =} \StringTok{"brown"}\NormalTok{),}
  \AttributeTok{col.dots =} \StringTok{"blue"}\NormalTok{,}
  \AttributeTok{grid =} \StringTok{"lightblue"}\NormalTok{, }
  \AttributeTok{pch =} \DecValTok{16}\NormalTok{)}

\FunctionTok{plot}\NormalTok{(exampleABAB, }\AttributeTok{style =}\NormalTok{ newstyle)}
\end{Highlighting}
\end{Shaded}

\begin{figure}
\centering
\includegraphics{scan-book_files/figure-latex/custom_style_example-1.pdf}
\caption{(\#fig:custom\_style\_example)A plot with a customized style.}
\end{figure}

\hypertarget{describe-and-manipulate-single-case-data-frames}{%
\chapter{Describe and manipulate single-case data frames}\label{describe-and-manipulate-single-case-data-frames}}

\hypertarget{describing-and-summarizing}{%
\section{Describing and summarizing}\label{describing-and-summarizing}}

A short description of the \emph{scdf} is provided by the \texttt{summary} command. The results are pretty much self explaining

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{summary}\NormalTok{(Huber2014)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
#A single-case data frame with 4 cases

           Measurements Design
 Adam                37    A-B
 Berta               29    A-B
 Christian           76    A-B
 David               76    A-B

Variable names:
mt <measurement-time variable>
compliance <dependent variable>
phase <phase variable>


Note:  Behavioral data (compliance in percent).
Author of data:  Christian Huber 
\end{verbatim}

\texttt{describe} is the basic command to get an overview on descriptive statistics. As an argument it only takes the name of the \emph{scdf} object. For each case of the \emph{scdf} and each phase within a case descriptive statistics are provided. The output table contains statistical indicators followed by a dot and the name of the phase (e.g., \texttt{n.A} for the number of measurements of phase A).

\begin{table}

\caption{\label{tab:table-describe}Statistics of the describe command}
\begin{tabular}[t]{>{\raggedright\arraybackslash}p{15em}>{\raggedright\arraybackslash}p{30em}}
\toprule
Parameter & What it means ...\\
\midrule
\textbf{n} & Number of measurements.\\
\textbf{mis} & Number of missing values.\\
\textbf{m} & Mean values.\\
\textbf{md} & Median of values.\\
\textbf{sd} & Standard deviation of values.\\
\textbf{mad} & Median average deviation of values.\\
\textbf{min/max} & Min and max of values.\\
\textbf{trend} & Slope of a regression line through values by time.\\
\bottomrule
\end{tabular}
\end{table}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{describe}\NormalTok{(exampleABC)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Describe Single-Case Data

       Marie Rosalind  Lise
Design A-B-C    A-B-C A-B-C
n.A       10       15    20
n.B       10        8     7
n.C       10        7     3
mis.A      0        0     0
mis.B      0        0     0
mis.C      0        0     0

          Marie Rosalind    Lise
m.A      52.000   52.267  52.350
m.B      72.100   73.250  73.571
m.C      68.000   66.429  71.333
md.A       53.5     52.0    52.0
md.B       72.5     72.0    73.0
md.C         69       68      76
sd.A      8.287    8.146  10.869
sd.B     11.367   13.134  10.644
sd.C     12.702   10.486  21.385
mad.A    11.119    7.413  10.378
mad.B    10.378   10.378  16.309
mad.C    17.791   11.861  20.756
min.A        39       37      35
min.B        47       54      60
min.C        51       52      48
max.A        63       65      74
max.B        85       97      87
max.C        87       78      90
trend.A  -1.915    0.500  -0.088
trend.B  -0.612    0.643   1.929
trend.C  -0.194   -2.929 -14.000
\end{verbatim}

The resulting table could be exported into a csv file to be used in other software (e.g., to inserted in a word processing document). Therefore, first write the results of the \texttt{describe} command into an R object and then use the \texttt{write.csv} (or \texttt{write.csv2} for a German OS system setup) to export the \texttt{descriptives} element of the object.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# write the results into a new R object named \textasciigrave{}res\textasciigrave{}}
\NormalTok{res }\OtherTok{\textless{}{-}} \FunctionTok{describe}\NormalTok{(exampleABC)}
\CommentTok{\# create a new file containing the descriptives on your harddrive}
\FunctionTok{write.csv}\NormalTok{(res}\SpecialCharTok{$}\NormalTok{descriptives, }\AttributeTok{file =} \StringTok{"descriptive data.csv"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

The file is written to the currently active working directory. If you are not sure where that is, type \texttt{getwd()} (you can use the \texttt{setwd()} command to define a different working directory. To get further details type \texttt{help(setwd)} into R).

\begin{rmdnote}
\textbf{Conflicting function names}\\
Sometimes R packages include the same function names. For example, the
\texttt{describe()} function is also part of the \texttt{psych} package.
Now, if you have loaded the \texttt{psych} package with
\texttt{library(psych)} after \texttt{scan} the \texttt{describe()}
function of scan will be masked (\texttt{describe()} would now call the
corresponding function of the \texttt{psych} package).\\
There are two solutions to this problem:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  activate the \texttt{psych} library before the \texttt{scan} library
  (now the psych \texttt{describe()} function will be masked) or
\item
  include the package name into the function call with the prefix
  \texttt{scan::}: \texttt{scan::describe()}.
\end{enumerate}
\end{rmdnote}

\hypertarget{autoregression-and-trendanalyses}{%
\section{Autoregression and trendanalyses}\label{autoregression-and-trendanalyses}}

The \texttt{autocorr} function calculates autocorrelations within each phase and across all phases. The \texttt{lag.max} argument defines the lag up to which the autocorrelation will be computed.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{autocorr}\NormalTok{(exampleABC, }\AttributeTok{lag.max =} \DecValTok{4}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Autocorrelations

Marie 
 Phase Lag 1 Lag 2 Lag 3 Lag 4
     A  0.29 -0.11  0.10  0.12
     B -0.28 -0.10 -0.14 -0.09
     C  0.00 -0.33 -0.14 -0.25
   all  0.21  0.10  0.25  0.12

Rosalind 
 Phase Lag 1 Lag 2 Lag 3 Lag 4
     A  0.37 -0.29 -0.33 -0.34
     B -0.34  0.24 -0.40  0.04
     C -0.07 -0.32  0.27  0.02
   all  0.49  0.38  0.22  0.17

Lise 
 Phase Lag 1 Lag 2 Lag 3 Lag 4
     A  0.04 -0.32 -0.05 -0.09
     B -0.63  0.50 -0.40  0.31
     C -0.38 -0.12    NA    NA
   all  0.33  0.36  0.23  0.27
\end{verbatim}

The \texttt{trend} function provides an overview of linear trends in single-case data. By default, it gives you the intercept and slope of a linear and a squared regression of measurement-time on scores. Models are computed separately for each phase and across all phases. For a more advanced application, you can add regression models using the R specific formula class.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Simple example}
\FunctionTok{trend}\NormalTok{(exampleABC[}\DecValTok{1}\NormalTok{])}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Trend for each phase

            Intercept      B   Beta
Linear.ALL     55.159  0.612  0.392
Linear.A       60.618 -1.915 -0.700
Linear.B       74.855 -0.612 -0.163
Linear.C       68.873 -0.194 -0.046
Squared.ALL    59.135  0.017  0.330
Squared.A      57.937 -0.208 -0.712
Squared.B      73.217 -0.039 -0.098
Squared.C      68.490 -0.017 -0.038

Note. Measurement-times start at 0  for each phase
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Complex example}
\FunctionTok{trend}\NormalTok{(exampleAB}\SpecialCharTok{$}\NormalTok{Johanna, }\AttributeTok{offset =} \DecValTok{0}\NormalTok{, }
        \AttributeTok{model =} \FunctionTok{c}\NormalTok{(}\StringTok{"Cubic"} \OtherTok{=}\NormalTok{ values }\SpecialCharTok{\textasciitilde{}} \FunctionTok{I}\NormalTok{(mt}\SpecialCharTok{\^{}}\DecValTok{3}\NormalTok{), }\StringTok{"Log Time"} \OtherTok{=}\NormalTok{ values }\SpecialCharTok{\textasciitilde{}} \FunctionTok{log}\NormalTok{(mt))}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Trend for each phase

             Intercept      B   Beta
Linear.ALL      50.484  1.787  0.908
Linear.A        54.300  0.100  0.066
Linear.B        61.133  1.625  0.813
Squared.ALL     57.879  0.079  0.871
Squared.A       54.747 -0.013 -0.054
Squared.B       66.343  0.094  0.775
Cubic.ALL       60.886  0.004  0.816
Cubic.A         54.959 -0.008 -0.169
Cubic.B         68.368  0.006  0.732
Log Time.ALL    43.532 12.149  0.848
Log Time.A      54.032  0.593  0.156
Log Time.B      57.300  9.051  0.791

Note. Measurement-times start at 1  for each phase
\end{verbatim}

\hypertarget{missing-values}{%
\section{Missing values}\label{missing-values}}

There are two kinds of missing values in single-case data series. First, missings that were explicitly recorded as \texttt{NA} and assigned to a phase and measurement-time as in the following example:

\begin{verbatim}
scdf(c(5, 3, 4, 6, 8, 7, 9, 7, NA, 6), phase_design = c(A = 4, B = 6))
\end{verbatim}

The second type of missing occurs when there are gaps between measurement-times that are not explicitly coded as in the following example:

\begin{verbatim}
scdf(c(5, 3, 4, 6, 8, 7, 9, 7, 6), phase_design = c(A = 4, B = 5), 
     mt = c(1, 2, 3, 4, 5, 6, 7, 8, 10))
\end{verbatim}

In both cases, missing values pose a threat to the internal validity of overlap indices. Randomization tests are more robust against the first type of missing values but are affected by the second type. Regression approaches are less impacted by both types as they take the interval between measurement-times into account.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{case1 }\OtherTok{\textless{}{-}} \FunctionTok{scdf}\NormalTok{(}\FunctionTok{c}\NormalTok{(}\DecValTok{3}\NormalTok{,}\DecValTok{6}\NormalTok{,}\DecValTok{2}\NormalTok{,}\DecValTok{4}\NormalTok{,}\DecValTok{3}\NormalTok{,}\DecValTok{5}\NormalTok{,}\DecValTok{2}\NormalTok{,}\DecValTok{6}\NormalTok{,}\DecValTok{3}\NormalTok{,}\DecValTok{2}\NormalTok{, }\DecValTok{6}\NormalTok{,}\DecValTok{7}\NormalTok{,}\DecValTok{5}\NormalTok{,}\DecValTok{8}\NormalTok{,}\DecValTok{6}\NormalTok{,}\DecValTok{7}\NormalTok{,}\DecValTok{4}\NormalTok{,}\DecValTok{8}\NormalTok{,}\DecValTok{5}\NormalTok{,}\DecValTok{6}\NormalTok{), }
              \AttributeTok{phase\_design =} \FunctionTok{c}\NormalTok{(}\AttributeTok{A =} \DecValTok{10}\NormalTok{, }\AttributeTok{B =} \DecValTok{10}\NormalTok{), }\AttributeTok{name =} \StringTok{"no NA"}\NormalTok{)}
\NormalTok{case2 }\OtherTok{\textless{}{-}} \FunctionTok{scdf}\NormalTok{(}\FunctionTok{c}\NormalTok{(}\DecValTok{3}\NormalTok{,}\DecValTok{6}\NormalTok{,}\DecValTok{2}\NormalTok{,}\DecValTok{4}\NormalTok{,}\DecValTok{3}\NormalTok{,}\DecValTok{5}\NormalTok{,}\DecValTok{2}\NormalTok{,}\ConstantTok{NA}\NormalTok{,}\DecValTok{3}\NormalTok{,}\DecValTok{2}\NormalTok{, }\DecValTok{6}\NormalTok{,}\DecValTok{7}\NormalTok{,}\DecValTok{5}\NormalTok{,}\DecValTok{8}\NormalTok{,}\DecValTok{6}\NormalTok{,}\ConstantTok{NA}\NormalTok{,}\DecValTok{4}\NormalTok{,}\DecValTok{8}\NormalTok{,}\DecValTok{5}\NormalTok{,}\DecValTok{6}\NormalTok{), }
              \AttributeTok{phase\_design =} \FunctionTok{c}\NormalTok{(}\AttributeTok{A =} \DecValTok{10}\NormalTok{, }\AttributeTok{B =} \DecValTok{10}\NormalTok{), }\AttributeTok{name =} \StringTok{"NAs"}\NormalTok{)}
\NormalTok{case3 }\OtherTok{\textless{}{-}} \FunctionTok{fill\_missing}\NormalTok{(case2)}
\FunctionTok{names}\NormalTok{(case3) }\OtherTok{\textless{}{-}} \StringTok{"interpolated NAs"}
\NormalTok{ex }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(case1, case2, case3)}
\FunctionTok{plot}\NormalTok{(ex)}
\end{Highlighting}
\end{Shaded}

\includegraphics{scan-book_files/figure-latex/fillmissing_example-1.pdf}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{overlap}\NormalTok{(ex)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Overlap Indices

Comparing phase 1 against phase 2 

             no NA  NAs interpolated NAs
Design         A-B  A-B              A-B
PND             40   33               30
PEM            100  100              100
PET            100  100              100
NAP             88   91               92
NAP rescaled    77   83               83
PAND            72   81               80
Tau_U         0.45 0.51             0.50
Base_Tau      0.59 0.64             0.64
Diff_mean     2.60 2.78             2.75
Diff_trend    0.02 0.11             0.12
SMD           1.65 1.96             2.02
Hedges_g      1.71 1.90             1.96
\end{verbatim}

\hypertarget{outlieranalysis}{%
\section{Outlieranalysis}\label{outlieranalysis}}

\emph{scan} provides several methods for analyzing outliers. All of them are implemented in the \texttt{outliers} function. Available methods are the \textbf{standard deviation}, \textbf{mean average deviation}, \textbf{confidence intervals}, and \textbf{Cook's distance}. The criteria argument takes a vector with two information, the first defines the analyzing method (``SD'', ``MAD'', CI'', ``Cook'') and the second the criteria. For ``SD'' the criteria is the number of standard deviations (\textbf{sd}) from the mean of each phase for which a value is not considered to be an outlier. For example, \texttt{criteria\ =\ c("SD",2)} would identify every value exceeding two \textbf{sd} above or below the mean as an outlier whereas \textbf{sd} and mean refer to phase of a value. As this might be misleading particularly for small samples Iglewicz and Hoaglin \citet{iglewicz_how_1993} recommend the use the much more robust median average deviation (\textbf{MAD}) instead. The \textbf{MAD} is is constructed similar to the \textbf{sd} but uses the median instead of the mean. Multiplying the \textbf{MAD} by 1.4826 approximates the \textbf{sd} in a normal distributed sample. This corrected MAD is applied in the \texttt{outlier} function. A deviation of 3.5 times the corrected \textbf{MAD} from the median is suggested to be an outlier. To use this criterion set \texttt{criteria\ =\ c("MAD",\ 3.5)}. \texttt{criteria\ =\ c("CI",\ 0.95)} takes exceeding the 95\% confidence interval as the criteria for outliers. The Cook's distance method for calculation outliers can be applied with a strict AB-phase design. in that case, the Cook's distance analyses are based on a piecewise-regression model. Most commonly, Cook's distance exceeding 4/n is used as a criteria. This could be implemented setting `criteria = c(``Cook'',``4/n'').

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{outlier}\NormalTok{(exampleABC\_outlier, }\AttributeTok{criteria =} \FunctionTok{c}\NormalTok{(}\StringTok{"MAD"}\NormalTok{, }\FloatTok{3.5}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Outlier Analysis for Single-Case Data

Criteria: Exceeds 3.5 Mean Average Deviations

$Bernadette
  phase md mad   lower    upper
1     A 57   9 10.2981 103.7019
2     B 76   7 39.6763 112.3237
3     C 69  12  6.7308 131.2692

$Penny
  phase md mad   lower    upper
1     A 52   6 20.8654  83.1346
2     B 74  10 22.1090 125.8910
3     C 68   8 26.4872 109.5128

$Amy
  phase md mad   lower    upper
1     A 54   9  7.2981 100.7019
2     B 73  11 15.9199 130.0801
3     C 76  14  3.3526 148.6474

Case Bernadette : Dropped 3 
Case Penny : Dropped 2 
Case Amy : Dropped 3 
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Visualizing outliers with the plot function}
\NormalTok{res }\OtherTok{\textless{}{-}} \FunctionTok{outlier}\NormalTok{(exampleABC\_outlier, }\AttributeTok{criteria =} \FunctionTok{c}\NormalTok{(}\StringTok{"MAD"}\NormalTok{, }\FloatTok{3.5}\NormalTok{))}
\FunctionTok{plot}\NormalTok{(exampleABC\_outlier, }\AttributeTok{marks =}\NormalTok{ res, }\AttributeTok{style =} \StringTok{"annotate"}\NormalTok{, }\AttributeTok{ylim =} \FunctionTok{c}\NormalTok{(}\DecValTok{40}\NormalTok{,}\DecValTok{160}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\includegraphics{scan-book_files/figure-latex/outlier-1.pdf}

\hypertarget{smoothing-data}{%
\section{Smoothing data}\label{smoothing-data}}

The \texttt{smooth\_cases} function provides procedures to smooth single-case data and eliminate noise. A moving average function (mean- or median-based) replaces each data point by the average of the surrounding data points step-by-step. A \emph{lag} defines the number of measurements before and after the calculation is based on. So a lag-1 will take the average of the proceeding and following value and lag-2 the average of the two proceeding and two following measurements. With a local regression function, each data point is regressed by its surrounding data points. Here, the proportion of measurements surrounding a value is usually defined. So an intensity of 0.2 will take the surrounding 20\% of data as the basis for a regression.\\
The function returns am scdf with smoothed data points.

\begin{Shaded}
\begin{Highlighting}[]
\DocumentationTok{\#\# Use the three different smoothing functions and compare the results}
\NormalTok{berta\_mmd }\OtherTok{\textless{}{-}} \FunctionTok{smooth\_cases}\NormalTok{(Huber2014}\SpecialCharTok{$}\NormalTok{Berta)}
\NormalTok{berta\_mmn }\OtherTok{\textless{}{-}} \FunctionTok{smooth\_cases}\NormalTok{(Huber2014}\SpecialCharTok{$}\NormalTok{Berta, }\AttributeTok{FUN =} \StringTok{"movingMean"}\NormalTok{)}
\NormalTok{berta\_lre }\OtherTok{\textless{}{-}} \FunctionTok{smooth\_cases}\NormalTok{(Huber2014}\SpecialCharTok{$}\NormalTok{Berta, }\AttributeTok{FUN =} \StringTok{"localRegression"}\NormalTok{)}
\NormalTok{new\_study }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(Huber2014}\SpecialCharTok{$}\NormalTok{Berta, berta\_mmd, berta\_mmn, berta\_lre)}
\FunctionTok{names}\NormalTok{(new\_study) }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\StringTok{"Original"}\NormalTok{, }\StringTok{"Moving Median"}\NormalTok{, }\StringTok{"Moving Mean"}\NormalTok{, }\StringTok{"Local Regression"}\NormalTok{)}
\FunctionTok{plot}\NormalTok{(new\_study, }\AttributeTok{style =} \StringTok{"grid2"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{scan-book_files/figure-latex/smooth_example-1.pdf}

\hypertarget{overlapping-indices}{%
\chapter{Overlapping indices}\label{overlapping-indices}}

\texttt{overlap} provides a table with some of the most important overlap indices for each case of an \emph{scdf}. For calculating overlap indicators is is important to know if a decrease or an increase of values is expected between phases. By default \texttt{overlap} assumes an increase in values. If the argument \texttt{decreasing\ =\ TRUE} is set, calculation will be based on the assumption of decreasing values.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{overlap}\NormalTok{(exampleAB)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Overlap Indices

Comparing phase 1 against phase 2 

             Johanna Karolina  Anja
Design           A-B      A-B   A-B
PND              100       87    93
PEM              100      100   100
PET              100       93   100
NAP              100       97    98
NAP rescaled     100       93    96
PAND             100       90    90
Tau_U           0.77     0.78  0.64
Base_Tau        0.63     0.59  0.61
Diff_mean      19.53    21.67 20.47
Diff_trend      1.53     0.54  2.50
SMD             8.11     3.17  6.71
Hedges_g        2.35     2.26  2.87
\end{verbatim}

Overlap measures refer to a comparison of two phases within a single-case data-set. By default, \texttt{overlap} compares a Phase A to a Phase B. The \texttt{phases} argument is needed if the phases of the \emph{scdf} do not include phases named A and B or a comparison between other phases in wanted.\\
The \texttt{phases} argument takes a list with two elements. One element for each of the two phases that should be compared. The elements could contain either the name of the two phases or the number of the position within the \emph{scdf}. If you want to compare the first to the third phase you can set \texttt{phases\ =\ list(1,3)}. If the phases of your case are named `A', `B', and `C' you could alternatively set \texttt{phases\ =\ list("A","C")}.\\
It is also possible to compare a combination of several cases against a combination of other phases. Each of the two list-elements could contain more than one phase which are concatenated with the \texttt{c} command. For example if you have an ABAB-Design and like to compare the two A-phases against the two B-phases \texttt{phases\ =\ list(\ c(1,3),\ c(2,4)\ )} will do the trick.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{overlap}\NormalTok{(exampleA1B1A2B2, }\AttributeTok{phases =} \FunctionTok{list}\NormalTok{( }\FunctionTok{c}\NormalTok{(}\StringTok{"A1"}\NormalTok{,}\StringTok{"A2"}\NormalTok{), }\FunctionTok{c}\NormalTok{(}\StringTok{"B1"}\NormalTok{,}\StringTok{"B2"}\NormalTok{)))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Overlap Indices

Comparing phases A1 + A2 against phases B1 + B2 

                   Pawel      Moritz      Jannis
Design       A1-B1-A2-B2 A1-B1-A2-B2 A1-B1-A2-B2
PND                   55          78          71
PEM                  100         100         100
PET                  100         100         100
NAP                   94          97          98
NAP rescaled          89          94          97
PAND                  82          85          90
Tau_U               0.45        0.46        0.38
Base_Tau            0.65        0.68        0.68
Diff_mean          12.25       13.58       15.27
Diff_trend         -0.05        0.00       -0.54
SMD                 2.68        3.27        3.62
Hedges_g            2.07        2.72        2.98
\end{verbatim}

\hypertarget{standardized-mean-differences}{%
\section{Standardized mean differences}\label{standardized-mean-differences}}

Standardized mean differences can be calculated in various ways. They refer to the difference in the means of two phases. The \texttt{smd} function provides an overview of the most common parameters for each single-case:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{smd}\NormalTok{(exampleAB\_score)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Standardized mean differences

                            Christiano Lionel Neymar
mA                                2.70   3.10   2.30
mB                               15.35  15.35  15.60
sdA                               1.42   1.59   1.49
sdB                               2.13   1.60   2.19
sd cohen                          1.81   1.60   1.87
sd hedges                         1.93   1.60   1.99
Glass' delta                      8.92   7.68   8.90
Hedges' g                         6.54   7.67   6.68
Hedges' g correction              6.37   7.46   6.50
Hedges' g durlak correction       6.15   7.21   6.28
Cohen's d                         6.98   7.67   7.10
\end{verbatim}

\hypertarget{percentage-non-overlapping-data-pnd}{%
\section{Percentage non-overlapping data (PND)}\label{percentage-non-overlapping-data-pnd}}

The percentage of non-overlapping data (PND) effect size measure was described by \citet{scruggs_quantitative_1987} . It is the percentage of all data-points of the second phase of a single-case study exceeding the maximum value of the first phase. In case you have a study where you expect a decrease of values in the second phase, PND is calculated as the percentage of data-point of the second phase below the minimum of the first phase.

\begin{figure}
\centering
\includegraphics{scan-book_files/figure-latex/unnamed-chunk-33-1.pdf}
\caption{\label{fig:unnamed-chunk-33}Illustration of PND. PND is 60\% as 9 out of 15 datapoints of phase B are higher than the maximum of phase A.}
\end{figure}

The function \texttt{pnd} provides the PND for each case as well as the mean of all PNDs of that \emph{scdf}. When you expect decreasing values set \texttt{decreasing\ =\ TRUE}. When there are more than two phases or phases are not named A and B, use the \texttt{phases} argument as described at the beginning of this chapter.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{pnd}\NormalTok{(exampleAB)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Percent Non-Overlapping Data

     Case    PND Total Exceeds
  Johanna   100%    15      15
 Karolina 86.67%    15      13
     Anja 93.33%    15      14

Mean  : 93.33 %
\end{verbatim}

\hypertarget{percentage-exceeding-the-median-pem}{%
\section{Percentage exceeding the median (PEM)}\label{percentage-exceeding-the-median-pem}}

The pem function returns the percentage of phase B data exceeding the phase A median. Additionally, a binomial test against a 50/50 distribution is computed. Different measures of central tendency can be addressed for alternative analyses.

\begin{figure}
\centering
\includegraphics{scan-book_files/figure-latex/unnamed-chunk-36-1.pdf}
\caption{\label{fig:unnamed-chunk-36}Illustration of PEM. PEM is 75\% as 9 out of 12 datapoints of phase B are higher than the median of phase A.}
\end{figure}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{pem}\NormalTok{(exampleAB)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Percent Exceeding the Median

         PEM positives total binom.p
Johanna  100        15    15       0
Karolina 100        15    15       0
Anja     100        15    15       0

Alternative hypothesis: true probability > 50%
\end{verbatim}

\hypertarget{percentage-exceeding-the-regression-trend-pet}{%
\section{Percentage exceeding the regression trend (PET)}\label{percentage-exceeding-the-regression-trend-pet}}

The pet function provides the percentage of phase B data points exceeding the prediction based on the phase A trend. A binomial test against a 50/50 distribution is computed. Furthermore, the percentage of phase B data points exceeding the upper (or lower) 95 percent confidence interval of the predicted progress is computed.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{pet}\NormalTok{(exampleAB)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Percent Exceeding the Trend

N cases =  3 

             PET binom.p  PET CI
Johanna  100.000       0  86.667
Karolina  93.333       0   0.000
Anja     100.000       0 100.000

Binom.test: alternative hypothesis: true probability > 50%
PET CI: Percent of values greater than upper 95% confidence threshold (greater 1.645*se above predicted value)
\end{verbatim}

\begin{figure}
\centering
\includegraphics{scan-book_files/figure-latex/unnamed-chunk-40-1.pdf}
\caption{\label{fig:unnamed-chunk-40}Illustration of PET. PET is 66.7\% as 10 out of 15 datapoints of phase B are higher than the projected trend-line of phase A.}
\end{figure}

\hypertarget{percentage-of-all-non-overlapping-data-pand}{%
\section{Percentage of all non-overlapping data (PAND)}\label{percentage-of-all-non-overlapping-data-pand}}

The \texttt{pand} function calculates the percentage of all non-overlapping data \citep{parker_percentage_2007}, an index to quantify a level increase (or decrease) in performance after the onset of an intervention. The argument \texttt{correction\ =\ TRUE} makes \texttt{pand} use a frequency matrix, which is corrected for ties. A tie is counted as the half of a measurement in both phases. Set \texttt{correction\ =\ FALSE} to use the uncorrected matrix, which is not recommended.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{pand}\NormalTok{(exampleAB)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Percentage of all non-overlapping data

PAND =  93.3 %
Φ =  0.822  ; Φ² =  0.676 

Number of cases: 3 
Total measurements: 60  (in phase A: 15; in phase B: 45)
n overlapping data per case: 0, 2, 2
Total overlapping data: n = 4 ; percentage = 6.7 

2 x 2 Matrix of proportions
    % expected
    A   B   total
%    A  21.7    3.3 25
real B  3.3 71.7    75
 total  25  75

2 x 2 Matrix of counts
    expected
    A   B   total
     A  13  2   15
real B  2   43  45
 total  15  45


Note. Matrix is corrected for ties

Correlation based analysis:

z = 6.316, p = 0.000, τ = 0.822 
\end{verbatim}

PAND indicates nonoverlap between phase A and B data (like PND), but uses all data and is therefore not based on one single (probably unrepresentative) datapoint. Furthermore, PAND allows the comparison of real and expected associations (Chi-square test) and estimation of the effect size Phi, which equals Pearsons r for dichotomous data. Thus, phi-Square is the amount of explained variance. The original procedure for computing PAND does not account for ambivalent datapoints (ties). The newer NAP overcomes this problem and has better precision-power \citep{parker_effect_2011}.

\hypertarget{nonoverlap-of-all-pairs-nap}{%
\section{Nonoverlap of all pairs (NAP)}\label{nonoverlap-of-all-pairs-nap}}

The \texttt{nap} function calculates the nonoverlap of all pairs \citep{parker_improved_2009}. NAP summarizes the overlap between all pairs of phase A and phase B data points. If an increase of phase B scores is expected, a non-overlapping pair has a higher phase B data point. The NAP equals number of pairs showing no overlap / number of pairs. Because NAP can only take values between 50 and 100 percent, a rescaled and therefore more intuitive NAP (0-100\%) is also displayed. NAP is equivalent to the the U-test and Wilcox rank sum test. Thus, a Wilcox test is conducted and reported for each case.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{nap}\NormalTok{(exampleAB)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Nonoverlap of All Pairs

     Case NAP Rescaled Pairs Positives Ties   W       p
  Johanna 100      100    75        75    0 0.0 0.00062
 Karolina  97       93    75        72    1 2.5 0.00129
     Anja  98       96    75        73    1 1.5 0.00095
\end{verbatim}

\hypertarget{tau-u}{%
\section{Tau-U}\label{tau-u}}

The \emph{Tau-U} statistic has been proposed by \citet{parker_combining_2011} and is one of the more broadly used approach for reporting effect sizes of single case data. Unfortunately, various and ambiguous implementations of Tau-U exist \citep{pustejovsky2016, brossart2018a}. The \texttt{tau\_u} function tries to cover several of these implementation. It takes a \emph{scdf} and returns Tau-U calculations for each single-case within that file. Additionally, an overall Tau-U value is calculated for all cases based on a meta-analysis.

Several arguments an be set to define how Tau-U should be calculated. By setting the argument \texttt{method\ =\ "parker"}, Tau-U is calculated as described in \citet{parker_combining_2011}. This procedure could lead to Tau-U values above 1 and below -1 which are difficult to interpret. \texttt{method\ =\ "complete}, which is the default, applies a correction that keeps the values within the -1 to 1 range and should be more appropriate. In the original method proposed by \citet{parker_combining_2011} data, calculations are based on Kendall's Tau A which does not correct for ties. Alternatively, Kendall's Tau B has a correction for Tau in the presence of ties. The \texttt{tau\_method}` can be set to decide on the tau method to use \texttt{"a"} for Kendall's Tau A and \texttt{"b"}` for Kendall's Tau B.

Here is an example with setting that reconstruct the values from the original example in \citet{parker2011} :

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{tau\_u}\NormalTok{(Parker2011, }\AttributeTok{method =} \StringTok{"parker"}\NormalTok{, }\AttributeTok{tau\_method =} \StringTok{"a"}\NormalTok{, }\AttributeTok{continuity\_correction =} \ConstantTok{FALSE}\NormalTok{, }\AttributeTok{ci =} \ConstantTok{NA}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Tau-U
Method: parker 
Applied Kendall's Tau-a

Case: Case1 
                            pairs pos neg ties  S  D   Tau CI lower CI upper
A vs. B                        20  17   1    2 16 20 0.800       NA       NA
Trend A                         6   4   1    1  3  6 0.500       NA       NA
Trend B                        10   8   1    1  7 10 0.700       NA       NA
A vs. B - Trend A              20  18   5    3 13 20 0.650       NA       NA
A vs. B + Trend B              30  25   2    3 23 30 0.767       NA       NA
A vs. B + Trend B - Trend A    36  26   6    4 20 36 0.556       NA       NA
                            SD_S VAR_S SE_Tau    Z     p
A vs. B                     8.16 66.67  0.408 1.96 0.050
Trend A                     2.94  8.67  0.491 1.02 0.308
Trend B                     4.08 16.67  0.408 1.71 0.086
A vs. B - Trend A           9.59 92.00  0.480 1.36 0.175
A vs. B + Trend B           9.59 92.00  0.320 2.40 0.016
A vs. B + Trend B - Trend A 9.59 92.00  0.266 2.09 0.037
\end{verbatim}

A different implementation of the method (provided at \href{http://www.singlecaseresearch.org/calculators/tau-u}{http://www.singlecaseresearch.org/calculators/tau-u)}) uses Kendall's Tau B:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{tau\_u}\NormalTok{(exampleAB}\SpecialCharTok{$}\NormalTok{Johanna, }\AttributeTok{method =} \StringTok{"parker"}\NormalTok{, }\AttributeTok{tau\_method =} \StringTok{"b"}\NormalTok{, }\AttributeTok{continuity\_correction =} \ConstantTok{FALSE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Tau-U
Method: parker 
Applied Kendall's Tau-b

Case: Johanna 
                            pairs pos neg ties   S   D   Tau CI lower CI upper
A vs. B                        75  75   0    0  75  75 1.000    0.401    1.599
Trend A                        10   5   5    0   0  10 0.000      NaN      NaN
Trend B                       105  87  17    1  70 104 0.670    0.291    1.049
A vs. B - Trend A              75  80   5    0  75 127 0.592    0.232    0.951
A vs. B + Trend B             180 162  17    1 145 184 0.786    0.462    1.111
A vs. B + Trend B - Trend A   190 167  22    1 145 189 0.765    0.447    1.084
                             SD_S VAR_S SE_Tau    Z     p
A vs. B                     22.91 525.0  0.306 3.27 0.001
Trend A                      4.08  16.7    NaN 0.00 1.000
Trend B                     20.21 408.3  0.193 3.46 0.001
A vs. B - Trend A           23.26 541.2  0.184 3.22 0.001
A vs. B + Trend B           30.53 932.4  0.166 4.75 0.000
A vs. B + Trend B - Trend A 30.81 949.0  0.163 4.71 0.000
\end{verbatim}

A different online calculator created by Rumen Manolov is available at \url{https://manolov.shinyapps.io/Overlap/} it applies an R code developed by Kevin Tarlow for caluclating Tau-U. This setting will replicated results from this approach:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{tau\_u}\NormalTok{(exampleAB}\SpecialCharTok{$}\NormalTok{Johanna, }\AttributeTok{method =} \StringTok{"complete"}\NormalTok{, }\AttributeTok{tau\_method =} \StringTok{"a"}\NormalTok{, }\AttributeTok{continuity\_correction =} \ConstantTok{FALSE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Tau-U
Method: complete 
Applied Kendall's Tau-a

Case: Johanna 
                            pairs pos neg ties   S   D   Tau CI lower CI upper
A vs. B                        75  75   0    0  75  75 1.000    0.401     1.60
Trend A                        10   5   5    0   0  10 0.000      NaN      NaN
Trend B                       105  87  17    1  70 105 0.667    0.289     1.04
A vs. B - Trend A              85  80   5    0  75  85 0.882    0.172     1.59
A vs. B + Trend B             180 162  17    1 145 180 0.806    0.470     1.14
A vs. B + Trend B - Trend A   190 167  22    1 145 190 0.763    0.445     1.08
                             SD_S VAR_S SE_Tau    Z     p
A vs. B                     22.91 525.0  0.306 3.27 0.001
Trend A                      4.08  16.7    NaN 0.00 1.000
Trend B                     20.21 408.3  0.192 3.46 0.001
A vs. B - Trend A           30.82 950.0  0.363 2.43 0.015
A vs. B + Trend B           30.82 950.0  0.171 4.70 0.000
A vs. B + Trend B - Trend A 30.82 950.0  0.162 4.70 0.000
\end{verbatim}

The standard return of the \texttt{tau\_u} function does not display all calculations. If you like to have more details, apply the \texttt{print} function with the additional argument \texttt{complete\ =\ TRUE}.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{tau\_u}\NormalTok{(exampleAB}\SpecialCharTok{$}\NormalTok{Johanna) }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{print}\NormalTok{(}\AttributeTok{complete =} \ConstantTok{TRUE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Tau-U
Method: complete 
Applied Kendall's Tau-b
95% CIs for tau are reported.

Case: Johanna 
                            pairs pos neg ties   S   D   Tau CI lower CI upper
A vs. B                        75  75   0    0  75  75 1.000    0.401    1.599
Trend A                        10   5   5    0   0  10 0.000      NaN      NaN
Trend B                       105  87  17    1  70 104 0.670    0.291    1.049
A vs. B - Trend A              85  80   5    0  75 127 0.592    0.232    0.951
A vs. B + Trend B             180 162  17    1 145 184 0.786    0.462    1.111
A vs. B + Trend B - Trend A   190 167  22    1 145 189 0.765    0.447    1.084
                             SD_S VAR_S SE_Tau    Z     p
A vs. B                     22.91 525.0  0.306 3.27 0.001
Trend A                      4.08  16.7    NaN 0.00 1.000
Trend B                     20.21 408.3  0.193 3.46 0.001
A vs. B - Trend A           23.26 541.2  0.184 3.22 0.001
A vs. B + Trend B           30.53 932.4  0.166 4.75 0.000
A vs. B + Trend B - Trend A 30.81 949.0  0.163 4.71 0.000
\end{verbatim}

When you provide multiple single-cases to the \texttt{tau-u}` function, it will calculate a Tau-U table for each case and an overall calculation. The overall Tau-U value is the average of all Tau-U values weighted by their standard error. You can choose between a random- and a fixed-effect approach for the meta-analyses (\texttt{meta\_method\ =\ "random"} or \texttt{"fixed"}).

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{tau\_u}\NormalTok{(exampleAB)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Tau-U
Method: complete 
Applied Kendall's Tau-b
95% CIs for tau are reported.

Overall Tau-U
Meta-anlysis model: random effect

                       Model Tau_U     se CI lower CI upper    z        p
                     A vs. B 0.969 0.1772    0.622    1.316 5.47 4.54e-08
           A vs. B - Trend A 0.590 0.1064    0.381    0.798 5.54 3.04e-08
           A vs. B + Trend B 0.740 0.0960    0.552    0.928 7.71 1.29e-14
 A vs. B + Trend B - Trend A 0.731 0.0942    0.546    0.915 7.75 9.09e-15

Case: Johanna 
                            pairs pos neg ties   S   D   Tau CI lower CI upper
A vs. B                        75  75   0    0  75  75 1.000    0.401    1.599
Trend A                        10   5   5    0   0  10 0.000      NaN      NaN
Trend B                       105  87  17    1  70 104 0.670    0.291    1.049
A vs. B - Trend A              85  80   5    0  75 127 0.592    0.232    0.951
A vs. B + Trend B             180 162  17    1 145 184 0.786    0.462    1.111
A vs. B + Trend B - Trend A   190 167  22    1 145 189 0.765    0.447    1.084
                             SD_S VAR_S SE_Tau    Z     p
A vs. B                     22.91 525.0  0.306 3.27 0.001
Trend A                      4.08  16.7    NaN 0.00 1.000
Trend B                     20.21 408.3  0.193 3.46 0.001
A vs. B - Trend A           23.26 541.2  0.184 3.22 0.001
A vs. B + Trend B           30.53 932.4  0.166 4.75 0.000
A vs. B + Trend B - Trend A 30.81 949.0  0.163 4.71 0.000

Case: Karolina 
                            pairs pos neg ties   S     D   Tau CI lower
A vs. B                        75  72   2    1  70  74.5 0.940    0.337
Trend A                        10   5   5    0   0  10.0 0.000      NaN
Trend B                       105  91  13    1  78 104.5 0.746    0.367
A vs. B - Trend A              85  77   7    1  70 126.4 0.554    0.193
A vs. B + Trend B             180 163  15    2 148 184.0 0.805    0.479
A vs. B + Trend B - Trend A   190 168  20    2 148 189.0 0.783    0.464
                            CI upper  SD_S VAR_S SE_Tau    Z     p
A vs. B                        1.542 22.91 525.0  0.308 3.06 0.002
Trend A                          NaN  4.08  16.7    NaN 0.00 1.000
Trend B                        1.125 20.21 408.3  0.193 3.86 0.000
A vs. B - Trend A              0.914 23.25 540.8  0.184 3.01 0.003
A vs. B + Trend B              1.130 30.52 931.4  0.166 4.85 0.000
A vs. B + Trend B - Trend A    1.102 30.79 948.0  0.163 4.81 0.000

Case: Anja 
                            pairs pos neg ties   S     D    Tau CI lower
A vs. B                        75  73   1    1  72  74.5  0.966   0.3636
Trend A                        10   2   8    0  -6  10.0 -0.600  -1.4002
Trend B                       105  71  29    5  42 102.5  0.410   0.0234
A vs. B - Trend A              85  81   3    1  78 125.1  0.624   0.2600
A vs. B + Trend B             180 144  30    6 114 182.0  0.626   0.2985
A vs. B + Trend B - Trend A   190 152  32    6 120 187.0  0.642   0.3198
                            CI upper  SD_S VAR_S SE_Tau     Z     p
A vs. B                        1.569 22.91 525.0  0.308  3.14 0.002
Trend A                        0.200  4.08  16.7  0.408 -1.47 0.142
Trend B                        0.796 20.21 408.3  0.197  2.08 0.038
A vs. B - Trend A              0.987 23.21 538.6  0.186  3.36 0.001
A vs. B + Trend B              0.954 30.45 927.0  0.167  3.74 0.000
A vs. B + Trend B - Trend A    0.964 30.71 943.3  0.164  3.91 0.000
\end{verbatim}

\hypertarget{baseline-corrected-tau}{%
\section{Baseline corrected tau}\label{baseline-corrected-tau}}

This method has been proposed by \citet{tarlowImprovedRankCorrelation2016a}. The baseline data are checked for a significant autocorrelation (based on Kendalls Tau). If so, a non-parameteric Theil-Sen regression is applied for the baseline data where the dependent values are regressed on the measurement time. The resulting slope information is then used to predict data of the B-phase. The dependent variable is now corrected for this baseline trend and the residuals of the Theil-Sen regression are taken for further calculations. Finally, Kendalls tau is calculated for the dependent variable and the dichotomous phase variable. The function here provides two extensions to this procedure: The more accurate Siegel repeated median regression is applied when \texttt{repeated\ =\ TRUE} \citep{siegelRobustRegressionUsing1982} and a continuity correction is applied when \texttt{continuity\ =\ TRUE} (both are the default settings).

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{dat }\OtherTok{\textless{}{-}} \FunctionTok{scdf}\NormalTok{(}\FunctionTok{c}\NormalTok{(}\AttributeTok{A =} \DecValTok{33}\NormalTok{,}\DecValTok{25}\NormalTok{,}\DecValTok{17}\NormalTok{,}\DecValTok{25}\NormalTok{,}\DecValTok{14}\NormalTok{,}\DecValTok{13}\NormalTok{,}\DecValTok{15}\NormalTok{, }\AttributeTok{B =} \DecValTok{15}\NormalTok{,}\DecValTok{16}\NormalTok{,}\DecValTok{16}\NormalTok{,}\DecValTok{5}\NormalTok{,}\DecValTok{7}\NormalTok{,}\DecValTok{9}\NormalTok{,}\DecValTok{6}\NormalTok{,}\DecValTok{5}\NormalTok{,}\DecValTok{3}\NormalTok{,}\DecValTok{3}\NormalTok{,}\DecValTok{8}\NormalTok{,}\DecValTok{11}\NormalTok{,}\DecValTok{7}\NormalTok{))}
\FunctionTok{corrected\_tau}\NormalTok{(dat)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Baseline corrected tau

Method: Theil-Sen regression
Continuity correction not applied.

                           tau     z     p
Baseline autocorrelation -0.68 -2.13  <.05
Uncorrected tau          -0.57 -2.94  <.01
Baseline corrected tau    0.70  3.61 <.001

Baseline correction should be applied.
\end{verbatim}

Here is a replication of an example provided by \citet{tarlowImprovedRankCorrelation2016a} :

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{dat }\OtherTok{\textless{}{-}} \FunctionTok{scdf}\NormalTok{(}\FunctionTok{c}\NormalTok{(}\AttributeTok{A =} \DecValTok{33}\NormalTok{, }\DecValTok{25}\NormalTok{, }\DecValTok{17}\NormalTok{, }\DecValTok{25}\NormalTok{, }\DecValTok{14}\NormalTok{, }\DecValTok{13}\NormalTok{,}\DecValTok{14}\NormalTok{, }\AttributeTok{B =} \DecValTok{14}\NormalTok{, }\DecValTok{15}\NormalTok{, }\DecValTok{15}\NormalTok{, }\DecValTok{4}\NormalTok{, }\DecValTok{6}\NormalTok{, }\DecValTok{9}\NormalTok{, }\DecValTok{5}\NormalTok{ ,}\DecValTok{4}\NormalTok{ ,}\DecValTok{2}\NormalTok{ ,}\DecValTok{2}\NormalTok{ ,}\DecValTok{8}\NormalTok{, }\DecValTok{11}\NormalTok{ ,}\DecValTok{7}\NormalTok{))}
\FunctionTok{corrected\_tau}\NormalTok{(dat, }\AttributeTok{repeated =} \ConstantTok{FALSE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Baseline corrected tau

Method: Theil-Sen regression
Continuity correction not applied.

                           tau     z     p
Baseline autocorrelation -0.75 -2.31  <.05
Uncorrected tau          -0.58 -2.98  <.01
Baseline corrected tau    0.69  3.57 <.001

Baseline correction should be applied.
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{model }\OtherTok{\textless{}{-}} \FunctionTok{plm}\NormalTok{(dat)}
\end{Highlighting}
\end{Shaded}

\hypertarget{reliable-change-index}{%
\section{Reliable change index}\label{reliable-change-index}}

Basically, the reliable change index (rci) depicts if a post-test is above a pre-test value. Based on the reliability of the measurements and the standard-deviation the standard error is calculated. The mean difference between phase-A and phase-B is divided by the standard-error. Several authors proposed refined methods for calculating the rci.

The \texttt{rci} function computes three indices of reliable change \citep{wise_methods_2004} and corresponding descriptive statistics.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{rci}\NormalTok{(exampleAB}\SpecialCharTok{$}\NormalTok{Johanna, }\AttributeTok{rel =} \FloatTok{0.8}\NormalTok{, }\AttributeTok{graph =} \ConstantTok{TRUE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{scan-book_files/figure-latex/unnamed-chunk-52-1.pdf}

\begin{verbatim}
Reliable Change Index

Mean Difference =  19.53333 
Standardized Difference =  1.678301 

Descriptives:
         n     mean       SD       SE
A-Phase  5 54.60000 2.408319 1.077033
B-Phase 15 74.13333 8.943207 3.999524

Reliability =  0.8 

95 % Confidence Intervals:
           Lower    Upper
A-Phase 52.48905 56.71095
B-Phase 66.29441 81.97226

Reliable Change Indices:
                             RCI
Jacobson et al.         18.13624
Christensen and Mendoza 12.82426
Hageman and Arrindell   18.49426
\end{verbatim}

\hypertarget{piecewise-linear-regressions}{%
\chapter{Piecewise linear regressions}\label{piecewise-linear-regressions}}

In a piecewise regression analysis (sometimes called segmented regression) a data-set is split at a specific break point and regression parameters (intercept and slopes) are calculated separately for data before and after the break point. This is done because we assume that at the break point a qualitative change happens affecting intercept and slope. This approach lends itself perfectly to analyze single-case data which are from a statistical point of view time-series data segmented into phases. A general model for single-case data based on the piecewise regression approach has been suggested by Huitema and McKean \citet{huitema_design_2000}. They refer to two-phase single-case designs with a pre-intervention phase containing some measurements before the start of the intervention (A-phase) and an intervention phase containing measurements beginning at the intervention's start and lasting throughout the intervention (B-phase).

In this model, four parameters predict the outcome at a specific measurement point:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  The performance at the beginning of the study (\textbf{intercept}),
\item
  a developmental effect leading to a continuous increase throughout all measurements (\textbf{trend effect}),
\item
  an intervention effect leading to an immediate and constant increase in performance (l\textbf{evel effect}), and
\item
  a second intervention effect that evolves continuously with the beginning of the intervention (\textbf{slope effect}).
\end{enumerate}

\includegraphics{scan-book_files/figure-latex/figure_plm-1.pdf}

\emph{scan} provides an implementation based on this piecewise regression approach. Though the original model is extended by several factors:

\begin{itemize}
\tightlist
\item
  multiple phase designs
\item
  additional (control) variables
\item
  autoregression modeling
\item
  logistic, binomial, and poisson distributed dependent variables and error terms
\item
  multivariate analyzes for analyzing the effect of an intervention on more than one outcome variable.
\end{itemize}

\hypertarget{the-basic-plm-function}{%
\section{The basic plm function}\label{the-basic-plm-function}}

The basic function for applying a regression analyzes to a single-case dataset is \texttt{plm}. This function analyzes one single-case. In its simplest way, \texttt{plm} takes one argument with an \emph{scdf} object and it returns a full piecewise regression analyzes.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{plm}\NormalTok{(exampleAB}\SpecialCharTok{$}\NormalTok{Johanna)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Piecewise Regression Analysis

Dummy model:  B&L-B 

Fitted a gaussian distribution.
F(3, 16) = 28.69; p = 0.000; R² = 0.843; Adjusted R² = 0.814

                   B   2.5%  97.5%    SE      t     p delta R²
Intercept     54.300 43.978 64.622 5.267 10.310 0.000         
Trend mt       0.100 -3.012  3.212 1.588  0.063 0.951   0.0000
Level phase B  6.333 -2.979 15.646 4.751  1.333 0.201   0.0174
Slope phase B  1.525 -1.642  4.692 1.616  0.944 0.359   0.0087

Autocorrelations of the residuals
 lag    cr
   1 -0.32
   2 -0.13
   3 -0.01

Formula: values ~ 1 + mt + phaseB + interB
\end{verbatim}

\hypertarget{dummy-model}{%
\subsection{Dummy model}\label{dummy-model}}

The \texttt{model} argument is used to code the \emph{dummy variable}. This \emph{dummy variable} is used to compute the slope and level effects of the \emph{phase} variable.\\
The \emph{phase} variable is categorical, identifying the phase of each measurement. Typically, categorical variables are implemented by means of dummy variables. In a piecewise regression model two phase effects have to be estimated: a level effect and a slope effect. The level effect is implemented quite straight forward: for each phase beginning with the second phase a new dummy variable is created with values of zero for all measurements except the measurements of the phase in focus where values of one are set.

\begin{tabular}{l|r|r}
\hline
phase & values & level\_B\\
\hline
A & 3 & 0\\
\hline
A & 6 & 0\\
\hline
A & 4 & 0\\
\hline
A & 7 & 0\\
\hline
B & 5 & 1\\
\hline
B & 3 & 1\\
\hline
B & 4 & 1\\
\hline
B & 6 & 1\\
\hline
B & 3 & 1\\
\hline
\end{tabular}

For estimating the \emph{slope effect} of each phase, another kind of dummy variables have to be created. Like the dummy variables for level effects the values are set to zero for all measurements except the ones of the phase in focus. Here, values start to increase with every measurement until the end of the phase.\\
Various suggestions have been made regarding the way in which these values increase. The \emph{B\&L-B} model starts with a one at the first measurement of the phase and increases with every measurement while the \emph{H-M} model starts with a zero.

\begin{tabular}{l|r|r|r|r}
\hline
phase & values & level & slope B\&L-M & slope H-M\\
\hline
A & 3 & 0 & 0 & 0\\
\hline
A & 6 & 0 & 0 & 0\\
\hline
A & 4 & 0 & 0 & 0\\
\hline
A & 7 & 0 & 0 & 0\\
\hline
B & 5 & 1 & 1 & 0\\
\hline
B & 3 & 1 & 2 & 1\\
\hline
B & 4 & 1 & 3 & 2\\
\hline
B & 6 & 1 & 4 & 3\\
\hline
B & 3 & 1 & 5 & 4\\
\hline
\end{tabular}

With single-case studies with more than two phases it gets a bit more complicated. Applying the a fore described models to three phases would result in a comparison of each phase to the first phase (usually the A Phase). That is, regression weights and significance tests will depict differences of each phase to the values of phase A. This might be OK depending on what you are interested in. But in a lot of cases we are more interested in analyzing the effects of a phase compared to the previous one.\\
This is achieved applying the \emph{JW} dummy model. In this model, the dummy variable for the level effect is set to zero for all phases preceding the phase in focus and set to one for all remaining measurements. Similar, the dummy variable for the slope effect is set to zero for all phases preceding the one in focus and starts with one for the first measurement of the target phase and increases until the last measurement of the case.

\begin{tabular}{l|r|r|r|r|r}
\hline
phase & values & level\_B & level\_C & slope\_B & slope\_C\\
\hline
A & 3 & 0 & 0 & 0 & 0\\
\hline
A & 6 & 0 & 0 & 0 & 0\\
\hline
A & 4 & 0 & 0 & 0 & 0\\
\hline
A & 7 & 0 & 0 & 0 & 0\\
\hline
B & 5 & 1 & 0 & 1 & 0\\
\hline
B & 3 & 1 & 0 & 2 & 0\\
\hline
B & 4 & 1 & 0 & 3 & 0\\
\hline
B & 6 & 1 & 0 & 4 & 0\\
\hline
B & 3 & 1 & 0 & 5 & 0\\
\hline
C & 7 & 1 & 1 & 6 & 1\\
\hline
C & 5 & 1 & 1 & 7 & 2\\
\hline
C & 6 & 1 & 1 & 8 & 3\\
\hline
C & 4 & 1 & 1 & 9 & 4\\
\hline
C & 8 & 1 & 1 & 10 & 5\\
\hline
\end{tabular}

\hypertarget{adjusting-the-model}{%
\subsection{Adjusting the model}\label{adjusting-the-model}}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{example }\OtherTok{\textless{}{-}} \FunctionTok{scdf}\NormalTok{(}
   \AttributeTok{values =} \FunctionTok{c}\NormalTok{(}\DecValTok{55}\NormalTok{, }\DecValTok{58}\NormalTok{, }\DecValTok{53}\NormalTok{, }\DecValTok{50}\NormalTok{, }\DecValTok{52}\NormalTok{, }\DecValTok{55}\NormalTok{, }\DecValTok{68}\NormalTok{, }\DecValTok{68}\NormalTok{, }\DecValTok{81}\NormalTok{, }\DecValTok{67}\NormalTok{, }\DecValTok{78}\NormalTok{, }\DecValTok{73}\NormalTok{, }\DecValTok{72}\NormalTok{, }\DecValTok{78}\NormalTok{, }\DecValTok{81}\NormalTok{, }\DecValTok{78}\NormalTok{, }\DecValTok{71}\NormalTok{, }\DecValTok{85}\NormalTok{, }\DecValTok{80}\NormalTok{, }\DecValTok{76}\NormalTok{),}
   \AttributeTok{phase\_design =} \FunctionTok{c}\NormalTok{(}\AttributeTok{A =} \DecValTok{5}\NormalTok{, }\AttributeTok{B =} \DecValTok{15}\NormalTok{)}
\NormalTok{)}

\FunctionTok{plm}\NormalTok{(example)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Piecewise Regression Analysis

Dummy model:  B&L-B 

Fitted a gaussian distribution.
F(3, 16) = 21.36; p = 0.000; R² = 0.800; Adjusted R² = 0.763

                   B   2.5%  97.5%    SE      t     p delta R²
Intercept     57.800 46.521 69.079 5.755 10.044 0.000         
Trend mt      -1.400 -4.801  2.001 1.735 -0.807 0.432   0.0081
Level phase B 14.467  4.291 24.642 5.192  2.786 0.013   0.0970
Slope phase B  2.500 -0.961  5.961 1.766  1.416 0.176   0.0250

Autocorrelations of the residuals
 lag    cr
   1 -0.28
   2  0.05
   3 -0.11

Formula: values ~ 1 + mt + phaseB + interB
\end{verbatim}

The piecewise regression reveals a significant level effect and two non significant effects for trend and slope. In a further analyses we would like to put the slope effect out of the equation. There are several ways to do this. The easiest way is the to set the \texttt{slope} argument to \texttt{FALSE}.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{plm}\NormalTok{(example, }\AttributeTok{slope =} \ConstantTok{FALSE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Piecewise Regression Analysis

Dummy model:  B&L-B 

Fitted a gaussian distribution.
F(2, 17) = 29.30; p = 0.000; R² = 0.775; Adjusted R² = 0.749

                   B   2.5%  97.5%    SE      t     p delta R²
Intercept     50.559 45.239 55.878 2.714 18.627 0.000         
Trend mt       1.014  0.364  1.664 0.332  3.057 0.007   0.1236
Level phase B 10.329  1.674 18.983 4.416  2.339 0.032   0.0724

Autocorrelations of the residuals
 lag    cr
   1 -0.07
   2  0.06
   3 -0.17

Formula: values ~ 1 + mt + phaseB
\end{verbatim}

In the resulting estimations the trend and level effects are now significant. The model estimated a trend effect of 1.01 points per measurement time and a level effect of 10.33 points. That is, with the beginning of the intervention (the B-phase) the score increases by 15.38 points (5 x 1.01 + 10.33).

\hypertarget{adding-additional-predictors}{%
\subsection{Adding additional predictors}\label{adding-additional-predictors}}

In more complex analyses additional predictors can be included in the piecewise regression model.

To do this, we have to change the regression formula `manually' by applying the \texttt{update} argument. The \texttt{update} argument allows to change the underlying regression formula. To add a new variable named for example \texttt{newVar}, set \texttt{update\ =\ .\textasciitilde{}.\ +\ newVar}. The \texttt{.\textasciitilde{}.} part takes the internally build formula and \texttt{+\ newVar} adds a variable named \texttt{newVar} to the equation.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{plm}\NormalTok{(exampleAB\_add, }\AttributeTok{update =}\NormalTok{ .}\SpecialCharTok{\textasciitilde{}}\NormalTok{. }\SpecialCharTok{+}\NormalTok{ cigarrets)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Piecewise Regression Analysis

Dummy model:  B&L-B 

Fitted a gaussian distribution.
F(4, 35) = 5.87; p = 0.001; R² = 0.402; Adjusted R² = 0.333

                            B   2.5%  97.5%    SE      t     p delta R²
Intercept              48.579 42.539 54.618 3.081 15.765 0.000         
Trend day               0.392 -0.221  1.005 0.313  1.253 0.218   0.0269
Level phase Medication  3.753 -2.815 10.321 3.351  1.120 0.270   0.0214
Slope phase Medication -0.294 -0.972  0.384 0.346 -0.850 0.401   0.0124
cigarrets              -0.221 -1.197  0.755 0.498 -0.443 0.660   0.0034

Autocorrelations of the residuals
 lag    cr
   1  0.20
   2 -0.19
   3 -0.16

Formula: wellbeing ~ day + phaseMedication + interMedication + cigarrets
\end{verbatim}

The formula has two parts divided by a tilde. Left of the tilde is the variable to be predicted and right of it the predictors. A \texttt{1} indicates the intercept, the variable \texttt{mt} estimates the trend effect, \texttt{phaseB} the level effect of the B-phase and the variable \texttt{interB} the slope effect of the B-phase. If \texttt{formula} is not explicitly defined it is set to \texttt{formula\ =\ values\ \textasciitilde{}\ 1\ +\ mt\ +\ phaseB\ +\ interB} (assuming an AB-design) to estimate the full piecewise regression model.

\hypertarget{to-be-written-modelling-autoregression}{%
\subsection{\texorpdfstring{\[to be written\] Modelling autoregression}{to be written Modelling autoregression}}\label{to-be-written-modelling-autoregression}}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{autocorr}\NormalTok{(Grosche2011)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Autocorrelations

Eva 
 Phase Lag 1 Lag 2 Lag 3
     A -0.04 -0.56 -0.01
     B  0.46  0.10  0.16
   all  0.48  0.13  0.24

Georg 
 Phase Lag 1 Lag 2 Lag 3
     A  0.51 -0.01 -0.13
     B -0.01 -0.02 -0.14
   all  0.40  0.15 -0.12

Olaf 
 Phase Lag 1 Lag 2 Lag 3
     A  0.64  0.29 -0.24
     B -0.45 -0.20  0.16
   all  0.35  0.12 -0.09
\end{verbatim}

\hypertarget{to-be-written-multivariate-piecewise-regression}{%
\section{\texorpdfstring{\[to be written\] Multivariate piecewise regression}{to be written Multivariate piecewise regression}}\label{to-be-written-multivariate-piecewise-regression}}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{mplm}\NormalTok{(exampleAB\_add, }\AttributeTok{dvar =} \FunctionTok{c}\NormalTok{(}\StringTok{"wellbeing"}\NormalTok{, }\StringTok{"depression"}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Multivariate piecewise linear model

Dummy model: B&L-B 

Coefficients: 
                       wellbeing depression
(Intercept)               48.038      4.086
day                        0.379      0.114
Level Phase Medication     3.863     -0.780
Slope Phase Medication    -0.275     -0.165

Formula: y ~ 1 + day + phaseMedication + interMedication

Type III MANOVA Tests: Pillai test statistic
                       Df test stat approx F num Df den Df Pr(>F)    
(Intercept)             1     0.897    152.1      2     35 <2e-16 ***
day                     1     0.055      1.0      2     35   0.38    
Level Phase Medication  1     0.038      0.7      2     35   0.50    
Slope Phase Medication  1     0.039      0.7      2     35   0.50    
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

The following variables were used in this analysis:
'wellbeing/ depression' as dependent variable, 'phase' as phase variable, and 'day' as measurement-time variable.
\end{verbatim}

\hypertarget{multilevel-plm-analyses}{%
\section{Multilevel plm analyses}\label{multilevel-plm-analyses}}

Multilevel analyses can take the piecewise-regression approach even further. It allows for

\begin{itemize}
\tightlist
\item
  analyzing the effects between phases for multiple single-cases at once
\item
  describing variability between subjects regarding these effects, and
\item
  introducing variables and factors for explaining the differences.
\end{itemize}

The basic function for applying a multilevel piecewise regression analysis is \texttt{hplm}. The \texttt{hplm} function is similar to the \texttt{plm} function, so I recommend that you get familar with \texttt{plm} before applying an \texttt{hplm}.

Here is a simple example:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{hplm}\NormalTok{(exampleAB\_50)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Hierarchical Piecewise Linear Regression

Estimation method ML 
Slope estimation method: B&L-B 
50 Cases

ICC = 0.287; L = 339.0; p = 0.000

Fixed effects (values ~ 1 + mt + phaseB + interB)

                   B    SE   df      t p
Intercept     47.819 1.517 1328 31.528 0
Trend mt       0.579 0.116 1328  5.006 0
Level phase B 13.136 0.584 1328 22.489 0
Slope phase B  0.902 0.119 1328  7.588 0

Random effects (~1 | case)

          EstimateSD
Intercept      9.970
Residual       5.285
\end{verbatim}

Here is an example inlcuding random slopes:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{hplm}\NormalTok{(exampleAB\_50, }\AttributeTok{random.slopes =} \ConstantTok{TRUE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Hierarchical Piecewise Linear Regression

Estimation method ML 
Slope estimation method: B&L-B 
50 Cases

ICC = 0.287; L = 339.0; p = 0.000

Fixed effects (values ~ 1 + mt + phaseB + interB)

                   B    SE   df      t p
Intercept     47.589 1.423 1328 33.432 0
Trend mt       0.622 0.113 1328  5.513 0
Level phase B 13.006 0.849 1328 15.328 0
Slope phase B  0.864 0.116 1328  7.427 0

Random effects (~1 + mt + phaseB + interB | case)

              EstimateSD
Intercept          9.297
Trend mt           0.100
Level phase B      4.543
Slope phase B      0.127
Residual           4.975
\end{verbatim}

\hypertarget{adding-additional-l2-variables}{%
\subsection{Adding additional L2-variables}\label{adding-additional-l2-variables}}

In some analyses researchers want to investigate whether attributes of the individuals contribute to the effectiveness of an intervention. For example might an intervention on mathematical abilities be less effective for student with a migration background due to too much language related material within the training. Such analyses can also be conducted with \emph{scan}. Therefore, we need to define a new \emph{data frame} including the relevant information of the subjects of the single-case studies we want to analyze. This \emph{data frame} consists of a variable labeled \texttt{case} which has to correspond to the case names of the \emph{scfd} and further variables with attributes of the subjects. To build a \emph{data frame} we can use the R function \texttt{data.frame}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{L2 }\OtherTok{\textless{}{-}} \FunctionTok{data.frame}\NormalTok{(}
  \AttributeTok{case =} \FunctionTok{c}\NormalTok{(}\StringTok{"Antonia"}\NormalTok{,}\StringTok{"Theresa"}\NormalTok{, }\StringTok{"Charlotte"}\NormalTok{, }\StringTok{"Luis"}\NormalTok{, }\StringTok{"Bennett"}\NormalTok{, }\StringTok{"Marie"}\NormalTok{), }
  \AttributeTok{age =} \FunctionTok{c}\NormalTok{(}\DecValTok{16}\NormalTok{, }\DecValTok{13}\NormalTok{, }\DecValTok{13}\NormalTok{, }\DecValTok{10}\NormalTok{, }\DecValTok{5}\NormalTok{, }\DecValTok{14}\NormalTok{), }
  \AttributeTok{sex =} \FunctionTok{c}\NormalTok{(}\StringTok{"f"}\NormalTok{,}\StringTok{"f"}\NormalTok{,}\StringTok{"f"}\NormalTok{,}\StringTok{"m"}\NormalTok{,}\StringTok{"m"}\NormalTok{,}\StringTok{"f"}\NormalTok{)}
\NormalTok{)}
\NormalTok{L2}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
       case age sex
1   Antonia  16   f
2   Theresa  13   f
3 Charlotte  13   f
4      Luis  10   m
5   Bennett   5   m
6     Marie  14   f
\end{verbatim}

Multilevel analyses require a high number of Level 2 units. The exact number depends on the complexity of the analyses, the size of the effects, the number of level 1 units, and the variability of the residuals. But surely we need at least about 30 level 2 units. In a single-case design that is, we need at least 30 single-cases (subjects) within the study. After setting the level 2 data frame we use the \texttt{data.l2} argument of the \texttt{hplm} function to include it into the analysis. Then we have to specify the regression function using the \texttt{update.fixed} argument. The level 2 variables can be added just like any other additional variable. For example, we have added a level 2 data-set with the two variables \texttt{sex} and \texttt{age}. \texttt{update} could be construed of the level 1 piecewise regression model \texttt{.\textasciitilde{}.} plus the additional level 2 variables of interest \texttt{+\ sex\ +\ age}. The complete argument is \texttt{update.fixed\ =\ .\textasciitilde{}.\ +\ sex\ +\ age}. This analyses will estimate a main effect of sex and age on the overall performance. In case we want to analyze an interaction between the intervention effects and for example the sex of the subject we have to add an additional interaction term (a cross-level interaction). An interaction is defined with a colon. So \texttt{sex:phase} indicates an interaction of sex and the level effect in the single case study. The complete formula now is \texttt{update.fixed\ =\ .\textasciitilde{}.\ +\ sex\ +\ age\ +\ sex:phase}.

\emph{scan} includes an example single-case study with 50 subjects \texttt{example50} and an additional level 2 data-set \texttt{example50.l2}. Here are the first 10 cases of \texttt{example50.l2}.

\begin{tabular}{l|l|r}
\hline
case & sex & age\\
\hline
Roman & m & 12\\
\hline
Brennen & m & 10\\
\hline
Ismael & m & 13\\
\hline
Donald & m & 11\\
\hline
Ricardo & m & 13\\
\hline
Izayah & m & 11\\
\hline
Ignacio & m & 12\\
\hline
Xavier & m & 12\\
\hline
Arian & m & 10\\
\hline
Paul & m & 10\\
\hline
\end{tabular}

Analyzing the data with \texttt{hplm} could look like this:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{hplm}\NormalTok{(exampleAB\_50, }\AttributeTok{data.l2 =}\NormalTok{ exampleAB\_50.l2, }\AttributeTok{update.fixed =}\NormalTok{ .}\SpecialCharTok{\textasciitilde{}}\NormalTok{. }\SpecialCharTok{+}\NormalTok{ sex }\SpecialCharTok{+}\NormalTok{ age)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Hierarchical Piecewise Linear Regression

Estimation method ML 
Slope estimation method: B&L-B 
50 Cases

ICC = 0.287; L = 339.0; p = 0.000

Fixed effects (values ~ mt + phaseB + interB + sex + age)

                   B     SE   df      t     p
Intercept     44.297 11.932 1328  3.713 0.000
Trend mt       0.581  0.116 1328  5.026 0.000
Level phase B 13.123  0.584 1328 22.456 0.000
Slope phase B  0.900  0.119 1328  7.569 0.000
sexm          -6.440  2.727   47 -2.362 0.022
age            0.603  1.073   47  0.562 0.577

Random effects (~1 | case)

          EstimateSD
Intercept      9.446
Residual       5.284
\end{verbatim}

\texttt{sex} is a factor with the levels \texttt{f} and \texttt{m}. So \texttt{sexm} is the effect of being male on the overall performance. \texttt{age} does not seem to have any effect. So we drop \texttt{age} out of the equation and add an interaction of sex and phase to see whether the \texttt{sex} effect is due to a weaker impact of the intervention on males.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{hplm}\NormalTok{(exampleAB\_50, }\AttributeTok{data.l2 =}\NormalTok{ exampleAB\_50.l2, }\AttributeTok{update.fixed =}\NormalTok{ .}\SpecialCharTok{\textasciitilde{}}\NormalTok{. }\SpecialCharTok{+}\NormalTok{ sex }\SpecialCharTok{+}\NormalTok{ sex}\SpecialCharTok{:}\NormalTok{phaseB)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Hierarchical Piecewise Linear Regression

Estimation method ML 
Slope estimation method: B&L-B 
50 Cases

ICC = 0.287; L = 339.0; p = 0.000

Fixed effects (values ~ mt + phaseB + interB + sex + phaseB:sex)

                        B    SE   df       t    p
Intercept          47.964 1.990 1327  24.097 0.00
Trend mt            0.609 0.109 1327   5.573 0.00
Level phase B      16.841 0.625 1327  26.965 0.00
Slope phase B       0.884 0.112 1327   7.868 0.00
sexm               -0.593 2.741   48  -0.216 0.83
Level phase B:sexm -7.732 0.609 1327 -12.699 0.00

Random effects (~1 | case)

          EstimateSD
Intercept      9.494
Residual       4.989
\end{verbatim}

Now the interaction \texttt{phase:sexm} is significant and the main effect is no longer relevant. It looks like the intervention effect is \(7.7\) points lower for male subjects. While the level-effect is \(16.8\) points for female subjects it is \(16.8\) - \(7.7\) = \(9.1\) for males.

\hypertarget{randomization-tests}{%
\chapter{Randomization tests}\label{randomization-tests}}

The \texttt{rand\_test} function computes a randomization test for single or multiple baseline single-case data. The function is based on an algorithm from the SCRT package (Bulte \& Onghena, 2009, 2012), but rewritten and extended for the use in AB designs.

The \texttt{statsitics} argument defines the statistic on which the comparison of the phases is based on. The following comparisons are possible:

\begin{itemize}
\tightlist
\item
  ``Mean A-B'': Uses the difference between the mean of phase A and the mean of phase B. * This is appropriate if a decrease of scores is expected for phase B.
\item
  ``Mean B-A'': Uses the difference between the mean of phase B and the mean of phase A. This is appropriate if an increase of scores is expected for phase B.
\item
  ``Mean \textbar A-B\textbar{}'': Uses the absolute value of the difference between the means of phases A and B.
\item
  ``Median A-B'': The same as ``Mean A-B'', but based on the median.
\item
  ``Median B-A'': The same as ``Mean B-A'', but based on the median.
\end{itemize}

\emph{number}\\
Sample size of the randomization distribution. The exactness of the p-value can not exceed 1/number (i.e., number = 100 results in p-values with an exactness of one percent). Default is number = 500. For faster processing use number = 100. For more precise p-values set number = 1000.

\emph{complete}\\
If TRUE, the distribution is based on a complete permutation of all possible starting combinations. This setting overwrites the number Argument. The default setting is FALSE.

\emph{limit}\\
Minimal number of data points per phase in the sample. The first number refers to the A-phase and the second to the B-phase (e.g., limit = c(5, 3)). If only one number is given, this number is applied to both phases. Default is limit = 5.

\emph{startpoints}\\
Alternative to the limit-parameter, startpoints exactly defines the possible start points of phase B (e.g., startpoints = 4:9 restricts the phase B start points to measurements 4 to 9. startpoints overwrite the limit-parameter.

\emph{exclude.equal}\\
If set to FALSE, which is the default, random distribution values equal to the observed distribution are counted as null-hypothesis conform. That is, they decrease the probability of rejecting the null-hypothesis (increase the p-value). exclude.equal should be set to TRUE if you analyse one single-case design (not a multiple baseline data set) to reach a sufficient power. But be aware, that it increases the chance of an alpha-error.

\emph{graph}\\
If set TRUE, a histogram of the resulting distribution is plotted.

\emph{phases}\\
A vector of two characters or numbers indicating the two phases that should be compared. E.g., phases = c(``A'',``C'') or phases = c(2,4) for comparing the second and the fourth phase. Phases could be combined by providing a list with two elements. E.g., phases = list(A = c(1,3), B = c(2,4)) will compare phases 1 and 3 (as A) against 2 and 4 (as B). Default is phases = c(``A'',``B'').

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{rand\_test}\NormalTok{(exampleAB, }\AttributeTok{graph =} \ConstantTok{TRUE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{scan-book_files/figure-latex/rand-1.pdf}

\begin{verbatim}
Randomization Test

Test for 3 cases.

Comparing phase A against phase B 
Statistic:  Mean B-A 

Minimal length of each phase: A = 5 , B = 5 
Observed statistic =  20.55556 

Distribution based on a random sample of all 1331 possible combinations.
n   =  500 
M   =  18.59305 
SD  =  1.114388 
Min =  16.05185 
Max =  21.34493 

Probability of observed statistic based on distribution:
p   =  0.036 

Shapiro-Wilk Normality Test: W = 0.979; p = 0.000  (Hypothesis of normality rejected)

Probabilty of observed statistic based on the assumption of normality:
z = 1.7611, p = 0.0391 (single sided)
\end{verbatim}

\hypertarget{power-analyses-scan-version-0.54-or-later}{%
\chapter{Power analyses (scan version 0.54 or later)}\label{power-analyses-scan-version-0.54-or-later}}

\hypertarget{the-idea-of-a-power-test}{%
\section{The idea of a power-test}\label{the-idea-of-a-power-test}}

The \texttt{powert\_test()} function provides the alpha error probability and power when analyzing a specific effect of a single-case design with a given statistical method.

For example, you have a one case design with phase length A = 10 and B = 20. You assume a strong level effect of d = 1 and you expect a slight trend effect of d = 0.02 (per measurement). You might be interested to answer two questions:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  How suitable is a plm model for detecting the level-effect? (also: what is the power to detect the level effect?).
\item
  What if I had the same design but without a level-effect. How often would the plm falsely find a significant level-effect? (also: how large is the alpha-error probability for the level-effect?).
\end{enumerate}

In principle, \texttt{power\_test()} takes a single case design and repeatedly generates random cases based on that design. Each case is now analyzed with a given statistical method. The proportion of significant effects in these analyses is an estimator of the test-power. In a second step the design is stripped of the target effect and again multiple cases are generated on this changed design and analyzed with the same method. Now, the proportion of significant effects is the estimator for the alpha-error probability.

\hypertarget{set-up-a-single-case-design}{%
\section{Set up a single-case design}\label{set-up-a-single-case-design}}

The \texttt{design} function sets up a single-case design. You can define various parameters of that design:

\begin{table}

\caption{\label{tab:design-arguments}Core arguments of the design function}
\begin{tabular}[t]{>{\raggedright\arraybackslash}p{15em}>{\raggedright\arraybackslash}p{30em}}
\toprule
Argument & What it does ...\\
\midrule
\textbf{n} & Number of cases to be created (Default is n = 1).\\
\textbf{phase\_design} & A list defining the length and label of each phase. E.g., phase.length = list(A1 = 10, B1 = 10, A2 = 10, B2 = 10). Use vectors if you want to define different values for each case phase.length = list(A = c(10, 15), B = c(10, 15).\\
\textbf{trend} & Defines the effect size of a trend added incrementally to each measurement across the whole data-set. To assign different trends to several single-cases, use a vector of values (e.g. trend = c(.1, .3, .5)). If the number of cases exceeds the length of the vector, values are recycled. When using a 'gaussian' distribution, the trend parameters indicate effect size d changes. When using a binomial or poisson distribution, trend indicates an increase in points / counts per measurement.\\
\textbf{level} & A list that defines the level increase (effect size d) at the beginning of each phase relative to the previous phase (e.g. list(A = 0, B = 1)). The first element must be zero as the first phase of a single-case has no level effect (if you have one less list element than the number of phases, scan will add a leading element with 0 values). Use vectors to define variable level effects for each case (e.g. list(A = c(0, 0), B = c(1, 2))). When using a 'gaussian' distribution, the level parameters indicate effect size d changes. When using a binomial or poisson distribution, level indicates an increase in points / counts with the onset of each phase.\\
\textbf{slope} & A list that defines the increase per measurement for each phase compared to the previous phase. slope = list(A = 0, B = .1 generates an incremental increase of 0.1 per measurement starting at the B phase. The first list element must be zero as the first phase of a single-case has no slope effect (if you have one less list element than the number of phases, scan will add a leading element with 0 values). Use vectors to define variable slope effects for each case (e.g. list(A = c(0, 0), B = c(0.1, 0.2))). If the number of cases exceeds the length of the vector, values are recycled. When using a 'gaussian' distribution, the slope parameters indicate effect size d changes per measurement. When using a binomial or poisson distribution, slope indicates an increase in points / counts per measurement.\\
\textbf{rtt} & Reliability of the underlying simulated measurements. Set rtt = .8 by default. To assign different reliabilities to several single-cases, use a vector of values (e.g. rtt = c(.6, .7, .8)). If the number of cases exceeds the length of the vector, values are repeated. rtt has no effect when you're using binomial or poisson distributed scores.\\
\textbf{start\_value} & Starting value at the first measurement. Default is 50. To assign different start values to several single-cases, use a vector of values (e.g. c(50, 42, 56)). If the number of cases exceeds the length of the vector, values are recycled.\\
\textbf{s} & Standard deviation used to calculate absolute values from level, slope, trend effects and to calculate and error distribution from the rtt values. Set to 10 by default. To assign different variances to several single-cases, use a vector of values (e.g. s = c(5, 10, 15)). If the number of cases exceeds the length of the vector, values are recycled. if the distribution is 'poisson' or 'binomial' s is not applied.\\
\textbf{extreme\_prop} & Probability of extreme values. extreme.p = .05 gives a five percent probability of an extreme value. A vector of values assigns different probabilities to multiple cases. If the number of cases exceeds the length of the vector, values are repeated.\\
\textbf{extreme\_range} & Range for extreme values, expressed as effect size d. extreme.d = c(-7,-6) uses extreme values within a range of -7 and -6 standard deviations. In case of a binomial or poisson distribution, extreme.d indicates points / counts. Caution: the first value must be smaller than the second, otherwise the procedure will fail.\\
\textbf{missing\_prop} & Portion of missing values. missing.p = 0.1 creates 10\% of all values as missing). A vector of values assigns different probabilities to multiple cases. If the number of cases exceeds the length of the vector, values are repeated.\\
\textbf{distribution} & Distribution of the scores. Default is distribution = 'normal'. Possible values are 'normal' (or 'gaussian'), 'binomial', and 'poisson'.\\
\textbf{prob} & If distribution is set 'binomial', prob passes the probability of occurrence.\\
\bottomrule
\end{tabular}
\end{table}

\hypertarget{conducting-a-power-test}{%
\section{Conducting a power-test}\label{conducting-a-power-test}}

When conduction a power test you firstly need to define a design which you like to be tested.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{design }\OtherTok{\textless{}{-}} \FunctionTok{design}\NormalTok{(}
  \AttributeTok{n =} \DecValTok{1}\NormalTok{,}
  \AttributeTok{phase\_design =} \FunctionTok{list}\NormalTok{(}\AttributeTok{A =} \DecValTok{10}\NormalTok{, }\AttributeTok{B =} \DecValTok{20}\NormalTok{),}
  \AttributeTok{level =} \FunctionTok{list}\NormalTok{(}\AttributeTok{A =} \DecValTok{0}\NormalTok{, }\AttributeTok{B =} \DecValTok{1}\NormalTok{),}
  \AttributeTok{trend =} \FloatTok{0.02}\NormalTok{,}
  \AttributeTok{distribution =} \StringTok{"normal"}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Then you have to choose the statistical method. The \texttt{power\_test} function applies three methods by default: \emph{plm}, \emph{randomization test}, and \emph{Tau U}.
These default values are only suitable when your design is a one case single-case study.

Let us start with the defaults and conduct a power analysis for our previously set design: \emph{(This might take some time. Even in the default setting with 100 simulations you might wait a few seconds. For more precise estimations I recommend 1000 simulations - or even higher.)}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{res }\OtherTok{\textless{}{-}} \FunctionTok{power\_test}\NormalTok{(design)}
\NormalTok{res}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Test-Power in percent:

    Method Power Alpha Error Alpha:Beta Correct p
 plm_level    74           4      1:6.5    85.0 0
      rand    73           4      1:6.8    84.5 0
      tauU   100          18      1:0.0    91.0 0
\end{verbatim}

The results show that the plm test and the randomization test have similar power and alpha-error probabilities (the differences here may be due to outliers of the random samples. A more intensive computation with 1000 simulations shows slightly better values for the plm). The tau U test has an unacceptably high alpha-error which is due to the trend we put into the design. \emph{Alpha:Beta} depicts the relation of the Alpha and Beta error (power = 1 - Beta). \emph{Correct} is the overall proportion of correct categorizations and \emph{p} is the results of a binomial-test of \emph{Correct} against 50\%.

\hypertarget{statistical-methods}{%
\section{Statistical methods}\label{statistical-methods}}

The \texttt{method} argument takes a list where each element depicts a statistical method. Currently, the following character strings are predefined:

\begin{table}

\caption{\label{tab:table-mc-func}Statistical methods}
\begin{tabular}[t]{>{\raggedright\arraybackslash}p{10em}>{\raggedright\arraybackslash}p{7em}l}
\toprule
Name & Single/ multiple cases & What it means ...\\
\midrule
\textbf{plm\_level} & single & A complete plm model for normal distributed dependent variables. It checks for the level effect.\\
\textbf{plm\_slope} & single & A complete plm model for normal distributed dependent variables. It checks for the slope effect.\\
\textbf{plm\_poisson\_level} & single & Like plm\_level but for poisson distributed dependent variables.\\
\textbf{plm\_poisson\_slope} & single & Like plm\_slope but for poisson distributed dependent variables.\\
\textbf{hplm\_level} & multiple & A complete hplm model for normal distributed dependent variables. It checks for the level effect.\\
\textbf{hplm\_slope} & multiple & A complete hplm model for normal distributed dependent variables. It checks for the slope effect.\\
\textbf{tauU} & sinlge & A tauU test with method complete and taub estimations. It checks the 'A vs. B - Trend A' variation.\\
\textbf{tauU\_slope} & sinlge & A tauU test with method complete and taub estimations. It checks the 'A vs. B - Trend A + Trend B' variation.\\
\textbf{tauU\_meta} & multiple & Like 'TauU' but with the results from a meta analyses (fixed effects). Very slow.\\
\textbf{tauU\_slope\_meta} & multiple & Like 'TauU\_slope' but with the results from a meta analyses (fixed effects). Very slow.\\
\textbf{base\_tau} & single & A baseline corrected tau test.\\
\textbf{rand} & single and multiple & A randomization test for 'Mean B-A' with 100 permutations.\\
\bottomrule
\end{tabular}
\end{table}

\hypertarget{advanced-methods}{%
\subsection{Advanced methods}\label{advanced-methods}}

\emph{Note: You need specific knowledge on how to create functions in R and on data structures to follow all aspects of this section.}

Instead of one of the predefined character strings you can also create you own functions and implement these. You function must take an scdf as the first argument and return a single numeric p-value.

Here is an example of a fast plm function for poisson distributed data based on the \texttt{fastglm} package:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{plm\_fast }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(data) \{}
\NormalTok{  data }\OtherTok{\textless{}{-}} \FunctionTok{unlist}\NormalTok{(data, }\AttributeTok{recursive =} \ConstantTok{FALSE}\NormalTok{)}
\NormalTok{  y  }\OtherTok{\textless{}{-}}\NormalTok{ data}\SpecialCharTok{$}\NormalTok{values}
\NormalTok{  n1 }\OtherTok{\textless{}{-}} \FunctionTok{sum}\NormalTok{(data}\SpecialCharTok{$}\NormalTok{phase }\SpecialCharTok{==} \StringTok{"A"}\NormalTok{)}
\NormalTok{  n2 }\OtherTok{\textless{}{-}} \FunctionTok{sum}\NormalTok{(data}\SpecialCharTok{$}\NormalTok{phase }\SpecialCharTok{==} \StringTok{"B"}\NormalTok{)}
\NormalTok{  D }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\FunctionTok{rep}\NormalTok{(}\DecValTok{0}\NormalTok{, n1), }\FunctionTok{rep}\NormalTok{(}\DecValTok{1}\NormalTok{, n2))}
\NormalTok{  mt }\OtherTok{\textless{}{-}}\NormalTok{ data}\SpecialCharTok{$}\NormalTok{mt}
\NormalTok{  inter }\OtherTok{\textless{}{-}}\NormalTok{ (mt }\SpecialCharTok{{-}}\NormalTok{ mt[n1]) }\SpecialCharTok{*}\NormalTok{ D}
\NormalTok{  x }\OtherTok{\textless{}{-}} \FunctionTok{matrix}\NormalTok{(}
    \FunctionTok{c}\NormalTok{(}\FunctionTok{rep}\NormalTok{(}\DecValTok{1}\NormalTok{, n1 }\SpecialCharTok{+}\NormalTok{ n2), mt, D, inter),}
    \AttributeTok{nrow =}\NormalTok{ n1 }\SpecialCharTok{+}\NormalTok{ n2,}
    \AttributeTok{ncol =} \DecValTok{4}
\NormalTok{  )}
\NormalTok{  full }\OtherTok{\textless{}{-}}\NormalTok{ fastglm}\SpecialCharTok{::}\FunctionTok{fastglm}\NormalTok{(}\AttributeTok{x =}\NormalTok{ x, }\AttributeTok{y =}\NormalTok{ y, }\AttributeTok{family =} \StringTok{"poisson"}\NormalTok{, }\AttributeTok{method =} \DecValTok{2}\NormalTok{)}
  \FunctionTok{summary}\NormalTok{(full)}\SpecialCharTok{$}\NormalTok{coef[}\DecValTok{3}\NormalTok{, }\DecValTok{4}\NormalTok{]}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{power\_test}\NormalTok{(design, }\AttributeTok{method =} \FunctionTok{list}\NormalTok{(}\StringTok{"fast plm"} \OtherTok{=}\NormalTok{ plm\_fast))}
\end{Highlighting}
\end{Shaded}

\hypertarget{computation-duration}{%
\section{Computation duration}\label{computation-duration}}

You can print the returning object of the \texttt{power\_test} function with added computation duration time by setting \texttt{duration\ =\ TRUE}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{print}\NormalTok{(res, }\AttributeTok{duration =} \ConstantTok{TRUE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Test-Power in percent:

    Method Power Alpha Error Alpha:Beta Correct p
 plm_level    74           4      1:6.5    85.0 0
      rand    73           4      1:6.8    84.5 0
      tauU   100          18      1:0.0    91.0 0

Computation duration is 1 seconds.
\end{verbatim}

The duration depends heavily on the applied test methods. Regressions are faster than randomization tests and tau U tests are quiet slow:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{res1 }\OtherTok{\textless{}{-}} \FunctionTok{power\_test}\NormalTok{(design, }\AttributeTok{method =} \StringTok{"plm\_level"}\NormalTok{)}
\NormalTok{res2 }\OtherTok{\textless{}{-}} \FunctionTok{power\_test}\NormalTok{(design, }\AttributeTok{method =} \StringTok{"rand"}\NormalTok{)}
\NormalTok{res3 }\OtherTok{\textless{}{-}} \FunctionTok{power\_test}\NormalTok{(design, }\AttributeTok{method =} \StringTok{"tauU"}\NormalTok{)}

\CommentTok{\# Elapsed time in seconds for each procedure}
\FunctionTok{attr}\NormalTok{(res1, }\StringTok{"computation\_duration"}\NormalTok{)[}\DecValTok{3}\NormalTok{]}
\NormalTok{elapsed }
  \FloatTok{0.077} 
\FunctionTok{attr}\NormalTok{(res2, }\StringTok{"computation\_duration"}\NormalTok{)[}\DecValTok{3}\NormalTok{]}
\NormalTok{elapsed }
  \FloatTok{0.207} 
\FunctionTok{attr}\NormalTok{(res3, }\StringTok{"computation\_duration"}\NormalTok{)[}\DecValTok{3}\NormalTok{]}
\NormalTok{elapsed }
  \FloatTok{0.764} 
\end{Highlighting}
\end{Shaded}

\ldots{} and what about our fast-glm function?

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{design }\OtherTok{\textless{}{-}} \FunctionTok{design}\NormalTok{(}
  \AttributeTok{n =} \DecValTok{1}\NormalTok{,}
  \AttributeTok{phase\_design =} \FunctionTok{list}\NormalTok{(}\AttributeTok{A =} \DecValTok{10}\NormalTok{, }\AttributeTok{B =} \DecValTok{20}\NormalTok{),}
  \AttributeTok{level =} \FunctionTok{list}\NormalTok{(}\AttributeTok{A =} \DecValTok{0}\NormalTok{, }\AttributeTok{B =} \DecValTok{1}\NormalTok{),}
  \AttributeTok{trend =} \FloatTok{0.02}\NormalTok{,}
  \AttributeTok{distribution =} \StringTok{"poisson"}
\NormalTok{)}

\NormalTok{res1 }\OtherTok{\textless{}{-}} \FunctionTok{power\_test}\NormalTok{(design, }\AttributeTok{method =} \FunctionTok{list}\NormalTok{(}\StringTok{"fast plm"} \OtherTok{=}\NormalTok{ plm\_fast))}
\NormalTok{res2 }\OtherTok{\textless{}{-}} \FunctionTok{power\_test}\NormalTok{(design, }\AttributeTok{method =} \StringTok{"rand"}\NormalTok{)}

\FunctionTok{attr}\NormalTok{(res1, }\StringTok{"computation\_duration"}\NormalTok{)[}\DecValTok{3}\NormalTok{]}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
elapsed 
  0.111 
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{attr}\NormalTok{(res2, }\StringTok{"computation\_duration"}\NormalTok{)[}\DecValTok{3}\NormalTok{]}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
elapsed 
  0.195 
\end{verbatim}

\ldots{} it is more that two times faster!

\hypertarget{default-settings}{%
\chapter{Default settings}\label{default-settings}}

Some of the default settings of scan can be changed with the \texttt{options()} argument. Table \ref{tab:table-options} shows a complete list of options and their default values.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# get the current value of an option}
\FunctionTok{getOption}\NormalTok{(}\StringTok{"scan.print.rows"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 15
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# set option to a different value}
\FunctionTok{options}\NormalTok{(}\AttributeTok{scan.print.rows =} \DecValTok{5}\NormalTok{, }\AttributeTok{scan.print.scdf.name =} \ConstantTok{FALSE}\NormalTok{)}
\FunctionTok{print}\NormalTok{(exampleAB)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
#A single-case data frame with 3 cases

 values mt phase ｜ values mt phase ｜ values mt phase ｜
     54  1     A ｜     41  1     A ｜     55  1     A ｜
     53  2     A ｜     59  2     A ｜     58  2     A ｜
     56  3     A ｜     56  3     A ｜     53  3     A ｜
     58  4     A ｜     51  4     A ｜     50  4     A ｜
     52  5     A ｜     52  5     A ｜     52  5     A ｜
# ... up to 15 more rows
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{options}\NormalTok{(}\AttributeTok{scan.print.rows =} \DecValTok{15}\NormalTok{, }\AttributeTok{scan.print.scdf.name =} \ConstantTok{TRUE}\NormalTok{)}
\FunctionTok{print}\NormalTok{(exampleAB)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
#A single-case data frame with 3 cases

 Johanna: values mt phase ｜ Karolina: values mt phase ｜ Anja: values mt phase
              54  1     A ｜               41  1     A ｜           55  1     A
              53  2     A ｜               59  2     A ｜           58  2     A
              56  3     A ｜               56  3     A ｜           53  3     A
              58  4     A ｜               51  4     A ｜           50  4     A
              52  5     A ｜               52  5     A ｜           52  5     A
              61  6     B ｜               57  6     B ｜           55  6     B
              62  7     B ｜               56  7     B ｜           68  7     B
              71  8     B ｜               67  8     B ｜           68  8     B
              66  9     B ｜               75  9     B ｜           81  9     B
              64 10     B ｜               66 10     B ｜           67 10     B
              78 11     B ｜               69 11     B ｜           78 11     B
              70 12     B ｜               68 12     B ｜           73 12     B
              74 13     B ｜               73 13     B ｜           72 13     B
              82 14     B ｜               77 14     B ｜           78 14     B
              77 15     B ｜               79 15     B ｜           81 15     B
 ｜
 ｜
 ｜
 ｜
 ｜
 ｜
 ｜
 ｜
 ｜
 ｜
 ｜
 ｜
 ｜
 ｜
 ｜
 ｜
# ... up to 5 more rows
\end{verbatim}

\begin{table}

\caption{\label{tab:table-options}Scan Options}
\begin{tabular}[t]{>{\raggedright\arraybackslash}p{15em}>{\raggedright\arraybackslash}p{15em}l}
\toprule
Option & Default & What it does ...\\
\midrule
\textbf{scan.print.cases} & "fit" & Max number of cases printed for scdf objects\\
\textbf{scan.print.rows} & 15 & Max number of rows printed for scdf objects\\
\textbf{scan.print.cols} & "all" & Max number of columns printed for scdf objects\\
\textbf{scan.print.digits} & 2 & Max number of digits printed for scdf objects\\
\textbf{scan.print.long} & FALSE & If TRUE, prints scdf objects in long format\\
\textbf{scan.print.scdf.name} & TRUE & If TRUE, prints case names of scdf\\
\textbf{scan.deprecated.warning} & FALSE & When TRUE returns information on deprecated functions\\
\textbf{scan.export.kable} & list(digits = 2, linesep = "", booktab = TRUE) & List with default arguments for the kable argument of the export function\\
\textbf{scan.export.kable\_styling} & list(bootstrap\_options = c("bordered", "condensed"), full\_width = FALSE, position = "left", latex\_options = "hold\_position", htmltable\_class = "lightable-classic") & List with default arguments for the kable\_styling argument of the export function\\
\textbf{scan.plot.style} & "grid" & NA\\
\textbf{scan.print.bar} & "｜" & NA\\
\bottomrule
\end{tabular}
\end{table}

\hypertarget{example-datasets}{%
\chapter{Example datasets}\label{example-datasets}}

\begin{table}

\caption{\label{tab:scanex}Scan Options}
\begin{tabular}[t]{>{\raggedright\arraybackslash}p{7em}>{\raggedright\arraybackslash}p{20em}l}
\toprule
Name & Info & Author\\
\midrule
\textbf{Beretvas2008} & Example from Beretvas, S., \& Chung, H. (2008). An evaluation of modified R2-change effect size indices for single-subject experimental designs. Evidence-Based Communication Assessment and Intervention, 2, 120-128. & \\
\textbf{Borckardt2014} & Example from Borckardt, J. J., \& Nash, M. R. (2014). Simulation modelling analysis for small sets of single-subject data collected over time. Neuropsychological Rehabilitation, 24(3-4), 492-506. & \\
\textbf{Grosche2011} & Data from Grosche, M. (2011). Effekte einer direkt-instruktiven Förderung der Lesegenauigkeit. Empirische Sonderpädagogik, 3(2), 147-161. & Michael Grosche\\
\textbf{Grosche2014} & Data from a multiple material multi person intervention study on reading. & Michael Grosche, Timo Lueke and Juergen Wilbert\\
\textbf{GruenkeWilbert2014} & Data from an intervention study on text comprehension. Gruenke, M., Wilbert, J., \& Stegemann-Calder, K. (2013). Analyzing the effects of story mapping on the reading comprehension of children with low intellectual abilities. Learning Disabilities: A Contemporary Journal, 11(2), 51-64. & Matthias Gruenke and Juergen Wilbert\\
\textbf{Huber2014} & Behavioral data (compliance in percent). & Christian Huber\\
\textbf{Huitema2000} & Example from Huitema, B. E., \& Mckean, J. W. (2000). Design specification issues in time-series intervention models. Educational and Psychological Measurement, 60(1), 38-58. & \\
\textbf{Leidig2018} & Data from: Leidig et. al (2018, unpublished). Effects of the Good Behavior Game on At-risk Students' from Primary Schools & \\
\textbf{Leidig2018\_l2} &  & \\
\textbf{Lenz2013} & Example from Lenz, A. S. (2013). Calculating Effect Size in Single-Case Research: A Comparison of Nonoverlap Methods. Measurement and Evaluation in Counseling and Development, 46(1), 64-73. & \\
\textbf{Parker2011} & Example from Parker, R. I., Vannest, K. J., Davis, J. L., \& Sauber, S. B. (2011). Combining Nonoverlap and Trend for Single-Case Research: Tau-U. Behavior Therapy, 42(2), 284-299. & \\
\textbf{SSDforR2017} & Example from the SSDforR package. & Charles Auerbach, PhD \& Wendy Zeitlin, PhD; Yeshiva University, Wurzweiler school of social work.\\
\textbf{Waddell2011} & Example from Waddell, D. E., Nassar, S. L., \& Gustafson, S. A. (2011). Single-Case Design in Psychophysiological Research: Part II: Statistical Analytic Approaches. Journal of Neurotherapy, 15(2), 160 - 169. & \\
\textbf{byHeart2011} & Data from university students learning vocabulary by heart and checking their progress with 20 flashcards each session. & Juergen Wilbert\\
\textbf{exampleA1B1A2B2} &  & \\
\textbf{exampleA1B1A2B2\_zvt} &  & \\
\textbf{exampleAB} & Randomly created data with normal distributed dependent variable. & \\
\textbf{exampleABAB} & Randomly created data with uniform distribution. & \\
\textbf{exampleABC} &  & \\
\textbf{exampleABC\_150} & Random data-set for testing out hplm. Level and slope effects vary. & \\
\textbf{exampleABC\_outlier} & Random data-set based on exampleABC but with outliers. & \\
\textbf{exampleAB\_50} &  & \\
\textbf{exampleAB\_50.l2} &  & \\
\textbf{exampleAB\_add} & Random data-set for testing out plm with additional variables. & \\
\textbf{exampleAB\_decreasing} & Random data-set from a poisson distribution. Level effect is negative. & \\
\textbf{exampleAB\_mpd} & A multiple phase design study. & Juergen Wilbert\\
\textbf{exampleAB\_score} & Random data-set for binomial data. & \\
\textbf{exampleAB\_simple} & A simple multiple baseline AB Design. & Juergen Wilbert\\
\textbf{example\_A24} & Number of injuries on a German autobahn before and after implementation of a speedlimit (130km/h). & Ministerium fuer Infrastruktur und Landesplanung. Land Brandenburg.\\
\bottomrule
\end{tabular}
\end{table}

\hypertarget{exporting-scan-results}{%
\chapter{\texorpdfstring{Exporting \emph{scan} results}{Exporting scan results}}\label{exporting-scan-results}}

The \texttt{export} function will make it easier to convert the results of your \texttt{scan} analyses into tables and descriptions you can add to your documents and presentations. Basically, \texttt{export} takes a \texttt{scan} object and converts it to an html-table or latex output.

\begin{rmdnote}
\texttt{export} it build on top of the \texttt{knitr} and
\texttt{kableextra} packages. The list provided in the
\texttt{kable\_options} argument is implemented in the \texttt{kable}
function of \texttt{knitr} and the list provided to the
\texttt{kable\_styling\_options} is implemented in the
\texttt{kable\_styling} command of the \texttt{kableExtra} package.
\texttt{export} sets some defaults for these functions but you can play
around and overwrite them.
\end{rmdnote}

\texttt{export} works best when used within an rmarkdown file and/or within \texttt{RStudio}.\\
In \texttt{RStudio}

{[}xxx to be continued!{]}

\hypertarget{single-case-data-files}{%
\section{Single case data files}\label{single-case-data-files}}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{export}\NormalTok{(exampleA1B1A2B2\_zvt)}
\end{Highlighting}
\end{Shaded}

\begin{table}[!h]

\caption{\label{tab:unnamed-chunk-84}Single case data frame with 3 cases}
\begin{tabular}[t]{cccccccccccc}
\toprule
\multicolumn{4}{c}{Tick} & \multicolumn{4}{c}{Trick} & \multicolumn{4}{c}{Track} \\
\cmidrule(l{3pt}r{3pt}){1-4} \cmidrule(l{3pt}r{3pt}){5-8} \cmidrule(l{3pt}r{3pt}){9-12}
zvt & d2 & day & part & zvt & d2 & day & part & zvt & d2 & day & part\\
\midrule
47 & 131 & 1 & A1 & 51 & 100 & 1 & A1 & 54 & 89 & 1 & A1\\
58 & 134 & 2 & A1 & 58 & 126 & 2 & A1 & 57 & 116 & 2 & A1\\
76 & 141 & 3 & A1 & 70 & 130 & 3 & A1 & 51 & 114 & 3 & A1\\
63 & 141 & 4 & B1 & 65 & 130 & 4 & B1 & 61 & 131 & 4 & B1\\
71 & 140 & 5 & B1 & 67 & 137 & 5 & B1 & 57 & 132 & 5 & B1\\
59 & 140 & 6 & B1 & 63 & 133 & 6 & B1 & 53 & 130 & 6 & B1\\
64 & 138 & 7 & A2 & 64 & 136 & 7 & A2 & 58 & 128 & 7 & A2\\
69 & 140 & 8 & A2 & 70 & 137 & 8 & A2 & 57 & 131 & 8 & A2\\
72 & 141 & 9 & A2 & 70 & 135 & 9 & A2 & 60 & 130 & 9 & A2\\
77 & 140 & 10 & B2 & 68 & 128 & 10 & B2 & 55 & 129 & 10 & B2\\
76 & 138 & 11 & B2 & 69 & 137 & 11 & B2 & 58 & 118 & 11 & B2\\
73 & 140 & 12 & B2 & 70 & 138 & 12 & B2 & 58 & 131 & 12 & B2\\
\bottomrule
\end{tabular}
\end{table}

\hypertarget{descriptive-stats}{%
\section{Descriptive stats}\label{descriptive-stats}}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{res }\OtherTok{\textless{}{-}} \FunctionTok{describe}\NormalTok{(GruenkeWilbert2014)}
\FunctionTok{export}\NormalTok{(res)}
\end{Highlighting}
\end{Shaded}

\begin{table}[!h]

\caption{\label{tab:unnamed-chunk-85}Descriptive statistics}
\begin{threeparttable}
\begin{tabular}[t]{lccccccccccccccccccl}
\toprule
\multicolumn{2}{c}{ } & \multicolumn{2}{c}{n} & \multicolumn{2}{c}{Missing} & \multicolumn{2}{c}{M} & \multicolumn{2}{c}{Median} & \multicolumn{2}{c}{SD} & \multicolumn{2}{c}{MAD} & \multicolumn{2}{c}{Min} & \multicolumn{2}{c}{Max} & \multicolumn{2}{c}{Trend} \\
\cmidrule(l{3pt}r{3pt}){3-4} \cmidrule(l{3pt}r{3pt}){5-6} \cmidrule(l{3pt}r{3pt}){7-8} \cmidrule(l{3pt}r{3pt}){9-10} \cmidrule(l{3pt}r{3pt}){11-12} \cmidrule(l{3pt}r{3pt}){13-14} \cmidrule(l{3pt}r{3pt}){15-16} \cmidrule(l{3pt}r{3pt}){17-18} \cmidrule(l{3pt}r{3pt}){19-20}
Case & Design & A & B & A & B & A & B & A & B & A & B & A & B & A & B & A & B & A & B\\
\midrule
Anton & A-B & 4 & 14 & 0 & 0 & 5.00 & 9.14 & 5 & 9 & 0.82 & 0.77 & 0.74 & 1.48 & 4 & 8 & 6 & 10 & -0.40 & 0.03\\
Bob & A-B & 7 & 11 & 0 & 0 & 3.00 & 8.82 & 3 & 9 & 0.82 & 0.87 & 1.48 & 0.00 & 2 & 7 & 4 & 10 & 0.04 & 0.04\\
Paul & A-B & 6 & 12 & 0 & 0 & 3.83 & 8.83 & 4 & 9 & 0.75 & 0.72 & 0.74 & 0.74 & 3 & 8 & 5 & 10 & -0.26 & 0.02\\
Robert & A-B & 8 & 10 & 0 & 0 & 4.12 & 8.90 & 4 & 9 & 0.83 & 0.99 & 1.48 & 1.48 & 3 & 7 & 5 & 10 & -0.06 & -0.14\\
Sam & A-B & 5 & 13 & 0 & 0 & 4.60 & 9.08 & 5 & 9 & 0.55 & 0.86 & 0.00 & 1.48 & 4 & 8 & 5 & 10 & 0.10 & 0.03\\
Tim & A-B & 4 & 14 & 0 & 0 & 3.00 & 9.00 & 3 & 9 & 0.82 & 0.96 & 0.74 & 1.48 & 2 & 7 & 4 & 10 & -0.60 & 0.00\\
\bottomrule
\end{tabular}
\begin{tablenotes}
\item \textit{Note: } 
\item n = Number of measurements; Missing = Number of missing values; M = Mean; Median = Median; SD = Standard deviation; MAD = Median average deviation; Min = Minimum; Max = Maximum; Trend = Slope of dependent variable regressed on measurement-time.
\end{tablenotes}
\end{threeparttable}
\end{table}

\hypertarget{overlap-indices}{%
\section{Overlap indices}\label{overlap-indices}}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{exampleA1B1A2B2\_zvt }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{select\_phases}\NormalTok{(}\AttributeTok{A =} \FunctionTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{,}\DecValTok{3}\NormalTok{), }\AttributeTok{B =} \FunctionTok{c}\NormalTok{(}\DecValTok{2}\NormalTok{,}\DecValTok{4}\NormalTok{)) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{overlap}\NormalTok{() }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{export}\NormalTok{(}\AttributeTok{flip =} \ConstantTok{TRUE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{table}[!h]

\caption{\label{tab:unnamed-chunk-86}Overlap indices. Comparing phase 1 against phase 2}
\begin{threeparttable}
\begin{tabular}[t]{llll}
\toprule
  & Tick & Trick & Track\\
\midrule
Design & A-B & A-B & A-B\\
PND & 16.67 & 0.00 & 16.67\\
PEM & 66.67 & 50.00 & 50.00\\
PET & 66.67 & 33.33 & 33.33\\
NAP & 68.06 & 51.39 & 58.33\\
NAP-R & 36.11 & 2.78 & 16.67\\
PAND & 66.67 & 50.00 & 54.17\\
Tau-U & 0.14 & 0.03 & -0.03\\
Base Tau & 0.27 & -0.25 & 0.13\\
Delta M & 5.50 & 3.17 & 0.83\\
Delta Trend & -0.31 & -1.10 & -0.74\\
SMD & 0.52 & 0.40 & 0.26\\
Hedges g & 0.56 & 0.50 & 0.26\\
\bottomrule
\end{tabular}
\begin{tablenotes}
\item \textit{Note: } 
\item PND = Percentage Non-Overlapping Data; PEM = Percentage Exceeding the Median; PET = Percentage Exceeding the Trend; NAP = Nonoverlap of all pairs; NAP-R = NAP rescaled; PAND = Percentage all nonoverlapping data;Tau U = Parker's Tau-U; Base Tau = Baseline corrected Tau; Delta M = Mean difference between phases; Delta Trend = Trend difference between phases; SMD = Standardized Mean Difference; Hedges g = Corrected SMD.
\end{tablenotes}
\end{threeparttable}
\end{table}

\hypertarget{piecewise-linear-models}{%
\section{Piecewise linear models}\label{piecewise-linear-models}}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{res }\OtherTok{\textless{}{-}} \FunctionTok{plm}\NormalTok{(exampleA1B1A2B2}\SpecialCharTok{$}\NormalTok{Pawel)}
\FunctionTok{export}\NormalTok{(res)}
\end{Highlighting}
\end{Shaded}

\begin{table}[!h]

\caption{\label{tab:unnamed-chunk-87}Piecewise-regression model predicting variable 'values'}
\begin{threeparttable}
\begin{tabular}[t]{lrrrrrrr}
\toprule
\multicolumn{2}{c}{ } & \multicolumn{2}{c}{CI(95\%)} & \multicolumn{4}{c}{ } \\
\cmidrule(l{3pt}r{3pt}){3-4}
Parameter & B & 2.5\% & 97.5\% & SE & t & p & Delta R²\\
\midrule
Intercept & 12.47 & 4.90 & 20.03 & 3.86 & 3.23 & <.01 & \\
Trend mt & 0.22 & -0.99 & 1.44 & 0.62 & 0.36 & .72 & .00\\
Level phase B1 & 17.69 & 7.71 & 27.67 & 5.09 & 3.48 & <.01 & .14\\
Level phase A2 & 2.58 & -16.96 & 22.12 & 9.97 & 0.26 & .79 & .00\\
Level phase B2 & 12.54 & -18.46 & 43.54 & 15.82 & 0.79 & .43 & .01\\
Slope phase B1 & -1.41 & -3.13 & 0.32 & 0.88 & -1.60 & .11 & .03\\
Slope phase A2 & -1.10 & -2.83 & 0.62 & 0.88 & -1.25 & .21 & .02\\
Slope phase B2 & -1.08 & -2.81 & 0.64 & 0.88 & -1.23 & .22 & .02\\
\bottomrule
\end{tabular}
\begin{tablenotes}
\item \textit{Note: } 
\item F(7, 32) = 7.86; p <.001; R² = 0.632; Adjusted R² = 0.552
\end{tablenotes}
\end{threeparttable}
\end{table}

\hypertarget{hierarchical-piecewise-regressions}{%
\section{Hierarchical piecewise regressions}\label{hierarchical-piecewise-regressions}}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{exampleAB\_50 }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{add\_l2}\NormalTok{(exampleAB\_50.l2) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{hplm}\NormalTok{(}\AttributeTok{lr.test =} \ConstantTok{TRUE}\NormalTok{, }\AttributeTok{random.slopes =} \ConstantTok{TRUE}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{export}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{table}[!h]

\caption{\label{tab:unnamed-chunk-88}Hierarchical Piecewise Linear Regression predicting variable 'values'}
\begin{threeparttable}
\begin{tabular}[t]{lrrrrr}
\toprule
Parameter & B & SE & df & t & p\\
\midrule
\addlinespace[0.3em]
\multicolumn{6}{l}{\textbf{Fixed effects}}\\
Intercept & 47.59 & 1.42 & 1328 & 33.43 & <.001\\
Trend mt & 0.62 & 0.11 & 1328 & 5.51 & <.001\\
Level phase B & 13.01 & 0.85 & 1328 & 15.33 & <.001\\
Slope phase B & 0.86 & 0.12 & 1328 & 7.43 & <.001\\
\midrule
\addlinespace[0.3em]
\multicolumn{6}{l}{\textbf{Random effects}}\\
 & SD & L & df & p & \\
Intercept & 9.3 & 246.48 & 4 & <.001 & \\
Trend mt & 0.1 & 0.85 & 4 & .93 & \\
Level phase B & 4.54 & 50.88 & 4 & <.001 & \\
Slope phase B & 0.13 & 0.78 & 4 & .94 & \\
Residual & 4.97 & NA & NA & NA & \\
\midrule
\addlinespace[0.3em]
\multicolumn{6}{l}{\textbf{Model}}\\
AIC & 8693.2 &  &  &  & \\
BIC & 8771.7 &  &  &  & \\
ICC & 0.29 & L = 339 & p <.001 &  & \\
\bottomrule
\end{tabular}
\begin{tablenotes}
\item \textit{Note: } 
\item Estimation method ML; Slope estimation method: B\&L-B; 50 cases
\end{tablenotes}
\end{threeparttable}
\end{table}

\hypertarget{appendix}{%
\chapter{Appendix}\label{appendix}}

\hypertarget{important-changes-with-version-0.53}{%
\section{Important changes with version 0.53}\label{important-changes-with-version-0.53}}

\hypertarget{single-case-studies-with-cases-of-varying-phase-design}{%
\subsection{Single-case studies with cases of varying phase design}\label{single-case-studies-with-cases-of-varying-phase-design}}

Sometimes it is necessary to combine single-cases with different phase-designs into one single-case study (for instance when some cases include an extension phase and others do not). Various functions in scan now can handle such a data structure.

\hypertarget{piping}{%
\subsection{Piping}\label{piping}}

The concept of piping is great for writing clean and intelligible code that is easier to debug. We imported the pipe function \texttt{\%\textgreater{}\%} from the \texttt{magrittr} package. Since version 4.1, R has its own pipe operator implementation \texttt{\textbar{}\textgreater{}}. This is great and works fine with the \texttt{scan} package. But since the \texttt{\textbar{}\textgreater{}} Operator is not backwards compatible for R prior versions 4.1, we will stick with the \texttt{\%\textgreater{}\%} for a while.

To allow for smooth ``piping'' we began adding some functions \texttt{select\_phases}, \texttt{subset}, \texttt{select\_cases}, \texttt{set\_var}, \texttt{set\_dvar}, \texttt{set\_mvar}, \texttt{set\_pvar}, and \texttt{add\_l2}.

\hypertarget{important-changes-with-version-0.50}{%
\section{Important changes with version 0.50}\label{important-changes-with-version-0.50}}

\hypertarget{new-function-names}{%
\subsection{New function names}\label{new-function-names}}

With version 0.50 scan introduced new names for its functions. The old function names are still usable but they will return a ``deprecated'' warning telling you to use the new function names.

Table \ref{tab:table-aliases} shows the changes.

\begin{table}

\caption{\label{tab:table-aliases}scan previous and current function names.}
\begin{tabular}[t]{>{\raggedright\arraybackslash}p{15em}>{\raggedright\arraybackslash}p{30em}}
\toprule
Current function name & Previous function name\\
\midrule
\textbf{autocorr} & autocorrSC\\
\textbf{corrected\_tau} & corrected\_tauSC\\
\textbf{describe [since v0.52]} & describeSC\\
\textbf{fill\_missing} & fillmissingSC\\
\textbf{outlier} & outlierSC\\
\textbf{overlap} & overlapSC\\
\textbf{power\_test} & power\_testSC\\
\textbf{rand\_test} & randSC; rand.test\\
\textbf{ranks} & rankSC\\
\textbf{rci} & rCi; rciSC\\
\textbf{shift} & shiftSC\\
\textbf{smooth\_cases} & smoothSC\\
\textbf{style\_plot} & style.plotSC; style\_plotSC\\
\textbf{tau\_u} & tauUSC\\
\textbf{trend} & trendSC\\
\textbf{truncate\_phase} & truncateSC\\
\bottomrule
\end{tabular}
\end{table}

\hypertarget{change-target-variables-in-functions}{%
\subsection{Change target variables in functions}\label{change-target-variables-in-functions}}

All functions in R that analyze data now allow for temporarily changing dependent, phase, and measurement-time variables by adding three argument:

\texttt{dvar} sets the dependent variable.\\
\texttt{pvar} sets the phase variable.\\
\texttt{mvar} sets the measurement-time variable.

For example, \texttt{overlap(exampleAB\_add,\ dvar\ =\ "depression")} will report overlap parameters for the variable \emph{depression} while \texttt{overlap(exampleAB\_add)} while take \emph{wellbeing} as the dependent variable (as defined in the scdf).

After finishing the analysis, the variables are set back to their original values as defined in the scdf.

\hypertarget{about-the-author}{%
\chapter*{About the author}\label{about-the-author}}
\addcontentsline{toc}{chapter}{About the author}

Currently, I am a professor for research methods and diagnostics at the department of inclusive education at the University of Potsdam in Germany. I studied education sciences at the University of Cologne where I also did my PhD in psychology. Thereafter, I got a tenured position as a senior researcher at the department of special education (also University of Cologne). Later I did my habilitation on ``Pedagogic and psychology in learning disabilities'' at the \emph{Carl von Ossietzky University} Oldenburg.

My current work focuses on:

\begin{itemize}
\tightlist
\item
  Single-case research designs, analyzing single case data, and reporting single-case based results.
\item
  Social inclusion and social participation in classrooms.
\item
  Implementation of Open Science and Data Science concepts into special education research.
\end{itemize}

You can find more information about me on my homepage:

\url{https://jazznbass.github.io/homepage/}

\hypertarget{references}{%
\chapter*{References}\label{references}}
\addcontentsline{toc}{chapter}{References}

  \bibliography{full.bib,packages.bib}

\end{document}
