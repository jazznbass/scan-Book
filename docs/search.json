[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Analyzing Single-Case Data with R and Scan",
    "section": "",
    "text": "Preface\nHello everyone!\nI am delighted to see that you have discovered this book, as it indicates that you are starting to explore the scan package. While scan has undergone extensive development, this book is still a work in progress (approximately 44% complete). I am continuously working on expanding its content. Currently, there is no official release version available; only this draft, which may contain errors.\nIf you have any suggestions for improving the book, wish to report bugs, or provide comments, feedback, and more, please visit the GitHub repository of this book and contribute here:\nhttps://github.com/jazznbass/scan-Book/discussions.\nThank you very much!\nJürgen\n22 September 2023"
  },
  {
    "objectID": "index.html#general-references",
    "href": "index.html#general-references",
    "title": "Analyzing Single-Case Data with R and Scan",
    "section": "General references",
    "text": "General references\nThis book has been created using Quarto within the RStudio (RStudio Team, 2018) environment. The analyses have been conducted with the R package scan at version 0.60.0.9999 (Wilbert & Lueke, 2023) and scplot at version 0.3.7 (Wilbert, 2023). R version 4.3.1 (2023-06-16) was used (R Core Team, 2023).\nNote: The cover has been designed by Tony Wilbert and Henry Ritter.\nThanx for that!\n\n\n\n\nR Core Team. (2023). R: A language and environment for statistical computing. Retrieved from https://www.R-project.org/\n\n\nRStudio Team. (2018). RStudio: Integrated development environment for r. Retrieved from http://www.rstudio.com/\n\n\nWilbert, J. (2023). Scplot: Plot function for single-case data frames.\n\n\nWilbert, J., & Lueke, T. (2023). Scan: Single-case data analyses for single and multiple baseline designs."
  },
  {
    "objectID": "ch_introduction.html#a-teaser",
    "href": "ch_introduction.html#a-teaser",
    "title": "1  Introduction",
    "section": "1.1 A teaser",
    "text": "1.1 A teaser\nBefore going into the details of how scan works, I would like to give you an example of what you can do with scan. It is meant as a teaser to get you motivated to tackle the steep learning curve associated with using R (but there is a land of milk and honey behind that curve!). So, do not worry if you do not understand every detail of this example, it will all be explained and obvious to you once you become familiar with scan.\nLet us set a fictional context. Suppose you are researching a method to improve the arithmetic skills of struggling fourth-grade students. You have developed an intervention programme called KUNO. In a pilot study, you want to get some evidence about the effectiveness of this new method, and you set up a multi-baseline single-case study with three students who participated in the KUNO programme over a period of ten weeks. during the course, you regularly measured each student’s numeracy skills 20 times using a reliable test. You also carried out a follow-up after eight weeks with five additional measures. I am now going to make up some data for this fictitious KUNO study, because it would be too difficult to carry out a real study and actually develop a real intervention method.\nWe use the scan package to code the data. Each case has 25 measurements. We have three phases: before the intervention (A), during the intervention (B), and a follow-up (C). Phases A and B are of different lengths. The cases are named and combined into a single object called strange_study.\n\nlibrary(scan)\nlibrary(scplot)\n\ncase1 &lt;- scdf(\n  c(A = 3, 2, 4, 6, 4, 3, \n    B = 6, 5, 4, 6, 7, 5, 6, 8, 6, 7, 8, 9, 7, 8, \n    C = 6, 6, 8, 5, 7), \n  name = \"Dustin\"\n)\ncase2 &lt;- scdf(\n  c(A = 0, 1, 3, 1, 4, 2, 1, \n    B = 2, 1, 4, 3, 5, 5, 7, 6, 3, 8, 6, 4, 7, \n    C = 6, 5, 6, 8, 6), \n  name = \"Mike\"\n)\ncase3 &lt;- scdf(\n  c(A = 7, 5, 6, 4, 4, 7, 5, 7, 4,\n    B = 8, 9, 11, 13, 12, 15, 16, 13, 17, 16, 18,\n    C = 17, 20, 22, 18, 20), \n  name = \"Will\"\n)\nstrange_study &lt;- c(case1, case2, case3)\n\nNow we visualize the cases:\n\nscplot(strange_study) %&gt;%\n  set_ylabel(\"Correct\") %&gt;%\n  set_xlabel (\"Days\") %&gt;%\n  add_statline(\"lowess\", color = \"red\") %&gt;%\n  set_phasenames(c(\"Baseline\", \"Intervention\", \"Follow-up\")) %&gt;%\n  set_yaxis(limits = c(0, 30)) %&gt;%\n  set_xaxis(increment = 2) %&gt;%\n  add_ridge(color = \"lightblue\") %&gt;%\n  set_theme(\"basic\")\n\n\n\n\nNow we need some descriptive statistics:\n\ndescribe(strange_study)\n\n\n\n\nDescriptive statistics\n\n\nParameter\nDustin\nMike\nWill\n\n\n\n\nDesign\nA-B-C\nA-B-C\nA-B-C\n\n\nn A\n6\n7\n9\n\n\nn B\n14\n13\n11\n\n\nn C\n5\n5\n5\n\n\nMissing A\n0\n0\n0\n\n\nMissing B\n0\n0\n0\n\n\nMissing C\n0\n0\n0\n\n\nm A\n3.67\n1.71\n5.44\n\n\nm B\n6.57\n4.69\n13.45\n\n\nm C\n6.4\n6.2\n19.4\n\n\nmd A\n3.5\n1.0\n5.0\n\n\nmd B\n6.5\n5.0\n13.0\n\n\nmd C\n6\n6\n20\n\n\nsd A\n1.37\n1.38\n1.33\n\n\nsd B\n1.40\n2.10\n3.27\n\n\nsd C\n1.14\n1.10\n1.95\n\n\nmad A\n0.74\n1.48\n1.48\n\n\nmad B\n1.48\n2.97\n4.45\n\n\nmad C\n1.48\n0.00\n2.97\n\n\nMin A\n2\n0\n4\n\n\nMin B\n4\n1\n8\n\n\nMin C\n5\n5\n17\n\n\nMax A\n6\n4\n7\n\n\nMax B\n9\n8\n18\n\n\nMax C\n8\n8\n22\n\n\nTrend A\n0.23\n0.21\n-0.08\n\n\nTrend B\n0.25\n0.36\n0.91\n\n\nTrend C\n0.1\n0.3\n0.4\n\n\n\nNote:   n = Number of measurements; Missing = Number of missing values; M = Mean; Median = Median; SD = Standard deviation; MAD = Median average deviation; Min = Minimum; Max = Maximum; Trend = Slope of dependent variable regressed on measurement-time.\n\n\n\n\n\n\n\n\n\n\n\nSingle-case data are often analysed using overlap indices. Let us get an overview by comparing phases A and B:\n\noverlap(strange_study)\n\n\n\n\nOverlap indices. Comparing phase 1 against phase 2\n\n\nStatistic\nDustin\nMike\nWill\n\n\n\n\nPND\n50.00\n53.85\n100.00\n\n\nPEM\n100.00\n92.31\n100.00\n\n\nPET\n71.43\n61.54\n100.00\n\n\nNAP\n92.86\n87.91\n100.00\n\n\nNAP-R\n85.71\n75.82\n100.00\n\n\nPAND\n90.00\n80.00\n100.00\n\n\nIRD\n0.76\n0.56\n1.00\n\n\nTau-U (A + B - trend A)\n0.53\n0.45\n0.67\n\n\nTau-U (A + B - trend A + trend B)\n0.66\n0.56\n0.80\n\n\nBase Tau\n0.60\n0.55\n0.74\n\n\nDelta M\n2.90\n2.98\n8.01\n\n\nDelta Trend\n0.02\n0.14\n0.99\n\n\nSMD\n2.13\n2.16\n6.01\n\n\nHedges g\n2.00\n1.51\n2.96\n\n\n\nNote:   PND = Percentage Non-Overlapping Data; PEM = Percentage Exceeding the Median; PET = Percentage Exceeding the Trend; NAP = Nonoverlap of all pairs; NAP-R = NAP rescaled; PAND = Percentage all nonoverlapping data; IRD = Improvement rate difference; Tau U (A + B - trend A) = Parker's Tau-U; Tau U (A + B - trend A + trend B) = Parker's Tau-U; Base Tau = Baseline corrected Tau; Delta M = Mean difference between phases; Delta Trend = Trend difference between phases; SMD = Standardized Mean Difference; Hedges g = Corrected SMD.\n\n\n\n\n\n\n\n\n\n\n\nHow do the changes hold up against the follow-up? Let us compare phases A and C:\n\noverlap(strange_study, phases = c(\"A\", \"C\"))\n\n\n\n\nOverlap indices. Comparing phase A against phase C\n\n\nStatistic\nDustin\nMike\nWill\n\n\n\n\nPND\n40.00\n100.00\n100.00\n\n\nPEM\n100.00\n100.00\n100.00\n\n\nPET\n0.00\n60.00\n100.00\n\n\nNAP\n93.33\n100.00\n100.00\n\n\nNAP-R\n86.67\n100.00\n100.00\n\n\nPAND\n81.82\n100.00\n100.00\n\n\nIRD\n0.82\n1.00\n1.00\n\n\nTau-U (A + B - trend A)\n0.48\n0.50\n0.61\n\n\nTau-U (A + B - trend A + trend B)\n0.46\n0.51\n0.61\n\n\nBase Tau\n0.67\n0.76\n0.74\n\n\nDelta M\n2.73\n4.49\n13.96\n\n\nDelta Trend\n-0.13\n0.09\n0.48\n\n\nSMD\n2.00\n3.25\n10.47\n\n\nHedges g\n1.97\n3.25\n8.34\n\n\n\nNote:   PND = Percentage Non-Overlapping Data; PEM = Percentage Exceeding the Median; PET = Percentage Exceeding the Trend; NAP = Nonoverlap of all pairs; NAP-R = NAP rescaled; PAND = Percentage all nonoverlapping data; IRD = Improvement rate difference; Tau U (A + B - trend A) = Parker's Tau-U; Tau U (A + B - trend A + trend B) = Parker's Tau-U; Base Tau = Baseline corrected Tau; Delta M = Mean difference between phases; Delta Trend = Trend difference between phases; SMD = Standardized Mean Difference; Hedges g = Corrected SMD.\n\n\n\n\n\n\n\n\n\n\n\nFinally, we conduct regression analyses for each cases with a piecewise regression model:\n\nplm(strange_study$Dustin)\nplm(strange_study$Mike)\nplm(strange_study$Will)\n\n\n\n\nPiecewise-regression model predicting variable 'values'\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCI(95%)\n\n\n\n\nParameter\nB\n2.5%\n97.5%\nSE\nt\np\ndelta R²\n\n\n\n\nIntercept\n3.10\n1.46\n4.73\n0.83\n3.72\n&lt;.01\n\n\n\nTrend mt\n0.23\n-0.31\n0.77\n0.28\n0.83\n.41\n0.012\n\n\nLevel phase B\n0.50\n-1.89\n2.90\n1.22\n0.41\n.68\n0.003\n\n\nLevel phase C\n-1.47\n-11.11\n8.17\n4.92\n-0.30\n.76\n0.002\n\n\nSlope phase B\n0.02\n-0.54\n0.58\n0.28\n0.06\n.95\n0.000\n\n\nSlope phase C\n-0.13\n-1.02\n0.77\n0.46\n-0.28\n.78\n0.001\n\n\n\nNote:   F(5, 19) = 7.88; p &lt;.001; R² = 0.675; Adjusted R² = 0.589\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPiecewise-regression model predicting variable 'values'\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCI(95%)\n\n\n\n\nParameter\nB\n2.5%\n97.5%\nSE\nt\np\ndelta R²\n\n\n\n\nIntercept\n1.07\n-0.95\n3.09\n1.03\n1.04\n.31\n\n\n\nTrend mt\n0.21\n-0.35\n0.78\n0.29\n0.75\n.46\n0.009\n\n\nLevel phase B\n-0.02\n-2.98\n2.93\n1.51\n-0.01\n.98\n0.000\n\n\nLevel phase C\n0.24\n-9.63\n10.12\n5.04\n0.05\n.96\n0.000\n\n\nSlope phase B\n0.14\n-0.46\n0.75\n0.31\n0.47\n.64\n0.004\n\n\nSlope phase C\n0.09\n-1.01\n1.18\n0.56\n0.15\n.88\n0.000\n\n\n\nNote:   F(5, 19) = 8.00; p &lt;.001; R² = 0.678; Adjusted R² = 0.593\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPiecewise-regression model predicting variable 'values'\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCI(95%)\n\n\n\n\nParameter\nB\n2.5%\n97.5%\nSE\nt\np\ndelta R²\n\n\n\n\nIntercept\n5.78\n3.96\n7.60\n0.93\n6.23\n&lt;.001\n\n\n\nTrend mt\n-0.08\n-0.47\n0.30\n0.20\n-0.43\n.67\n0.001\n\n\nLevel phase B\n3.88\n1.16\n6.60\n1.39\n2.80\n&lt;.05\n0.022\n\n\nLevel phase C\n14.49\n7.89\n21.08\n3.37\n4.31\n&lt;.001\n0.052\n\n\nSlope phase B\n0.99\n0.52\n1.47\n0.24\n4.10\n&lt;.01\n0.047\n\n\nSlope phase C\n0.48\n-0.53\n1.49\n0.52\n0.94\n.36\n0.002\n\n\n\nNote:   F(5, 19) = 68.16; p &lt;.001; R² = 0.947; Adjusted R² = 0.933\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nR Core Team. (2023). R: A language and environment for statistical computing. Retrieved from https://www.R-project.org/"
  },
  {
    "objectID": "ch_scan_package.html#installing-the-scan-package",
    "href": "ch_scan_package.html#installing-the-scan-package",
    "title": "2  The scan package",
    "section": "2.1 Installing the scan package",
    "text": "2.1 Installing the scan package\nYou can use the install.packages function to install scan.\ninstall.packages(\"scan\") will install the stable version.\nThe current stable release is version 0.60.0. Please refer to the Software Reference section to see which version of scan was used to create this book, and make sure you have this or a newer version installed.\nR contains many packages, and it would slow things down considerably if all packages were loaded into memory at the beginning of each R session. Therefore, after installing scan, you need to enable it at the beginning of each session in which you use R. Normally, a session starts when you start the R program and ends when you quit it.\nTo activate a package, you need the library function. In this case, library(scan). You should get something like\n\n\n\u001b[34mscan 0.60.0.9999 (2023-09-22)\n\u001b[31mFor information on citing scan, type citation(\"scan\").\n\n\nindicating that everything went smoothly and scan is ready for work.\nFor creating single-case plots, please install the add-on package scplot with install.packages(\"scplot\")."
  },
  {
    "objectID": "ch_scan_package.html#development-version-of-scan",
    "href": "ch_scan_package.html#development-version-of-scan",
    "title": "2  The scan package",
    "section": "2.2 Development version of scan",
    "text": "2.2 Development version of scan\nAlternatively, you can compile the development version of scan yourself. This may be necessary if the stable version has some bugs or missing features that have been fixed.\nYou may need some computer knowledge to get the development version running. It is hosted on gitHub at &lt;https://github.com/jazznbass/scan&gt;.\nFor installation, you can apply the install_github function from the devtools package (make sure you have installed the devtools package before):\ndevtools::install_github(\"jazznbass/scan\", dependencies = TRUE)\nIf you are using a Windows operating system, you will probably need to install Rtools first. Rtools contains additional programs (e.g. compilers) needed to compile R source packages.\nYou can find Rtools here: &lt;https://cran.r-project.org/bin/windows/Rtools/&gt;"
  },
  {
    "objectID": "ch_scan_package.html#reporting-issues-with-scan-and-suggesting-enhancements",
    "href": "ch_scan_package.html#reporting-issues-with-scan-and-suggesting-enhancements",
    "title": "2  The scan package",
    "section": "2.3 Reporting issues with scan and suggesting enhancements",
    "text": "2.3 Reporting issues with scan and suggesting enhancements\nThe scan gitHub repository at &lt;https://github.com/jazznbass/scan&gt; is the ideal place to report bugs, problems, or ideas for enhancing scan. Please use the issue tool (direct link: &lt;https://github.com/jazznbass/scan/issues&gt;).\nWe are very thankful for any feedback, corrections, or whatever helps to improve scan!"
  },
  {
    "objectID": "ch_scan_package.html#functions-overview",
    "href": "ch_scan_package.html#functions-overview",
    "title": "2  The scan package",
    "section": "2.4 Functions overview",
    "text": "2.4 Functions overview\nThe functions of the scan package can be divided into the following categories:\nManage data, analyze, manipulate, simulate, and depict.\nThe following tables give an overview of the central functions. Furthermore, you can see the current life cycle stage of a function. The life cycle stage categorization is based on the tidyverse package and described in detail here https://lifecycle.r-lib.org/articles/stages.html.\n\n2.4.1 Management\n\n\n\n\nTable 2.1: Functions for data management\n\n\n\n\n\n\n\n\nFunction\nWhat it does …\nLifecycle stage\nChapter\n\n\n\n\nscdf\nCreates a single-case data-frame\nStable\nSection 3.2\n\n\nselect_cases\nSelects specific cases of an scdf\nStable\nSection 4.1\n\n\nselect_phases\nSelects and/or recombines phases\nStable\nSection 8.1.1\n\n\nsubset\nSelects specific measurements or variables of an scdf\nstable\nSection 4.2\n\n\nread_scdf\nLoads external data into an scdf\nStable\nSection 3.4.1\n\n\nwrite_scdf\nWrites scdf into an external file\nStable\nSection 3.4.3\n\n\nconvert\nConverts an scdf object into R syntax\nStable\nSection 3.5\n\n\nset_var\n(Re)sets dependent, measurement, and phase variable of an scdf\nStable\n-\n\n\nadd_l2\nAdds level-two data to an scdf\nStable\nSection 10.0.1\n\n\nas_scdf\nTransforms a data.frame into an scdf\nStable\n-\n\n\nas.data.frame/as.data.frame.scdf\nTransforms an scdf into a data frame\nStable\n-\n\n\n\n\n\n\n\n\n2.4.2 Depiction\n\n\n\n\nTable 2.2: Functions for data depiction/visualisation\n\n\nFunction\nWhat it does ...\nLifecycle stage\n\n\n\n\nplot/plot.scdf\nCreates plots of single cases\nStable\n\n\nscplot\nAdd-on package `scplot`. Create advanced ggplot2 plots.\nStable\n\n\nstyle_plot\nDefines single-case plot graphical styles\nSuperseded\n\n\nexport\nCreates html or latex tables from the output of various can functions\nExperimental\n\n\nprint/print.scdf\nPrints an scdf\nStable\n\n\nsummary/summary.scdf\nSummaizes an scdf\nStable\n\n\nplot_rand\nCreate a distribution plot from a randomization test obejct\nExperimental\n\n\n\n\n\n\n\n\n\n\n2.4.3 Analysis\n\n\n\n\nTable 2.3: Functions for data analysis\n\n\nFunction\nWhat it does ...\nLifecycle stage\n\n\n\n\nautocorr\nAutocorrelations for each phase of each case\nStable\n\n\ncorrected_tau\nBaseline corrected tau\nStable\n\n\ndescribe\nDescriptive statistics for each phase of each case\nStable\n\n\noverlap\nAn overview of overlap indeces for each case\nStable\n\n\nsmd\nVarious standardized mean differences between phase A and B\nStable\n\n\nrci\nReliable change index\nExperimental\n\n\nrand_test\nRandomization test\nStable\n\n\ntau_u\nTau-U for each case and all cases\nStable\n\n\ntrend\nTrend analyses for each case\nStable\n\n\nplm\nPiecewise linear regression model\nStable\n\n\nmplm\nMultivariate piecewise linear regression model\nExperimental\n\n\nhplm\nHierarchical piecewise linear regression model\nStable\n\n\nnap\nNon-overlap of all pairs for each case\nStable\n\n\npnd\nPercentage of non overlapping data for each case\nStable\n\n\npand\nPercentage of all non overlapping data for all cases\nStable\n\n\npem\nPercantage exceeding the mean for each case\nStable\n\n\npet\nPercentage exceeding the trend for each case\nStable\n\n\ncdc\nConservative dual-criterion test\nStable\n\n\noutlier\nDetect outliers for all cases\nStable\n\n\n\n\n\n\n\n\n\n\n2.4.4 Manipulation\n\n\n\n\nTable 2.4: Functions for data manipulation\n\n\nFunction\nWhat it does ...\nLifecycle stage\n\n\n\n\ntransform\nCalculate new and change existing variables\nStable\n\n\nall_cases\nHelper function for transform that executes an expression across all cases of an scdf\nStable\n\n\nacross_cases\nHelper function for transform that calculates a variable for all cases of an scdf\nStable\n\n\nmoving_mean\nHelper function for transform to smooth with moving means\nStable\n\n\nmoving_media\nHelper function for transform to smooth with moving medians\nStable\n\n\nlocal_regression\nHelper function for transform to smooth with local regressions\nStable\n\n\nfill_missing\nInterpolate missign values or missing measurement times\nStable\n\n\nranks\nCovert data into ranked data across all cases\nSuperseded\n\n\ntransform\nChange and create new variabes\nSuperseded\n\n\nsmooth_cases\nSmoothes time series data\nSuperseded\n\n\ntruncate_phase\nDeletes measurements of phases\nSuperseded\n\n\nstandardize\nStandardizes or centers variables across cases\nSuperseded\n\n\n\n\n\n\n\n\n\n\n2.4.5 Simulation\n\n\n\n\nTable 2.5: Functions for data simulation\n\n\nFunction\nWhat it does ...\nLifecycle stage\n\n\n\n\ndesign\nDefines a design of one or multiple single-cases\nStable\n\n\npower_test\nCalculates power and alpha error of a specific analyzes for a specific single-case design\nStable\n\n\nestimate_design\nExtraxt a deisgn template from an existing scdf\nExperimental\n\n\nrandom_scdf\nCreats random single-case studies from a single-case design\nStable\n\n\nmcscan\nAdd-on package `mcscan`. Create Monte-Carlo designs and analyses with `scan`\n(Upcoming not yet functioning)"
  },
  {
    "objectID": "ch_create_display_and_store_scdfs.html#a-single-case-data-frame",
    "href": "ch_create_display_and_store_scdfs.html#a-single-case-data-frame",
    "title": "3  Create, display, and store single-case data",
    "section": "3.1 A single-case data frame",
    "text": "3.1 A single-case data frame\nScan provides its own data-class for encoding single-case data: the single-case data frame (short scdf). An scdf is an object that contains one or multiple single-case data sets and is optimized for managing and displaying these data. Think of an scdf as a file including a separate datasheet for each single case. Each datasheet is made up of at least three variables: The measured values, the phase identifier for each measured value, and the measurement time (mt) of each measure. Optionally, scdfs could include further variables for each single-case (e.g., control variables), and also a name for each case.\n\n\n\n\n\n\nNote\n\n\n\nTechnically, an scdf object is a list containing data frames. It is of the class c(\"scdf\",\"list\"). Additionally, an scdf entails an attribute scdf with a list with further attributes. var.values, var.phase, and var.mt contain the names of the values, phase, and the measurement time variable. By default, these names are set to values, phase, and mt.\n\n\nSeveral functions are available for creating, transforming, merging, and importing/exporting scdfs."
  },
  {
    "objectID": "ch_create_display_and_store_scdfs.html#sec-scdf",
    "href": "ch_create_display_and_store_scdfs.html#sec-scdf",
    "title": "3  Create, display, and store single-case data",
    "section": "3.2 Create single-case data frames",
    "text": "3.2 Create single-case data frames\n\n\n\n\n\n\nThe scdf function call\n\n\n\nscdf(values, B_start, mt, phase, phase_design = NULL, name = NULL, dvar = “values”, pvar = “phase”, mvar = “mt”, …)\n\n\nThe scdf() function is the basic tool for creating a single-case data frame. Basically, you have to provide the measurement values and the phase structure and an scdf object is build. There are three different ways of defining the phase structure. First, defining the beginning of the B-phase with the B_start argument, second, defining a design with the phase_design argument and third, setting parameters in a named vector of the dependent variable.\n\n### Three ways to code the same scdf\nscdf(values = c(A = 2,2,4,5, B = 8,7,6,9,8,7))\nscdf(values = c(2,2,4,5,8,7,6,9,8,7), B_start = 5)\nscdf(values = c(2,2,4,5,8,7,6,9,8,7), phase_design = c(A = 4, B = 6))\n\nThe B_start argument is only applicable when the single-case consists of a single A-phase followed by a B-phase. It is a remnant from the time when scan could only handle sign-case designs with two phases. The number assigned to B_start indicates the measurement-time as defined in the mt argument. That is, assume a vector for the measurement times mt = c(1,3,7,10,15,17,18,20) and B_start = 15 then the first measurement of the B-phase will start with the fifth measurement at which mt = 15.\nThe phase_design argument is a named vector with the name and length of each phase. The phase names can be set arbitrary, although I recommend to use capital letters (A, B, C, …) for each phase followed by, when indicated, a number if the phases repeat (A1, B1, A2, B2, …). Although it is possible to give the same name to more than one phase (A, B, A, B) this might lead to some confusion and errors when coding analyzes with scan.\nWhen the vector of the dependent variable includes named values, a phase_design structure is created automatically. Each named value sets the beginning of a new phase. For example c(A = 3,2,4, B = 5,4,3, C = 6,7,6,5) will create an ABC-phase design with 3, 3, and 4 values per phase.\nUse only one of the three methods at a time and I recommend to use the phase_design argument or the named vector method as they are the most versatile.\nIf no measurement times are specified, they are automatically created as a series 1, 2, 3, …, N, where N is the number of measurements. in some circumstances it might be useful to define individual measurement times for each measurement. For example, if you want to include the days since the beginning of the study as time intervals between measurements are widely varying you might get more valid results this way when analyzing the data in a regression approach.\n\n# example of a more complex design \nscdf(\n  values = c(2,2,4,5, 8,7,6,9,8,7, 12,11,13), \n  mt = c(1,2,3,6, 8,9,11,12,16,18, 27,28,29),\n  phase_design = c(A = 4, B = 6, C = 3)\n)\n\n#A single-case data frame with one case\n\n Case1: values mt phase\n             2  1     A\n             2  2     A\n             4  3     A\n             5  6     A\n             8  8     B\n             7  9     B\n             6 11     B\n             9 12     B\n             8 16     B\n             7 18     B\n            12 27     C\n            11 28     C\n            13 29     C\n\n\nMissing values could be coded using NA (not available).\n\nscdf(values = c(A = 2,2,NA,5, B = 8,7,6,9,NA,7))\n\nMore variables are implemented by adding new variable names with a vector containing the values. Please be aware that a new variable must never have the same name as one of the arguments of the function (i.e. B_start, phase_design, name, dvar, pvar, mvar).\n\nscdf(\n  values = c(A = 2,2,3,5, B = 8,7,6,9,7,7), \n  teacher = c(0,0,1,1,0,1,1,1,0,1), \n  hour = c(2,3,4,3,3,1,6,5,2,2)\n)\n\n#A single-case data frame with one case\n\n Case1: values teacher hour mt phase\n             2       0    2  1     A\n             2       0    3  2     A\n             3       1    4  3     A\n             5       1    3  4     A\n             8       0    3  5     B\n             7       1    1  6     B\n             6       1    6  7     B\n             9       1    5  8     B\n             7       0    2  9     B\n             7       1    2 10     B\n\n\nTable 3.1 shows a complete list of arguments that could be passed to the function.\n\n\n\n\nTable 3.1: Arguments of the scdf function\n\n\nArgument\nWhat it does ...\n\n\n\n\nvalues\nThe default vector with values for the dependent variable. It can be changed with the dvar argument.\n\n\nphase\nUsually, this variable is not defined manually and will be created by the function. It is the default vector with values for the phase variable. It can be changed with the pvar argument.\n\n\nmt\nThe default vector with values for the measurement-time variable. It can be changed with the mvar argument.\n\n\nphase_design\nA vector defining the length and label of each phase.\n\n\nB_start\nThe first measurement of phase B (simple coding if design is strictly AB).\n\n\nname\nA name for the case.\n\n\ndvar\nThe name of the dependent variable. By default this is 'values'.\n\n\npvar\nThe name of the variable containing the phase information. By default this is 'phase'.\n\n\nmvar\nThe name of the variable with the measurement-time. The default is 'mt'.\n\n\n...\nAny number of variables with a vector asigned to them.\n\n\n\n\n\n\n\n\nIf you want to create a dataset comprising several singlecases, the easiest way is to first create an scdf for each case and then merge them into a new scdf using the c command:\n\ncase1 &lt;- scdf(\n  values = c(A = 5, 7, 10, 5, 12, B = 7, 10, 18, 15, 14, 19), \n  name = \"Charlotte\"\n)\ncase2 &lt;- scdf(\n  values = c(A = 3, 4, 3, 5, B = 7, 4, 7, 9, 8, 10, 12), \n  name = \"Theresa\"\n)\ncase3 &lt;- scdf(\n  values = c(A = 9, 8, 8, 7, 5, 7, B = 6, 14, 15, 12, 16), \n  name = \"Antonia\"\n)\nmbd &lt;- c(case1, case2, case3)\n\nIf you want to use other than the default variable names (“values”, “phase” and “mt”), you can define them with the arguments dvar (for the dependent variable), pvar (the variable specifying the phase) and mvar (the measurement time variable).\n\n# Example: Using a different name for the dependent variable\ncase &lt;- scdf(\n  score = c(A = 5, 7, 10, 5, 12, B = 7, 10, 18, 15, 14, 19), \n  dvar = \"score\"\n)\n\n# Example: Using new names for the dependent and the phase variables\ncase &lt;- scdf(\n  score = c(A = 3, 4, 3, 5, B = 7, 4, 7, 9, 8, 10, 12), \n  dvar = \"score\", pvar = \"section\"\n)\n\n# Example: Using new names for dependent, phase, and measurement-time variables\ncase &lt;- scdf(\n  score = c(A = 9, 8, 8, 7, 5, 7, B = 6, 14, 15, 12, 16), \n  name = \"Antonia\", dvar = \"score\", pvar = \"section\", mvar = \"day\"\n)\n\nsummary(case)\n\n#A single-case data frame with one case\n\n         Measurements Design\n Antonia           11    A-B\n\nVariable names:\nscore &lt;dependent variable&gt;\nsection &lt;phase variable&gt;\nday &lt;measurement-time variable&gt;"
  },
  {
    "objectID": "ch_create_display_and_store_scdfs.html#save-and-read-single-case-data-frames",
    "href": "ch_create_display_and_store_scdfs.html#save-and-read-single-case-data-frames",
    "title": "3  Create, display, and store single-case data",
    "section": "3.3 Save and read single-case data frames",
    "text": "3.3 Save and read single-case data frames\nNormally, it is not necessary to save an scdf in a separate file on your computer. In most cases, you can keep the coding of the scdf as described above and run it again each time you work with your data. However, for large files, it is sometimes more convenient to save the data separately in a file for later use.\nThe easiest way is to use the R base functions saveRDS and readRDS for this purpose. saveRDS takes at least two arguments: The first is the object you want to save, and the second is a filename for the resulting file. If you have an scdf named study1, you can use saveRDS(study1, \"study1.rds\") to save the scdf to your drive. You can read this file with study1 &lt;- readRDS(\"study1.rds\"). With getwd() you get the path to the current active folder you are saving and reading data from."
  },
  {
    "objectID": "ch_create_display_and_store_scdfs.html#import-and-export-single-case-data-frames",
    "href": "ch_create_display_and_store_scdfs.html#import-and-export-single-case-data-frames",
    "title": "3  Create, display, and store single-case data",
    "section": "3.4 Import and export single-case data frames",
    "text": "3.4 Import and export single-case data frames\n\n3.4.1 Import data\n\n\n\n\n\n\nThe read_scdf function call\n\n\n\nread_scdf(file, cvar = “case”, pvar = “phase”, dvar = “values”, mvar = “mt”, sort_cases = FALSE, phase_names = NULL, type = NA, na = c(““,”NA”), sort.labels = NULL, phase.names = NULL, …)\n\n\nWhen you are working with other programs besides R you need to export and import the scdf into a common file format. read_scdf imports a comma-separated-variable (csv) file and converts it into an scdf object. By default, the csv-file has to contain the columns case, phase, and values. Optionally, a further column named mt could be provided. The csv file should be build up like this:\n\n\n\nFigure 3.1: How to format a single-case file in a spreadsheet program for importing into scan\n\n\nIn case your variables names differ from the standard (i.e. “case”, “values”, “phase”, and “mt” ), you could set additional arguments to fit your file. read_scdf(\"example.csv\", cvar = \"name\", dvar = \"wellbeing\", pvar = \"intervention\", mvar = \"time\") for example will set the variables attributes of the resulting scdf. Cases will be split by the variable \"name\", \"wellbeing\" is set as the dependent variable (default is values), phase information are in the variable \"intervention\", and measurement times in the variable \"time\". You could also reassign the phase names within the phase variable by setting the argument phase.names. Assume for example your file contains the values 0 and 1 to identify the two phases I recommend to set them to “A” and “B” with read_scdf(\"example.csv\", phase.names = c(\"A\", \"B\")).\nFor some reasons, computer systems with a German (and some other) language setups export csv-files by default with a comma as a decimal point and a semicolon as a separator between values. In these cases you have to set two extra arguments to import the data:\nread_scdf(\"example.csv\", dec = \",\", sep = \";\")\n\n\n3.4.2 Other data formats\nread_scdf also allows for directly importing Microsoft Excel .xlsx or .xls files. You need to have the library readxl installed in your R setup for this to work. Excel files will be automatically detected by the filename extension xlsor xlsx or by explicitly setting the type argument (e.g. type = \"xlsx\").\n\ndat &lt;- read_scdf(\n  \"example2.xlsx\", cvar = \"name\", pvar = \"intervention\", \n  dvar = \"wellbeing\", mvar = \"time\", phase.names = c(\"A\",\"B\")\n)\n\nImported 20 cases\n\nsummary(dat)\n\n#A single-case data frame with 20 cases\n\n          Measurements Design\n Charles            20    A-B\n Kolten             20    A-B\n Annika             20    A-B\n Kaysen             20    A-B\n Urijah             20    A-B\n Leila              20    A-B\n Leia               20    A-B\n Aleigha            20    A-B\n Greta              20    A-B\n Alijah             20    A-B\n... [skipped 10 cases]\n\nVariable names:\nwellbeing &lt;dependent variable&gt;\nintervention &lt;phase variable&gt;\ntime &lt;measurement-time variable&gt;\nage\ngender\ngym\n\n\nBasically, you can import data from any file format with the help of additional R packages. Here are two examples:\n\n# Open document example. You need to have the readODS package installed.\ndf &lt;- readODS::read_ods(\"filename.ods\")\nscdf &lt;- as_scdf(df)\n\n# SPSS example. You need to have the haven package installed.\ndf &lt;- haven::read_sav(\"filename.sav\")\nscdf &lt;- as_scdf(df)\n\n\n\n3.4.3 Export data\n\n\n\n\n\n\nThe write_scdf function call\n\n\n\nwrite_scdf(data, filename = NULL, sep = “,”, dec = “.”, …)\n\n\nwrite_scdf() exports an scdf object as a comma-separated-variables file (csv) which can be imported into any other software for data analyses (MS OFFICE, Libre Office etc.). The scdf object is converted into a single data frame with a case variable identifying the rows for each subject. The first argument of the command identifies the scdf to be exported and the second argument (file) the name of the resulting csv-file. If no file argument is provided, a dialog box is opened to choose a file interactively. By default, writeSC exports into a standard csv-format with a dot as the decimal point and a comma for separating variables. If your system expects a comma instead of a point for decimal numbers you may use the dec and the sep arguments. For example, write_scdf(example, file = \"example.csv\", dec = \",\", sep = \";\") exports a csv variation usually used for example in Germany.\n\n\n3.4.4 Other data formats\nThe R system has many add on packages that allow to write data to almost any file format available. If you like to export an scdf in those formats, you firstly need to convert the scdf into a standard R data-frame with the as.data.frame() function. Now you can export the resulting data-frame applying the respective function from another package.\n\ndf &lt;- as.data.frame(exampleABC)\n\n# Open document example. You need to have the readODS package installed.\nreadODS::write_ods(df, \"filename.ods\")\n\n# Excel example. You need to have the openxlsx package installed.\nopenxlsx::write.xlsx(df, \"filename.xlsx\")\n\n# SPSS example. You need to have the haven package installed.\nhaven::write_sav(df, \"filename.sav\")"
  },
  {
    "objectID": "ch_create_display_and_store_scdfs.html#sec-convert",
    "href": "ch_create_display_and_store_scdfs.html#sec-convert",
    "title": "3  Create, display, and store single-case data",
    "section": "3.5 Convert an scdf object back to scan syntax",
    "text": "3.5 Convert an scdf object back to scan syntax\n\n\n\n\n\n\nThe convert function call\n\n\n\nconvert(scdf, file = ““, study_name =”study”, case_name = “case”, inline = FALSE, indent = 2, silent = FALSE)\n\n\nYou can also reconvert an scdf object back to “raw” scan syntax. This is a convenient way when you imported data from an Excel or csv file and want to keep everything clean and transparent within your R syntax files.\nHere is an example:\n\nconvert(exampleABC)\n\ncase1 &lt;- scdf(\n  values = c(\n    58, 56, 60, 63, 51, 45, 44, 59, 45, 39, 83, 65, 70, 83, 70, 85, 47, 66,\n    77, 75, 51, 87, 80, 68, 70, 56, 52, 70, 83, 63\n  ),\n  phase_design = c(A = 10, B = 10, C = 10),\n  name = \"Marie\"\n)\n\ncase2 &lt;- scdf(\n  values = c(\n    47, 41, 47, 52, 54, 65, 55, 37, 51, 60, 60, 65, 55, 46, 49, 54, 77, 73,\n    97, 64, 84, 71, 66, 74, 78, 68, 52, 76, 63, 54\n  ),\n  phase_design = c(A = 15, B = 8, C = 7),\n  name = \"Rosalind\"\n)\n\ncase3 &lt;- scdf(\n  values = c(\n    50, 45, 63, 53, 66, 57, 35, 45, 74, 63, 47, 45, 47, 36, 51, 55, 35, 66,\n    59, 55, 73, 60, 85, 62, 79, 69, 87, 76, 90, 48\n  ),\n  phase_design = c(A = 20, B = 7, C = 3),\n  name = \"Lise\"\n)\n\nstudy &lt;- c(\n  case1, case2, case3\n) \n\n\nNow you can copy and past the output into your R file or you set the file argument to save the output into an R file convert(exampleABC, file = \"scdf.R\")."
  },
  {
    "objectID": "ch_create_display_and_store_scdfs.html#display-single-case-data-frames",
    "href": "ch_create_display_and_store_scdfs.html#display-single-case-data-frames",
    "title": "3  Create, display, and store single-case data",
    "section": "3.6 Display single-case data frames",
    "text": "3.6 Display single-case data frames\nscdf are displayed by just typing the name of the object.\n\n#Beretvas2008 is an example scdf included in scan\nBeretvas2008\n\n#A single-case data frame with one case\n\n Case1: values mt phase\n           0.7  1     A\n           1.6  2     A\n           1.4  3     A\n           1.6  4     A\n           1.9  5     A\n           1.2  6     A\n           1.3  7     A\n           1.6  8     A\n            10  9     B\n          10.8 10     B\n          11.9 11     B\n            11 12     B\n            13 13     B\n          12.7 14     B\n            14 15     B\n\n\nThe print command allows you to specify the output. Some possible arguments are cases (the number of cases to display; three by default), rows (the maximum number of rows to display; fifteen by default), and digits (the number of digits). cases = 'all' and rows = 'all' prints all cases and rows.\n\n# Huber2014 is an example scdf included in scan\nprint(Huber2014, cases = 2, rows = 10)\n\n#A single-case data frame with four cases\n\n Adam: mt compliance phase ｜ Berta: mt compliance phase ｜\n        1         25     A ｜         1         25     A ｜\n        2       20.8     A ｜         2       20.8     A ｜\n        3       39.6     A ｜         3       39.6     A ｜\n        4         75     A ｜         4         75     A ｜\n        5         45     A ｜         5         45     A ｜\n        6       39.6     A ｜         6       14.6     A ｜\n        7       54.2     A ｜         7       45.8     A ｜\n        8         50     A ｜         8       33.3     A ｜\n        9       28.1     A ｜         9       31.3     A ｜\n       10         40     A ｜        10       32.5     A ｜\n# ... up to 66 more rows\n#  two more cases\n\n\nThe argument long = TRUE prints each case one after the other instead of side by side (e.g., print(exampleAB, long = TRUE)).\nA short description of the scdf is provided by the summary command. The results are pretty much self explaining:\nsummary() gives a very concise overview of an scdf\n\nsummary(Huber2014)\n\n#A single-case data frame with four cases\n\n           Measurements Design\n Adam                37    A-B\n Berta               29    A-B\n Christian           76    A-B\n David               76    A-B\n\nVariable names:\ncompliance &lt;dependent variable&gt;\nphase &lt;phase variable&gt;\nmt &lt;measurement-time variable&gt;\n\nNote: Behavioral data (compliance in percent). \n\nAuthor of data: Christian Huber"
  },
  {
    "objectID": "ch_working_with_scdfs.html#sec-select-cases",
    "href": "ch_working_with_scdfs.html#sec-select-cases",
    "title": "4  Working with single-case data frames",
    "section": "4.1 Select cases",
    "text": "4.1 Select cases\nYou can extract one or more single-cases from an scdf with multiple cases in two ways.\nThe first method follows the basic rules of the R syntax. If the case has a name, you can address it with the $ operator\n\nHuber2014$David\n\nor you can use squared brackets to select by the number (its position) of a case\n\nHuber2014[1] #extracts case 1\nHuber2014[2:3] #extracts cases 2 and 3\n\n\nnew.huber2014 &lt;- Huber2014[c(1, 4)] #extracts cases 1 and 4\nnew.huber2014\n\n#A single-case data frame with two cases\n\n Adam: mt compliance phase ｜ David: mt compliance phase ｜\n        1         25     A ｜         1       65.6     A ｜\n        2       20.8     A ｜         2       37.5     A ｜\n        3       39.6     A ｜         3       58.3     A ｜\n        4         75     A ｜         4       72.9     A ｜\n        5         45     A ｜         5       33.3     A ｜\n        6       39.6     A ｜         6       59.4     A ｜\n        7       54.2     A ｜         7       77.1     A ｜\n        8         50     A ｜         8       54.2     A ｜\n        9       28.1     A ｜         9       68.8     A ｜\n       10         40     A ｜        10       43.8     A ｜\n       11       52.1     B ｜        11       62.5     B ｜\n       12       31.3     B ｜        12       64.6     B ｜\n       13       15.6     B ｜        13       60.4     B ｜\n       14       29.2     B ｜        14       81.3     B ｜\n       15       43.8     B ｜        15       79.2     B ｜\n# ... up to 61 more rows\n\n\nThe second method is to use the select_cases function.\n\n\n\n\n\n\nThe select_cases function call\n\n\n\nselect_cases(scdf, …)\n\n\n\n\n\n\n\n\nNote\n\n\n\nSince version 0.53, scan includes functions to work with pipe-operators. scan imports the pipe operator %&gt;% from the magrittr package. Alternatively, you can use R’s native pipe operator |&gt;.\n\n\nThe select_cases() function takes case names and/or numbers for selecting cases:\n\n# With pipes:\nHuber2014 %&gt;%\n  select_cases(Adam, Berta, 4) %&gt;%\n  summary()\n\n#A single-case data frame with three cases\n\n       Measurements Design\n Adam            37    A-B\n Berta           29    A-B\n David           76    A-B\n\nVariable names:\ncompliance &lt;dependent variable&gt;\nphase &lt;phase variable&gt;\nmt &lt;measurement-time variable&gt;\n\nNote: Behavioral data (compliance in percent). \n\nAuthor of data: Christian Huber \n\n# 1. Take the scdf Huber2014,\n# 2. select the cases Adam, Berta and case number four,\n# 3. show a summary of the remaining cases in the study. \n\nCase names can also be defined within a specific range by the colon operator:\n\nHuber2014 %&gt;%\n  select_cases(Berta:David) %&gt;%\n  summary()\n\n#A single-case data frame with three cases\n\n           Measurements Design\n Berta               29    A-B\n Christian           76    A-B\n David               76    A-B\n\nVariable names:\ncompliance &lt;dependent variable&gt;\nphase &lt;phase variable&gt;\nmt &lt;measurement-time variable&gt;\n\nNote: Behavioral data (compliance in percent). \n\nAuthor of data: Christian Huber"
  },
  {
    "objectID": "ch_working_with_scdfs.html#sec-subset",
    "href": "ch_working_with_scdfs.html#sec-subset",
    "title": "4  Working with single-case data frames",
    "section": "4.2 Select measurements",
    "text": "4.2 Select measurements\n\n\n\n\n\n\nThe subset function call\n\n\n\nsubset(x, …)\n\n\n\n\n\n\n\n\nNote\n\n\n\nThe subset function is a method for the generic subset function. To call the help file you have to add the class to the function name: ?subset.scdf\n\n\nThe subset() function helps to extract measurements (or rows) from an scdf according to specific criteria.\nSubset takes an scdf as the first argument and a logical expression (filter) as the second argument. Only measurements for which the logical argument is true are included in the returned scdf object.\nFor example, the scdf Huber2014 has a variable compliance and we want to keep measurements where compliance is greater than 10 because we assume the others are outliers:\n\nHuber2014 %&gt;%\n  subset(compliance &gt; 10) %&gt;%\n  summary()\n\n#A single-case data frame with four cases\n\n           Measurements Design\n Adam                37    A-B\n Berta               20    A-B\n Christian           76    A-B\n David               76    A-B\n\nVariable names:\ncompliance &lt;dependent variable&gt;\nphase &lt;phase variable&gt;\nmt &lt;measurement-time variable&gt;\n\nNote: Behavioral data (compliance in percent). \n\nAuthor of data: Christian Huber \n\n\nIn a more complex example, we want to keep only values less than 60 when they are in phase A, or values equal to or greater than 60 when they are in phase B:\n\nexampleAB %&gt;%\n  subset((values &lt; 60 & phase == \"A\") | (values &gt;= 60 & phase == \"B\")) %&gt;%\n  summary()\n\n#A single-case data frame with three cases\n\n          Measurements Design\n Johanna            20    A-B\n Karolina           18    A-B\n Anja               19    A-B\n\nVariable names:\nvalues &lt;dependent variable&gt;\nphase &lt;phase variable&gt;\nmt &lt;measurement-time variable&gt;\n\nNote: Randomly created data with normal distributed dependent variable."
  },
  {
    "objectID": "ch_working_with_scdfs.html#change-and-create-variables",
    "href": "ch_working_with_scdfs.html#change-and-create-variables",
    "title": "4  Working with single-case data frames",
    "section": "4.3 Change and create variables",
    "text": "4.3 Change and create variables\n\n\n\n\n\n\nThe transform function call\n\n\n\ntransform(_data, …)\n\n\nWith the help of the transform() function, you can add new variables or change existing variables for each case of an scdf. This can be useful if you want to\n\nz-standardize a variable,\ncalculate a new variable as the sum of two existing variables\nconvert a frequency to a percentage,\n\nor in many other cases.\n\n\n\n\n\n\nNote\n\n\n\nThe transform function is a method for the generic transform function. To call the help file you have to add the class to the function name: ?transform.scdf\n\n\nHere is an example of standardizing the dependent variable “values”:\n\nexampleAB_z &lt;- transform(\n  exampleAB, values = (values-mean(values)) / sd(values)\n)\n\n# note: alternatively for the same result:\n# exampleAB_z &lt;- transform(exampleAB, values = scale(values))\n\nHere is an example where a new percentage variable is added and the measurement times shifted to start with 0:\n\nexampleAB_score %&gt;%\n  transform(\n    percentage = values / trials * 100,\n    mt = mt - mt[1]\n  )\n\n#A single-case data frame with three cases\n\n Christiano: values trials mt phase percentage\n                  1     20  0     A          5\n                  3     20  1     A         15\n                  3     20  2     A         15\n                  3     20  3     A         15\n                  5     20  4     A         25\n                  3     20  5     A         15\n                  0     20  6     A          0\n                  2     20  7     A         10\n                  4     20  8     A         20\n                  3     20  9     A         15\n                 12     20 10     B         60\n                 13     20 11     B         65\n                 15     20 12     B         75\n                 11     20 13     B         55\n                 15     20 14     B         75\n# ... up to 15 more rows\n#  two more cases\n\n\n\n4.3.1 all_cases\nThe all_cases helper function returns the values of a variable across all cases. This allows for calculations where you need values within a case and values across cases, for example when you want to standardize a variable based on all cases:\n\nexampleAB %&gt;%\n  transform(\n    values = (values - mean(all_cases(values))) / sd(all_cases(values))\n  ) %&gt;%\n  setNames(paste0(names(exampleAB), \"_z\")) %&gt;%\n  c(exampleAB) %&gt;%\n  smd()\n\nStandardized mean differences\n\n                            Johanna_z Karolina_z Anja_z Johanna Karolina  Anja\nmA                             -1.194     -1.431 -1.279   54.60    51.80 53.60\nmB                              0.454      0.398  0.449   74.13    73.47 74.07\nsdA                             0.203      0.577  0.257    2.41     6.83  3.05\nsdB                             0.755      0.824  0.639    8.94     9.76  7.57\nsd cohen                        0.553      0.711  0.487    6.55     8.43  5.77\nsd hedges                       0.673      0.776  0.577    7.97     9.19  6.83\nGlass' delta                    8.111      3.171  6.711    8.11     3.17  6.71\nHedges' g                       2.451      2.357  2.996    2.45     2.36  3.00\nHedges' g correction            2.348      2.258  2.869    2.35     2.26  2.87\nHedges' g durlak correction     2.227      2.142  2.722    2.23     2.14  2.72\nCohen's d                       2.983      2.572  3.545    2.98     2.57  3.54\n\n# 1. Take the exampleAB scdf,\n# 2. Z-standardise the values of each case based on all measurements,\n# 3. rename the cases by adding a \"_z\" suffix,\n# 4. add the original untransformed cases,\n# 5. analyse the data by calculating measures of standardized mean differences.\n\n\n\n4.3.2 Smoothing\nFor smoothing the data dependent variable, transform has a number of helper functions:\n\nmoving_mean calculates the moving median of a series of values. The lag argument specifies the number of values from which to calculate the mean (the default is 1, where the mean is calculated from a value and a measurement before and after that value),\nmoving_median is the same as before, but calculates the median instead of the mean,\nlocal_regression regresses each value on the surrounding values. The argument f defines the fraction of the values (the default f = 0.2 considers the surrounding 20% of the values). You must also provide the measurement time variable with the argument mt.\n\n\ntransform(Huber2014,\n  \"compliance (moving median)\" = moving_median(compliance),\n  \"compliance (moving mean)\" = moving_mean(compliance),\n  \"compliance (local regression)\" = local_regression(compliance, mt = mt)\n)\n\n#A single-case data frame with four cases\n\n Adam: mt compliance phase compliance (moving median) compliance (moving mean)\n        1         25     A                         25                       25\n        2       20.8     A                         25                    28.47\n        3       39.6     A                       39.6                    47.69\n        4         75     A                         45                     55.9\n        5         45     A                         45                    46.83\n        6       39.6     A                         45                    46.88\n        7       54.2     A                         50                    50.36\n        8         50     A                         50                    42.82\n        9       28.1     A                         40                    36.97\n       10         40     A                         40                    43.02\n       11       52.1     B                         40                    42.14\n       12       31.3     B                       31.3                    29.68\n       13       15.6     B                       29.2                    24.83\n       14       29.2     B                       29.2                    32.61\n       15       43.8     B                       29.2                     33.8\n compliance (local regression)\n                         22.51\n                         28.81\n                         34.49\n                         40.55\n                         44.56\n                         46.31\n                         46.14\n                         43.98\n                         42.21\n                         40.06\n                         37.24\n                         33.11\n                         29.56\n                         29.11\n                         28.94\n# ... up to 61 more rows\n#  three more cases\n\n\n\n\n4.3.3 Transform values at the begining of a phase\nThe first_of helper function is specifically designed to replace values at or around the beginning of a phase. The first argument is a logical vector defining a selection criterion. The positions argument is a vector of positions to be addressed. Negative numbers refer to positions before and positive numbers to positions after the selection criteria. This is useful, for example, if you want to discard the first two measurements of a phase.\nHere is an example that replaces the values at the beginning of phase A and the value after that to missing (NA), and also replaces the values at the beginning of phase B and the value before that to NA:\n\nbyHeart2011 %&gt;%\n  transform(\n    values = replace(values, first_of(phase == \"A\", 0:1), NA),\n    values = replace(values, first_of(phase == \"B\", -1:0), NA)\n  )\n\n#A single-case data frame with 11 cases\n\n Lisa (Turkish): values mt phase ｜ Patrick (Spanish): values mt phase ｜\n                   &lt;NA&gt;  1     A ｜                      &lt;NA&gt;  1     A ｜\n                   &lt;NA&gt;  2     A ｜                      &lt;NA&gt;  2     A ｜\n                      0  3     A ｜                         3  3     A ｜\n                      0  4     A ｜                         0  4     A ｜\n                   &lt;NA&gt;  5     A ｜                      &lt;NA&gt;  5     A ｜\n                   &lt;NA&gt;  6     B ｜                      &lt;NA&gt;  6     B ｜\n                      5  7     B ｜                         8  7     B ｜\n                      6  8     B ｜                         8  8     B ｜\n                      7  9     B ｜                         8  9     B ｜\n                     10 10     B ｜                        12 10     B ｜\n                     10 11     B ｜                        13 11     B ｜\n                     15 12     B ｜                        13 12     B ｜\n                     16 13     B ｜                        15 13     B ｜\n                     14 14     B ｜                        14 14     B ｜\n                     17 15     B ｜                        15 15     B ｜\n# ... up to 11 more rows\n#  nine more cases"
  },
  {
    "objectID": "ch_scplot.html#install-scplot",
    "href": "ch_scplot.html#install-scplot",
    "title": "5  Creating a single-case data plot",
    "section": "5.1 Install scplot",
    "text": "5.1 Install scplot\nsplot is available from the CRAN repository. Execute install.packages(\"scplot\") to install it.\nIf you are more adventures you can install the developmental version of scplot from github.The project is hosted at https://github.com/jazznbass/scplot. You can install it with devtools::install_github(\"jazznbass/scplot\") from your R console. Make sure you have the package devtools installed before. The scplot package has to be compiled. When you are running R on a Windows machine you also have to install Rtools. Rtools is not an R package and can be downloaded from CRAN at https://cran.r-project.org/bin/windows/Rtools/.\nThe following chapter has been written with scplot version 0.3.7. If you have problems replicating the examples, please update to this version."
  },
  {
    "objectID": "ch_scplot.html#basic-principal",
    "href": "ch_scplot.html#basic-principal",
    "title": "5  Creating a single-case data plot",
    "section": "5.2 Basic principal",
    "text": "5.2 Basic principal\nYou start by providing an scdf object (see Section 3.2) to the scplot() function (e.g. scplot(exampleAB)). This already creates a default plot.\n\nscplot(exampleAB)\n\n\n\n\nNow you use a series of pipe-operators (%&gt;% or |&gt;) to apply functions that add elements and change characteristics of the resulting plot. For example:\n\nscplot(exampleABC) |&gt;\n  add_title(\"My plot\") |&gt;\n  set_xlabel(\"Days\", color = \"red\", size = 1.3)\n\nHere is an overview of possible functions:\n\n\n\n\nTable 5.1: scplot functions\n\n\nFunction\nWhat it does ...\n\n\n\n\nset_dataline\nChange the default dataline/ add an additional dataline\n\n\nadd_statline\nAdd a line or curve representing statistical parameters\n\n\nadd_arrow\nAdd an arrow to a specific case at a specific position\n\n\nadd_line\nAdd a line to a specific case\n\n\nadd_grid\nAdd a grid to the plot pannel\n\n\nadd_labels\nAdd value labels to each data-point\n\n\nadd_legend\nAdd a plot legend\n\n\nadd_marks\nMark specific data points of specific cases\n\n\nadd_ridge\nColour the area below the dataline\n\n\nadd_text\nAdd text to a specific case at a specific position\n\n\nadd_title\nAdd a title above the plot\n\n\nadd_caption\nAdd a caption below the plot\n\n\nset_xlabel/ set_ylabel\nChange and style axis labels\n\n\nset_xaxis/ set_yaxis\nSet the value range, increments etc. of the x- and y-axis\n\n\nset_background\nSet colour and texture of the plot background\n\n\nset_panel\nSet colour and texture of the plot panel\n\n\nset_phasenames\nRename and style the phases\n\n\nset_casenames\nRename and style the phases\n\n\nset_separator\nStyle the vertical separator line between phases\n\n\nset_theme\nApply a predefined visual theme\n\n\nset_theme_element\nStyle specific elements of the plot\n\n\nas_ggplot\nReturn a ggplot2 object for further processing\n\n\nnew_theme\nCreate/define a new visual theme\n\n\n\n\n\n\n\n\nAll text, line, dot, and area elements have a set of arguments to change visual characteristics.\nText arguments can be applied to the following functions: add_caption(), add_labels(), add_legend(), add_text(), add_title(), set_xlabel(), set_ylabel(), set_phasenames(), set_casenames().\nPossible arguments are:\n\n\n\n\nTable 5.2: Arguments for text elements\n\n\nArgument\nWhat it does ...\n\n\n\n\ncolor\nChange color. Either a color name or a color code (e.g. 'red' or '#110044').\n\n\nsize\nRelativ size to the base text size.\n\n\nfamily\nThe font ('serif', 'sans', 'mono')\n\n\nface\nThe font face (\"plain\",\"bold\",\"italic\",\"bold.italic\")\n\n\nhjust\nHorizontal alignment (0 = left, 0.5 = centered, 1 = right)\n\n\nvjust\nVertical alignment (0 = upper, 0.5 = centered, 1 = lower)\n\n\n\n\n\n\n\n\nLine arguments can be applied to the following functions: set_dataline(), add_statline(), add_line(), add_arrow(), add_ridge(), add_ridge(), set_xaxis(), set_yaxis(), set_separator().\n\n\n\n\nTable 5.3: Arguments for line elements\n\n\nArgument\nWhat it does ...\n\n\n\n\ncolor\nEither a color name or a color code (e.g. 'red' or '#110044').\n\n\nlinewidth\nRelativ width of the line.\n\n\nlinetype\nLinetype ('solid', 'dashed', 'dotted')\n\n\n\n\n\n\n\n\nPoint arguments can be applied to the following functions: set_dataline(), add_statline(), add_marks(), add_arrow().\n\n\n\n\nTable 5.4: Arguments for point elements\n\n\nArgument\nWhat it does ...\n\n\n\n\ncolor\nEither a color name or a color code (e.g. 'red' or '#110044').\n\n\nsize\nRelative size.\n\n\nshpae\nPoint shape.\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 5.1: Some possible shapes"
  },
  {
    "objectID": "ch_scplot.html#set-and-add-datalines",
    "href": "ch_scplot.html#set-and-add-datalines",
    "title": "5  Creating a single-case data plot",
    "section": "5.3 Set and add datalines",
    "text": "5.3 Set and add datalines\n\n\n\n\n\n\nThe set_dataline function call\n\n\n\nset_dataline(object, variable = NULL, line, point, type = “continuous”, …)\n\n\nBy default, the single-case plot will depict the main dependent variable as defined in the scdf object. For changing this default behaviour or adding a second data line, use the set_dataline() function. The function takes the argument variable (with the main dependent variable as a default) which must correspond to a variable name within the applied scdf.\n\nscplot(exampleAB_add) |&gt;\n  set_dataline(\"depression\")\n\n\n\n\nStyling parameters like line and point colour will be set automatically based on the applied graphic theme. We will learn later about how to change and modify these themes. If you want to directly change the styling parameters, you can use the line and point arguments which take lists with styling parameters. For line, the parameters are colour, linewidth, linetype, lineend, and arrow. For point the parameters are colour, size, and shape.\n\nscplot(exampleAB_add) |&gt;\n  set_dataline(\n    line = list(colour = \"darkred\", linewidth = 2), \n    point = list(colour = \"black\", size = 3, shape = 15)\n  )"
  },
  {
    "objectID": "ch_scplot.html#add-statlines",
    "href": "ch_scplot.html#add-statlines",
    "title": "5  Creating a single-case data plot",
    "section": "5.4 Add statlines",
    "text": "5.4 Add statlines\n\n\n\n\n\n\nThe add_statline function call\n\n\n\nadd_statline(object, stat = c(“mean”, “median”, “min”, “max”, “quantile”, “sd”, “mad”, “trend”, “trendA”, “trendA theil-sen”, “moving mean”, “moving median”, “loreg”, “lowess”, “loess”), phase = NULL, color = NULL, linewidth = NULL, linetype = NULL, variable = NULL, …)\n\n\n\n5.4.1 Lines indicating a constant for each phase\nPossible functions: mean, min, max, median, sd, quantile\n\nscplot(exampleABC) |&gt;\n  add_statline(\"mean\") |&gt;\n  add_statline(\"max\") |&gt;\n  add_statline(\"min\") |&gt; \n  add_statline(\"median\")\n\n\n\n\n\n\n5.4.2 Lines indicating a constant for a specific phase\nSet the phase argument with one or multiple phase-names or phase-numbers\nPossible functions: mean, min, max, quantile\nThe following example sets a line with the mean of phase A, the maximum of phases B and C and the minimum of phases 2 and 3:\n\nscplot(exampleABC) |&gt;\n  add_statline(\"mean\", phase = \"A\") |&gt;\n  add_statline(\"max\", phase = c(\"B\", \"C\")) |&gt;\n  add_statline(\"min\", phase = c(2, 3)) |&gt; add_legend()\n\n\n\n\n\n\n5.4.3 Trend-lines\ntrend (separate trend-line for each phase), trendA (extrapolated trend-line of first phase):\n\nscplot(exampleABC) |&gt;\n  add_statline(\"trend\") |&gt;\n  add_statline(\"trendA\")\n\n\n\n\nYou scan specify various methods with the method argument for the trendA statistic:\n\nscplot(exampleABC) |&gt; \n  add_statline(\"trendA\") |&gt; \n  add_statline(\"trendA\", method = \"theil-sen\") |&gt; \n  add_statline(\"trendA\", method = \"bisplit\") |&gt; \n  add_statline(\"trendA\", method = \"trisplit\") |&gt; \n  add_legend()\n\n\n\n\nFor the trend statistic you can set method = \"theil-sen\" for median based Theil-Sen slope lines.\n\n\n5.4.4 Smoothed curves\nPossible functions: moving mean, moving median, loess, lowess:\n\nscplot(exampleABC) |&gt;\n  add_statline(\"loess\") |&gt;\n  add_statline(\"moving mean\")\n\n\n\n\n\n\n5.4.5 Refine with additional arguments\nSome of the statistics allow additional arguments to specify parameters:\n\n\n\n\n\n\n\n\nStatistic\nArgument\nWhat it does …\n\n\n\n\nmean\ntrim\nTrims the mean. trim = 0.10 calculates a 10$ trimmed mean.\n\n\nquantile\nprobs\nProbability. probs = 0.25 calculates the 25% quantile.\n\n\nmoving mean, moving median\nlag\nLag surrounding the estimated value. lag = 2 will calculate mean or median based on the two values before and after the to be replaced value.\n\n\nloess\nspan\nProportion of the surrounding point to estimate a value.\n\n\nlowess\nf\nProportion of the surrounding point to estimate a value.\n\n\n\n\nscplot(exampleABC) |&gt;\n  add_statline(\"moving mean\", lag = 1) |&gt;\n  add_statline(\"quantile\", probs = 0.75)\n\n\n\n\n\n\n5.4.6 Specify data-line\nIf you do not specify the variable argument, the default first data-line is addressed.\n\nscplot(exampleAB_add) |&gt;\n  set_dataline(\"cigarrets\") |&gt;\n  add_statline(\"mean\", variable = \"cigarrets\") |&gt;\n  add_statline(\"trend\")"
  },
  {
    "objectID": "ch_scplot.html#annotate-and-mark",
    "href": "ch_scplot.html#annotate-and-mark",
    "title": "5  Creating a single-case data plot",
    "section": "5.5 Annotate and mark",
    "text": "5.5 Annotate and mark\n\n5.5.1 Add marks\nThe positions argument can take a numeric vector:\n\nscplot(exampleABC) |&gt;\n  add_marks(case = 1, positions = c(7, 12)) |&gt;\n  add_marks(case = 3, positions = c(3, 17), color = \"blue\", size = 7)\n\n\n\n\nThe positions argument can also be a string containing a logical expression. This will be evaluated and the respective positions will be marked.\n\nscplot(exampleABC) |&gt;\n  add_marks(case = 1, positions = \"mt &gt; 15\") |&gt;\n  add_marks(case = 2, positions = 'phase == \"B\"', color = \"green\", size = 5) |&gt;\n  add_marks(case = 3, positions = \"values &gt; quantile(values, probs = 0.80)\", color = \"blue\", size = 7) |&gt;\n  add_marks(case = \"all\", positions = \"values &lt; quantile(values, probs = 0.20)\", color = \"yellow\", size = 7) |&gt;\n  add_caption(\"Note.\nred: mt &gt; 15 in case 1; \ngreen: phase 'B' in case 2; \nblue: values &gt; 80% quantile of case 3; \nyellow: values &lt; 20% quantile of all cases\")\n\n\n\n\nAnd the positions argument can take the results from a scan outlier analyses and mark the positions of the outliers of each case:\n\nscplot(exampleABC_outlier) |&gt; \n  add_marks(positions = outlier(exampleABC_outlier), size = 3)\n\n\n\n\n\n\n5.5.2 Add text\n\nscplot(exampleABC) |&gt;\n  add_text(\"Here!\", case = 2, x = 10, y = 80, color = \"red\")\n\n\n\n\n\n\n5.5.3 Add line\nDraw lines either by providing starting (x0 and y0) an end coordinates (x1 and y1) or a horizontal (hline) or vertical (vline) position:\n\nscplot(exampleABC) |&gt;\n  add_line(case = 1, x0 = 6, y0 = 90, x1 = 3, y1 = 63, color = \"red\") |&gt; \n  add_line(case = 2, hline = 80, color = \"blue\") |&gt; \n  add_line(case = 3, vline = 15, color = \"green\")\n\n\n\n\nDraw an arrow:\n\nscplot(exampleABC) |&gt;\n  add_arrow(case = 1, x0 = 6, y0 = 90, x1 = 3, y1 = 63) |&gt;\n  add_text(\"Problem\", case = 1, x = 6, y = 94, color = \"red\", size = 1, hjust = 0 )"
  },
  {
    "objectID": "ch_scplot.html#change-appearance-of-basic-plot-elements",
    "href": "ch_scplot.html#change-appearance-of-basic-plot-elements",
    "title": "5  Creating a single-case data plot",
    "section": "5.6 Change appearance of basic plot elements",
    "text": "5.6 Change appearance of basic plot elements\n\n5.6.1 Data line\n\nscplot(exampleABC) |&gt;\n  set_dataline(color = \"blue\", linewidth = 1, linetype = \"dotted\", \n               point = list(colour = \"red\", size = 1, shape = 2) )\n\n\n\n# Equivalent_\n# scplot(exampleABC) |&gt;\n#   set_dataline(line = list(colour = \"blue\", size = 1, linetype = \"dotted\"), \n#                point = list(colour = \"red\", size = 1, shape = 2)) \n\n\n\n5.6.2 Background\n\nscplot(exampleABC) |&gt;\n  set_background(fill = \"grey90\", color = \"black\", size = 2)\n\n\n\n\n\n\n5.6.3 Panel\n\nscplot(exampleABC) |&gt;\n  set_panel(fill = \"tan1\", color = \"palevioletred\", size = 2)\n\n\n\n\n\n\n5.6.4 A different panel color for each phase\nNote: The colors are 50% transparent. So they might appear different.\n\nscplot(exampleABC) |&gt;\n  set_panel(fill = c(\"grey80\", \"white\", \"blue4\"))"
  },
  {
    "objectID": "ch_scplot.html#themes",
    "href": "ch_scplot.html#themes",
    "title": "5  Creating a single-case data plot",
    "section": "5.7 Themes",
    "text": "5.7 Themes\nThemes are complete styles that define various elements of a plot.\nFunction set_theme(\"theme_name\")\nPossible themes:\nbasic, grid, default, small, tiny, big, minimal, dark, sienna, phase_color, phase_shade, grid2, illustration\n\n5.7.1 An overview\n\n\n\n\n\nVarious scplot themes.\n\n\n\n\n\n\n5.7.2 Combine themes\nWhen providing multiple themes the order is important as the latter overwrites styles of the former.\n\nscplot(exampleABC) |&gt;\n  set_theme(\"sienna\", \"minimal\", \"small\", \"phase_color\")\n\n\n\n\n\n\n5.7.3 Set base text\nThe base text size is the absolute size. All other text sizes are relative to this base text size.\n\nscplot(exampleAB_decreasing$Peter) |&gt;\n  set_base_text(colour = \"blue\", family = \"serif\", face = \"italic\", size = 14)"
  },
  {
    "objectID": "ch_scplot.html#add-title-and-caption",
    "href": "ch_scplot.html#add-title-and-caption",
    "title": "5  Creating a single-case data plot",
    "section": "5.8 Add title and caption",
    "text": "5.8 Add title and caption\n\nscplot(exampleAB_decreasing) |&gt;\n  add_title(\"A new plot\", color = \"darkblue\", size = 1.3) |&gt;\n  add_caption(\"Note. What a nice plot!\", face = \"italic\", color = \"darkred\")"
  },
  {
    "objectID": "ch_scplot.html#add-a-legend",
    "href": "ch_scplot.html#add-a-legend",
    "title": "5  Creating a single-case data plot",
    "section": "5.9 Add a legend",
    "text": "5.9 Add a legend\n\nscplot(exampleABC) |&gt;\n  add_statline(\"mean\", color = \"darkred\") |&gt;\n  add_statline(\"min\", phase = \"B\", linewidth = 0.2, color = \"darkblue\") |&gt;\n  add_legend()\n\n\n\n\nand set specific elements\n\nscplot(exampleABC) |&gt;\n  add_statline(\"mean\", color = \"darkred\") |&gt;\n  add_legend(\n    position = \"left\", \n    title = list(size = 12, face = \"italic\"),\n    background = list(fill = \"grey95\", colour = \"black\")\n  )"
  },
  {
    "objectID": "ch_scplot.html#customize-axis-settings",
    "href": "ch_scplot.html#customize-axis-settings",
    "title": "5  Creating a single-case data plot",
    "section": "5.10 Customize axis settings",
    "text": "5.10 Customize axis settings\nWhen axis ticks are to close together set the increment argument to leave additional space (e.g. increment = 2 will annotate every other value). When you set increment_from = 0 an additional tick will be set at 1 although counting of the increments will start at 0.\n\nscplot(exampleA1B1A2B2) |&gt; \n  set_xaxis(increment_from = 0, increment = 5, \n            color = \"darkred\", size = 0.7, angle = -90) |&gt;\n  set_yaxis(limits = c(0, 50), size = 0.7, color = \"darkred\")"
  },
  {
    "objectID": "ch_scplot.html#customize-axis-labels",
    "href": "ch_scplot.html#customize-axis-labels",
    "title": "5  Creating a single-case data plot",
    "section": "5.11 Customize axis labels",
    "text": "5.11 Customize axis labels\n\nscplot(exampleA1B1A2B2) |&gt; \n  set_ylabel(\"Score\", color = \"darkred\", angle = 0) |&gt;\n  set_xlabel(\"Session\", color = \"darkred\")"
  },
  {
    "objectID": "ch_scplot.html#change-casenames",
    "href": "ch_scplot.html#change-casenames",
    "title": "5  Creating a single-case data plot",
    "section": "5.12 Change Casenames",
    "text": "5.12 Change Casenames\n\nscplot(exampleA1B1A2B2) |&gt;\n  set_casenames(c(\"A\", \"B\", \"C\"), color = \"darkblue\", size = 1)\n\n\n\n\nCasenames as strips:\n\nscplot(exampleA1B1A2B2) |&gt;\n  set_casenames(position = \"strip\", \n                background = list(fill = \"lightblue\"))"
  },
  {
    "objectID": "ch_scplot.html#add-value-labels",
    "href": "ch_scplot.html#add-value-labels",
    "title": "5  Creating a single-case data plot",
    "section": "5.13 Add value labels",
    "text": "5.13 Add value labels\n\nscplot(exampleABC) |&gt; \n  add_labels(text = list(color = \"black\", size = 0.7), \n             background = list(fill = \"grey98\"), nudge_y = 7)\n\nWarning: Removed 1 rows containing missing values (`geom_label()`).\n\n\n\n\n\nIf you set the nudge_y argument to 0, the label will be set on-top the datapoints:\n\nscplot(exampleABC) |&gt; \n  add_labels(text = list(color = \"black\", size = 0.7), \n             background = list(fill = \"grey98\"), nudge_y = 0)"
  },
  {
    "objectID": "ch_scplot.html#add-a-ridge",
    "href": "ch_scplot.html#add-a-ridge",
    "title": "5  Creating a single-case data plot",
    "section": "5.14 Add a ridge",
    "text": "5.14 Add a ridge\n\nscplot(exampleAB_mpd) |&gt; \n  add_ridge(\"grey50\")"
  },
  {
    "objectID": "ch_scplot.html#extending-scplot-with-ggplot2",
    "href": "ch_scplot.html#extending-scplot-with-ggplot2",
    "title": "5  Creating a single-case data plot",
    "section": "5.15 Extending scplot with ggplot2",
    "text": "5.15 Extending scplot with ggplot2\nscplot() generates ggplot2 objects. You can keep the ggplot2 object and assign it into a new object with the as_ggplot() function. Thereby, you can use many ggplot2 functions to rework your graphics:\n\np1 &lt;- scplot(byHeart2011$`Lisa (Turkish)`) |&gt; \n        set_theme(\"minimal\") |&gt;\n        as_ggplot()\np2 &lt;- scplot(byHeart2011$`Patrick (Spanish)`) |&gt; \n        set_theme(\"minimal\") |&gt; \n        as_ggplot()\np3 &lt;- scplot(byHeart2011$`Anna (Twi)`) |&gt; \n        set_theme(\"minimal\") |&gt; \n        as_ggplot()\np4 &lt;- scplot(byHeart2011$`Melanie (Swedish)`) |&gt; \n        set_theme(\"minimal\") |&gt; \n        as_ggplot()\n\nlibrary(patchwork)\np1 + p2 + p3 + p4 + plot_annotation(tag_levels = \"a\", tag_suffix =  \")\")"
  },
  {
    "objectID": "ch_scplot.html#complexs-examples",
    "href": "ch_scplot.html#complexs-examples",
    "title": "5  Creating a single-case data plot",
    "section": "5.16 Complexs examples",
    "text": "5.16 Complexs examples\nHere are some more complex examples\n\nscplot(example_A24) |&gt; \n  add_statline(\"lowess\", linewidth = 1.5) |&gt;\n  add_statline(\"loess\", linewidth = 1.5) |&gt;\n  add_statline(\"moving mean\", lag = 3, linewidth = 1.5) |&gt;\n  set_xaxis(size = 0.8, angle = 35) |&gt;\n  set_dataline(point = \"none\") |&gt;\n  add_legend(position = c(0.8, 0.75), background = list(color = \"grey50\")) |&gt;\n  set_phasenames(c(\"no speedlimit\", \"with speedlimit\"), \n                 position = \"left\", hjust = 0, vjust = 1) |&gt;\n  set_casenames(position = \"none\") |&gt;\n  add_title(\"Effect of a speedlimit on the A24\") |&gt;\n  add_caption(\"Note: Moving mean calculated with lag three\", face = 3, size = 1) |&gt;\n  add_ridge(color = \"lightblue\")\n\n\n\n\n\nscplot(exampleAB_add) |&gt;\n  set_dataline(\"cigarrets\", point = list(size = 1)) |&gt;\n  add_statline(\"trend\", linetype = \"dashed\") |&gt;\n  add_statline(\"mean\", variable = \"cigarrets\", color = \"darkred\") |&gt;\n  add_marks(positions = c(14,20), size = 3, variable = \"cigarrets\")|&gt;\n  add_marks(positions = \"cigarrets &gt; quantile(cigarrets, 0.75)\", size = 3) |&gt;\n  set_xaxis(increment = 5) |&gt;\n  set_phasenames(color = NA) |&gt;\n  set_casenames(position = \"strip\") |&gt;\n  add_legend(\n    section_labels = c(\"\", \"\"),\n    labels = c(NA, NA, \"Trend of wellbeing\", \"Mean of cigarrets\"),\n    text = list(face = 3)\n  ) |&gt;\n  set_panel(fill = c(\"lightblue\", \"grey80\")) |&gt;\n  add_ridge(color = \"snow\", variable = \"cigarrets\") |&gt;\n  add_labels(variable = \"cigarrets\", nudge_y = 2, \n             text = list(color = \"blue\", size = 0.5)) |&gt;\n  add_labels(nudge_y = 2, text = list(color = \"black\", size = 0.5),\n             background = list(fill = \"white\"))\n\nWarning: Removed 1 rows containing missing values (`geom_label()`).\n\n\n\n\n\n\nscplot(exampleA1B1A2B2) |&gt; \n  set_xaxis(increment = 4, size = 0.7) |&gt;\n  set_yaxis(color = \"sienna3\") |&gt;\n  set_ylabel(\"Points\", color = \"sienna3\", angle = 0) |&gt;\n  set_xlabel(\"Weeks\", size = 1, color = \"brown\") |&gt;\n  add_title(\"Points by week\", color = \"sienna4\", face = 3) |&gt;\n  add_caption(\"Note: An extensive Example.\",\n              color = \"black\", size = 1, face = 3) |&gt;\n  set_phasenames(c(\"Baseline\", \"Intervention\", \"Fall-Back\", \"Intervention_2\"), \n                 size = 0) |&gt;\n  add_ridge(scales::alpha(\"lightblue\", 0.5)) |&gt;\n  set_casenames(labels = sample_names(3), color = \"steelblue4\", size = 0.7) |&gt;\n  set_panel(fill = c(\"grey80\", \"grey95\"), color = \"sienna4\") |&gt;\n  add_grid(color = \"grey85\", linewidth = 0.1) |&gt;\n  set_dataline(size = 0.5, linetype = \"solid\", \n               point = list(colour = \"sienna4\", size = 0.5, shape = 18)) |&gt;\n  add_labels(text = list(color = \"sienna\", size = 0.7), nudge_y = 4) |&gt;\n  set_separator(size = 0.5, linetype = \"solid\", color = \"sienna\") |&gt;\n  add_statline(stat = \"trendA\", color = \"tomato2\") |&gt;\n  add_statline(stat = \"max\", phase = c(1, 3), linetype = \"dashed\") |&gt;\n  add_marks(case = 1:2, positions = 14, color = \"red3\", size = 2, shape = 4) |&gt;\n  add_marks(case = \"all\", positions = \"values &lt; quantile(values, 0.1)\", \n            color = \"blue3\", size = 1.5) |&gt;\n  add_marks(positions = outlier(exampleABAB), color = \"brown\", size = 2) |&gt;\n  add_text(case = 1, x = 5, y = 35, label = \"Interesting\", \n           color = \"darkgreen\", angle = 20, size = 0.7) |&gt;\n  add_arrow(case = 1, 5, 30, 5, 22, color = \"steelblue\") |&gt;\n  set_background(fill = \"white\") |&gt;\n  add_legend() |&gt;\n  set_theme(\"basic\") |&gt;\n  set_theme_element(panel.spacing.y = unit(0, \"points\"))\n\nWarning: Removed 6 rows containing missing values (`geom_text()`).\n\n\n\n\n\nAdding bars is a bit more complicated:\n\nSet the type argument to \"bar\"\n\nExtend the limits of the x-axis by 1 (here from 0 to 41)\n\nSet the left margin of the x-axis to 0 with the expand argument.\n\n\nscplot(exampleAB_add) |&gt;\n  set_xaxis(expand = c(0, 0), limits = c(0, 41)) |&gt;\n  set_dataline(\"cigarrets\", type = \"bar\", linewidth = 0.6, point = \"none\") |&gt;\n  add_statline(\"mean\", variable = \"cigarrets\", color = \"darkred\") |&gt;\n  add_statline(\"trend\", linetype = \"dashed\") |&gt;\n  set_casenames(position = \"strip\")"
  },
  {
    "objectID": "ch_missing_values_outliers.html#missing-values",
    "href": "ch_missing_values_outliers.html#missing-values",
    "title": "6  Missing values and outliers",
    "section": "6.1 Missing values",
    "text": "6.1 Missing values\nThere are two kinds of missing values in single-case data series. First, missings that were explicitly recorded as NA and assigned to a phase and measurement-time as in the following example:\nscdf(c(5, 3, 4, 6, 8, 7, 9, 7, NA, 6), phase_design = c(A = 4, B = 6))\nThe second type of missing occurs when there are gaps between measurement-times that are not explicitly coded as in the following example:\nscdf(c(5, 3, 4, 6, 8, 7, 9, 7, 6), phase_design = c(A = 4, B = 5), \n     mt = c(1, 2, 3, 4, 5, 6, 7, 8, 10))\nIn both cases, missing values pose a threat to the internal validity of overlap indices. Randomization tests are more robust against the first type of missing values but are affected by the second type. Regression approaches are less impacted by both types as they take the interval between measurement-times into account.\n\ncase1 &lt;- scdf(c(3,6,2,4,3,5,2,6,3,2, 6,7,5,8,6,7,4,8,5,6), \n              phase_design = c(A = 10, B = 10), name = \"no NA\")\ncase2 &lt;- scdf(c(3,6,2,4,3,5,2,NA,3,2, 6,7,5,8,6,NA,4,8,5,6), \n              phase_design = c(A = 10, B = 10), name = \"NAs\")\ncase3 &lt;- fill_missing(case2)\nnames(case3) &lt;- \"interpolated NAs\"\nex &lt;- c(case1, case2, case3)\nscplot(ex)\n\nWarning: Removed 2 rows containing missing values (`geom_point()`).\n\n\n\n\noverlap(ex)\n\nOverlap Indices\n\nComparing phase 1 against phase 2 \n\n             no NA  NAs interpolated NAs\nDesign         A-B  A-B              A-B\nPND             40   33               30\nPEM            100  100              100\nPET            100  100              100\nNAP             88   91               92\nNAP rescaled    77   83               83\nPAND            80   89               90\nIRD           0.60 0.67             0.70\nTau_U(A)      0.53 0.61             0.61\nTau_U(BA)     0.45 0.51             0.50\nBase_Tau      0.59 0.64             0.64\nDiff_mean     2.60 2.78             2.75\nDiff_trend    0.02 0.11             0.12\nSMD           1.65 1.96             2.02\nHedges_g      1.71 1.90             1.96"
  },
  {
    "objectID": "ch_missing_values_outliers.html#outlieranalysis",
    "href": "ch_missing_values_outliers.html#outlieranalysis",
    "title": "6  Missing values and outliers",
    "section": "6.2 Outlieranalysis",
    "text": "6.2 Outlieranalysis\n\n\n\n\n\n\nThe outlier function call\n\n\n\noutlier(data, dvar, pvar, mvar, method = c(“MAD”, “Cook”, “SD”, “CI”), criteria = 3.5)\n\n\nscan provides several methods for analyzing outliers. All of them are implemented in the outliers function. Available methods are the standard deviation, mean average deviation, confidence intervals, and Cook’s distance. The criteria argument takes a vector with two information, the first defines the analyzing method (“SD”, “MAD”, CI”, “Cook”) and the second the criteria. For “SD” the criteria is the number of standard deviations (sd) from the mean of each phase for which a value is not considered to be an outlier. For example, criteria = c(\"SD\",2) would identify every value exceeding two sd above or below the mean as an outlier whereas sd and mean refer to phase of a value. As this might be misleading particularly for small samples Iglewicz and Hoaglin Iglewicz & Hoaglin (1993) recommend the use the much more robust median average deviation (MAD) instead. The MAD is is constructed similar to the sd but uses the median instead of the mean. Multiplying the MAD by 1.4826 approximates the sd in a normal distributed sample. This corrected MAD is applied in the outlier function. A deviation of 3.5 times the corrected MAD from the median is suggested to be an outlier. To use this criterion set criteria = c(\"MAD\", 3.5). criteria = c(\"CI\", 0.95) takes exceeding the 95% confidence interval as the criteria for outliers. The Cook’s distance method for calculation outliers can be applied with a strict AB-phase design. in that case, the Cook’s distance analyses are based on a piecewise-regression model. Most commonly, Cook’s distance exceeding 4/n is used as a criteria. This could be implemented setting criteria = c(\"Cook\",\"4/n\").\n\noutlier(exampleABC_outlier, criteria = c(\"MAD\", 3.5))\n\nOutlier Analysis for Single-Case Data\n\nCase Bernadette : Dropped 3 \nCase Penny : Dropped 2 \nCase Amy : Dropped 3 \n\n# Visualizing outliers with the plot function\nres &lt;- outlier(exampleABC_outlier, criteria = c(\"MAD\", 3.5))\n\nscplot(exampleABC_outlier) %&gt;%\n  add_labels(nudge_y = 15, text = list(size = 0.7)) %&gt;%\n  add_marks(positions = res) %&gt;%\n  add_theme(\"basic\") %&gt;%\n  set_casenames(position = \"strip\") %&gt;%\n  set_yaxis(limits = c(20, 120))\n\nWarning in add_theme(., \"basic\"): Deprecated. Please use `set_theme()`\n\n\nWarning: Removed 9 rows containing missing values (`geom_text()`).\n\n\n\n\n\n\n\n\n\nIglewicz, B., & Hoaglin, D. C. (1993). How to detect and handle outliers. Milwaukee, Wis. : ASQC Quality Press."
  },
  {
    "objectID": "ch_describe.html#descriptive-statistics",
    "href": "ch_describe.html#descriptive-statistics",
    "title": "7  Describe single-case data frames",
    "section": "7.1 Descriptive statistics",
    "text": "7.1 Descriptive statistics\n\n\n\n\n\n\nThe describe function call\n\n\n\ndescribe(data, dvar, pvar, mvar)\n\n\ndescribe is the basic command to get an overview on descriptive statistics. As an argument it only takes the name of an scdf object. For each case of the scdf and each phase within a case descriptive statistics are provided. The output table contains statistical indicators followed by a dot and the name of the phase (e.g., n.A for the number of measurements of phase A).\n\n\n\nStatistics of the describe command \n\n\nParameter\nWhat it means ...\n\n\n\n\nn\nNumber of measurements.\n\n\nmis\nNumber of missing values.\n\n\nm\nMean values.\n\n\nmd\nMedian of values.\n\n\nsd\nStandard deviation of values.\n\n\nmad\nMedian average deviation of values.\n\n\nmin/max\nMin and max of values.\n\n\ntrend\nSlope of a regression line through values by time.\n\n\n\n\n\n\n\n\ndescribe(exampleABC)\n\nDescribe Single-Case Data\n\n       Marie Rosalind  Lise\nDesign A-B-C    A-B-C A-B-C\nn.A       10       15    20\nn.B       10        8     7\nn.C       10        7     3\nmis.A      0        0     0\nmis.B      0        0     0\nmis.C      0        0     0\n\n          Marie Rosalind    Lise\nm.A      52.000   52.267  52.350\nm.B      72.100   73.250  73.571\nm.C      68.000   66.429  71.333\nmd.A       53.5     52.0    52.0\nmd.B       72.5     72.0    73.0\nmd.C         69       68      76\nsd.A      8.287    8.146  10.869\nsd.B     11.367   13.134  10.644\nsd.C     12.702   10.486  21.385\nmad.A    11.119    7.413  10.378\nmad.B    10.378   10.378  16.309\nmad.C    17.791   11.861  20.756\nmin.A        39       37      35\nmin.B        47       54      60\nmin.C        51       52      48\nmax.A        63       65      74\nmax.B        85       97      87\nmax.C        87       78      90\ntrend.A  -1.915    0.500  -0.088\ntrend.B  -0.612    0.643   1.929\ntrend.C  -0.194   -2.929 -14.000\n\n\nThe resulting table could be exported into a csv file to be used in other software (e.g., to inserted in a word processing document). Therefore, first write the results of the describe command into an R object and then use the write.csv to export the descriptives element of the object.\n\n# write the results into a new R object named `res`\nres &lt;- describe(exampleABC)\n# create a new file containing the descriptives on your harddrive\nwrite.csv(res$descriptives, file = \"descriptive data.csv\")\n\nThe file is written to the currently active working directory. If you are not sure where that is, type getwd() (you can use the setwd() command to define a different working directory. To get further details type help(setwd) into R).\n\n\n\n\n\n\nConflicting function names\n\n\n\nSometimes R packages include the same function names. For example, the describe() function is also part of the psych package. Now, if you have loaded the psych package with library(psych) after scan the describe() function of scan will be masked (describe() would now call the corresponding function of the psych package).\nThere are two solutions to this problem:\n\nactivate the psych library before the scan library (now the psych describe() function will be masked) or\ninclude the package name into the function call with the prefix scan::: scan::describe()."
  },
  {
    "objectID": "ch_describe.html#standardized-mean-differences",
    "href": "ch_describe.html#standardized-mean-differences",
    "title": "7  Describe single-case data frames",
    "section": "7.2 Standardized mean differences",
    "text": "7.2 Standardized mean differences\n```{r}\n#| results: asis\nfunction_structure(\"smd\")\n```\n\n\n\n\n\n\nThe smd function call\n\n\n\nsmd(data, dvar, pvar, mvar, decreasing = FALSE, phases = c(1, 2))\n\n\nStandardized mean differences are measures of effect sizes. The smd functions calculated various variations.\n\nsmd(exampleAB)\n\nStandardized mean differences\n\n                            Johanna Karolina  Anja\nmA                            54.60    51.80 53.60\nmB                            74.13    73.47 74.07\nsdA                            2.41     6.83  3.05\nsdB                            8.94     9.76  7.57\nsd cohen                       6.55     8.43  5.77\nsd hedges                      7.97     9.19  6.83\nGlass' delta                   8.11     3.17  6.71\nHedges' g                      2.45     2.36  3.00\nHedges' g correction           2.35     2.26  2.87\nHedges' g durlak correction    2.23     2.14  2.72\nCohen's d                      2.98     2.57  3.54"
  },
  {
    "objectID": "ch_describe.html#auto-regression",
    "href": "ch_describe.html#auto-regression",
    "title": "7  Describe single-case data frames",
    "section": "7.3 Auto-regression",
    "text": "7.3 Auto-regression\n```{r}\n#| results: asis\nfunction_structure(\"autocorr\")\n```\n\n\n\n\n\n\nThe autocorr function call\n\n\n\nautocorr(data, dvar, pvar, mvar, lag_max = 3, …)\n\n\nThe autocorr function calculates autocorrelations within each phase and across all phases. The lag_max argument defines the lag up to which the autocorrelation will be computed.\n\nautocorr(exampleABC, lag_max = 4)\n\nAutocorrelations\n\nMarie \n Phase Lag 1 Lag 2 Lag 3 Lag 4\n     A  0.29 -0.11  0.10  0.12\n     B -0.28 -0.10 -0.14 -0.09\n     C  0.00 -0.33 -0.14 -0.25\n   all  0.21  0.10  0.25  0.12\n\nRosalind \n Phase Lag 1 Lag 2 Lag 3 Lag 4\n     A  0.37 -0.29 -0.33 -0.34\n     B -0.34  0.24 -0.40  0.04\n     C -0.07 -0.32  0.27  0.02\n   all  0.49  0.38  0.22  0.17\n\nLise \n Phase Lag 1 Lag 2 Lag 3 Lag 4\n     A  0.04 -0.32 -0.05 -0.09\n     B -0.63  0.50 -0.40  0.31\n     C -0.38 -0.12    NA    NA\n   all  0.33  0.36  0.23  0.27"
  },
  {
    "objectID": "ch_describe.html#trend-analysis",
    "href": "ch_describe.html#trend-analysis",
    "title": "7  Describe single-case data frames",
    "section": "7.4 Trend analysis",
    "text": "7.4 Trend analysis\n```{r}\n#| results: asis\nfunction_structure(\"trend\")\n```\n\n\n\n\n\n\nThe trend function call\n\n\n\ntrend(data, dvar, pvar, mvar, offset = “deprecated”, first_mt = 0, model = NULL)\n\n\nThe trend function provides an overview of linear trends in single-case data. By default, it gives you the intercept and slope of a linear and a squared regression of measurement-time on scores. Models are computed separately for each phase and across all phases. For a more advanced application, you can add regression models using the R specific formula class.\n\n# Simple example\ntrend(exampleABC$Marie)\n\nTrend for each phase\n\n              Intercept      B   Beta\nLinear.ALL       55.159  0.612  0.392\nLinear.A         60.618 -1.915 -0.700\nLinear.B         74.855 -0.612 -0.163\nLinear.C         68.873 -0.194 -0.046\nQuadratic.ALL    59.135  0.017  0.330\nQuadratic.A      57.937 -0.208 -0.712\nQuadratic.B      73.217 -0.039 -0.098\nQuadratic.C      68.490 -0.017 -0.038\n\nNote. Measurement-times start at 0  for each phase\n\n# Complex example\ntrend(\n  exampleAB$Johanna, \n  model = c(\"Cubic\" = values ~ I(mt^3), \n            \"Log Time\" = values ~ log(mt+1))\n)\n\nTrend for each phase\n\n              Intercept      B   Beta\nLinear.ALL       52.271  1.787  0.908\nLinear.A         54.400  0.100  0.066\nLinear.B         62.758  1.625  0.813\nQuadratic.ALL    58.582  0.086  0.864\nQuadratic.A      54.841 -0.040 -0.110\nQuadratic.B      66.985  0.106  0.767\nCubic.ALL        61.354  0.004  0.806\nCubic.A          55.050 -0.022 -0.251\nCubic.B          68.784  0.007  0.721\nLog Time.ALL     43.532 12.149  0.848\nLog Time.A       54.032  0.593  0.156\nLog Time.B       57.300  9.051  0.791\n\nNote. Measurement-times start at 0  for each phase"
  },
  {
    "objectID": "ch_overlapping_indices.html#overlap-overview",
    "href": "ch_overlapping_indices.html#overlap-overview",
    "title": "8  Overlapping indices",
    "section": "8.1 Overlap overview",
    "text": "8.1 Overlap overview\n\n\n\n\n\n\nThe overlap function call\n\n\n\noverlap(data, dvar, pvar, mvar, decreasing = FALSE, phases = c(1, 2))\n\n\noverlap provides a table with some of the most important overlap indices for each case of an scdf. For calculating overlap indicators it is important to know if a decrease or an increase of values is expected between phases. By default overlap assumes an increase in values. If the argument decreasing = TRUE is set, calculation will be based on the assumption of decreasing values.\n\noverlap(exampleAB)\n\nOverlap Indices\n\nComparing phase 1 against phase 2 \n\n             Johanna Karolina  Anja\nDesign           A-B      A-B   A-B\nPND              100       87    93\nPEM              100      100   100\nPET              100       93   100\nNAP              100       97    98\nNAP rescaled     100       93    96\nPAND             100       90    90\nIRD             1.00     0.73  0.87\nTau_U(A)        0.59     0.55  0.62\nTau_U(BA)       0.77     0.78  0.64\nBase_Tau        0.63     0.59  0.61\nDiff_mean      19.53    21.67 20.47\nDiff_trend      1.53     0.54  2.50\nSMD             8.11     3.17  6.71\nHedges_g        2.35     2.26  2.87\n\n\nOverlap measures refer to a comparison of two phases within a single-case data-set. By default, overlap compares the first to the second phase.\n\n8.1.1 Select and recombine phases\n\n\n\n\n\n\nThe select_phases function call\n\n\n\nselect_phases(data, A, B, phase_names = “auto”)\n\n\nThe select_phases() function is needed if you like to compare specific phases or even like to combine several phases. select_phases() is designed to work within a pipe structure. So the first argument is an scdf and it returns an scdf.\n\nscdf |&gt; select_phases(A = 1, B = 3) |&gt; ...\n\nselect_phases() has the arguments A and B. Each argument takes a vector with the names or the numbers of the phases to be selected. If you want to compare the first to the third phase you can set select_phases(scdf, 1,3). If the phases of your case are named ‘A’, ‘B’, and ‘C’ you could alternatively set select_phases(scdf, \"A\",\"C\"). It is also possible to compare a combination of several cases against a combination of other phases. Each of the two list-elements could contain more than one phase which are concatenated with the c command. For example if you have an ABAB-Design and like to compare the two A-phases against the two B-phases select_phases(scdf, c(1,3), c(2,4) ) will do the trick.\n(As an alternative approach you can set the phases argument within the overlap() function. This argument takes a list with two elements where the first element defines the phases for the A-phase and the second argument the phases for the B-phase.)\n\nexampleA1B1A2B2 |&gt;\n  select_phases(c(\"A1\",\"A2\"), c(\"B1\",\"B2\")) |&gt;\n  overlap()\n\nOverlap Indices\n\nComparing phase 1 against phase 2 \n\n                 Pawel    Moritz    Jannis\nDesign       A1A2-B1B2 A1A2-B1B2 A1A2-B1B2\nPND                 55        78        71\nPEM                100       100       100\nPET                100       100       100\nNAP                 94        97        98\nNAP rescaled        89        94        97\nPAND                85        85        90\nIRD               0.75      0.80      0.89\nTau_U(A)          0.54      0.44      0.43\nTau_U(BA)         0.45      0.46      0.38\nBase_Tau          0.65      0.68      0.68\nDiff_mean        12.25     13.58     15.27\nDiff_trend       -0.05      0.00     -0.54\nSMD               2.68      3.27      3.62\nHedges_g          2.07      2.72      2.98\n\n# Alternatively:\n# overlap(exampleA1B1A2B2, phases = list( c(\"A1\",\"A2\"), c(\"B1\",\"B2\")))"
  },
  {
    "objectID": "ch_overlapping_indices.html#standardized-mean-differences",
    "href": "ch_overlapping_indices.html#standardized-mean-differences",
    "title": "8  Overlapping indices",
    "section": "8.2 Standardized mean differences",
    "text": "8.2 Standardized mean differences\n\n\n\n\n\n\nThe smd function call\n\n\n\nsmd(data, dvar, pvar, mvar, decreasing = FALSE, phases = c(1, 2))\n\n\nStandardized mean differences can be calculated in various ways. They refer to the difference in the means of two phases. The smd function provides an overview of the most common parameters for each single-case:\n\nsmd(exampleAB_score)\n\nStandardized mean differences\n\n                            Christiano Lionel Neymar\nmA                                2.70   3.10   2.30\nmB                               15.35  15.35  15.60\nsdA                               1.42   1.59   1.49\nsdB                               2.13   1.60   2.19\nsd cohen                          1.81   1.60   1.87\nsd hedges                         1.93   1.60   1.99\nGlass' delta                      8.92   7.68   8.90\nHedges' g                         6.54   7.67   6.68\nHedges' g correction              6.37   7.46   6.50\nHedges' g durlak correction       6.15   7.21   6.28\nCohen's d                         6.98   7.67   7.10"
  },
  {
    "objectID": "ch_overlapping_indices.html#percentage-non-overlapping-data-pnd",
    "href": "ch_overlapping_indices.html#percentage-non-overlapping-data-pnd",
    "title": "8  Overlapping indices",
    "section": "8.3 Percentage non-overlapping data (PND)",
    "text": "8.3 Percentage non-overlapping data (PND)\n\n\n\n\n\n\nThe pnd function call\n\n\n\npnd(data, dvar, pvar, decreasing = FALSE, phases = c(1, 2))\n\n\nThe percentage of non-overlapping data (PND) effect size measure was described by Scruggs, Mastropieri, & Casto (1987) . It is the percentage of all data-points of the second phase of a single-case study exceeding the maximum value of the first phase. In case you have a study where you expect a decrease of values in the second phase, PND is calculated as the percentage of data-point of the second phase below the minimum of the first phase.\n\n\n\n\n\nIllustration of PND. PND is 60% as 9 out of 15 datapoints of phase B are higher than the maximum of phase A\n\n\n\n\nThe function pnd provides the PND for each case as well as the mean of all PNDs of that scdf. When you expect decreasing values set decreasing = TRUE. When there are more than two phases or phases are not named A and B, use the phases argument as described at the beginning of this chapter.\n\npnd(exampleAB)\n\nPercent Non-Overlapping Data\n\n     Case    PND Total Exceeds\n  Johanna   100%    15      15\n Karolina 86.67%    15      13\n     Anja 93.33%    15      14\n\nMean  : 93.33 %"
  },
  {
    "objectID": "ch_overlapping_indices.html#percentage-exceeding-the-median-pem",
    "href": "ch_overlapping_indices.html#percentage-exceeding-the-median-pem",
    "title": "8  Overlapping indices",
    "section": "8.4 Percentage exceeding the median (PEM)",
    "text": "8.4 Percentage exceeding the median (PEM)\n\n\n\n\n\n\nThe pem function call\n\n\n\npem(data, dvar, pvar, decreasing = FALSE, binom.test = TRUE, chi.test = FALSE, FUN = median, phases = c(1, 2), …)\n\n\nThe pem function returns the percentage of phase B data exceeding the phase A median. Additionally, a binomial test against a 50/50 distribution is computed. Different measures of central tendency can be addressed for alternative analyses.\n\n\n\n\n\nIllustration of PEM. PEM is 75% as 13 out of 15 datapoints of phase B are higher than the median of phase A\n\n\n\n\n\npem(exampleAB)\n\nPercent Exceeding the Median\n\n     Case PEM positives total  binom.p\n  Johanna 100        15    15 3.05e-05\n Karolina 100        15    15 3.05e-05\n     Anja 100        15    15 3.05e-05\n\nAlternative hypothesis: true probability &gt; 50%"
  },
  {
    "objectID": "ch_overlapping_indices.html#percentage-exceeding-the-regression-trend-pet",
    "href": "ch_overlapping_indices.html#percentage-exceeding-the-regression-trend-pet",
    "title": "8  Overlapping indices",
    "section": "8.5 Percentage exceeding the regression trend (PET)",
    "text": "8.5 Percentage exceeding the regression trend (PET)\n\n\n\n\n\n\nThe pet function call\n\n\n\npet(data, dvar, pvar, mvar, ci = 0.95, decreasing = FALSE, phases = c(1, 2))\n\n\nThe pet function provides the percentage of phase B data points exceeding the prediction based on the phase A trend. A binomial test against a 50/50 distribution is computed. Furthermore, the percentage of phase B data points exceeding the upper (or lower) 95 percent confidence interval of the predicted progress is computed.\n\npet(exampleAB)\n\nPercent Exceeding the Trend\n\n     Case   PET PET CI  binom.p\n  Johanna 100.0   86.7 3.05e-05\n Karolina  93.3    0.0 4.88e-04\n     Anja 100.0  100.0 3.05e-05\n\nBinom.test: alternative hypothesis: true probability &gt; 50%\nPET CI: Percent of values greater than upper 95% confidence threshold (greater 1.645*se above predicted value)\n\n\n\n\n\n\n\nIllustration of PET. PET is 66.7% as 10 out of 15 datapoints of phase B are higher than the projected trend-line of phase A"
  },
  {
    "objectID": "ch_overlapping_indices.html#percentage-of-all-non-overlapping-data-pand",
    "href": "ch_overlapping_indices.html#percentage-of-all-non-overlapping-data-pand",
    "title": "8  Overlapping indices",
    "section": "8.6 Percentage of all non-overlapping data (PAND)",
    "text": "8.6 Percentage of all non-overlapping data (PAND)\n\n\n\n\n\n\nThe pand function call\n\n\n\npand(data, dvar, pvar, decreasing = FALSE, phases = c(1, 2), method = c(“sort”, “minimum”))\n\n\nThe pand function calculates the percentage of all non-overlapping data (Richard I. Parker, Hagan-Burke, & Vannest, 2007), an index to quantify a level increase (or decrease) in performance after the onset of an intervention. The authors emphasize that PAND is designed for application in a multiple case design with a substantial number of measurements, technically at least 20 to 25, but preferably 60 or more. PAND is defined as 100% minus the percentage of data points that need to be removed from either phase in order to ensure nonoverlap between the phases.\nSeveral approaches have been suggested to calculate PAND, leading to potentially different outcomes. In their 2007 paper, Parker and colleagues present an algorithm for computing PAND. The algorithm involves sorting the scores of a time series, including the associated phases, and comparing the resulting phase order with the original phase order using a contingency table. To account for ties, the algorithm includes a randomization process where ties are randomly assigned to one of the two phases. Consequently, executing the algorithm multiple times could yield different results. It is important to note that this algorithm does not produce the same results as the PAND definition provided earlier in the same paper. However, it offers the advantage of allowing the calculation of an effect size measure phi, and the application of statistical tests for frequency distributions. phi equals Pearsons r for dichotomous data. Thus, phi-Square is the amount of explained variance.\nPustejovsky (2019) presented a mathematical formulation of Parker’s original definition for comparing two phases of a single case:\n\\[PAND = \\frac{1}{m+n}max\\{(i+j)I(y^A_{i}&lt;y^B_{n+1-j}\\}\\]\nThis formulation provides accurate results for PAND, but the original definition has the drawback of an unknown distribution under the null hypothesis, making a statistical test difficult.\nThe pand() function enables the calculation of PAND using both methods. The first approach (method = \"sort\") follows the algorithm described above, with the exclusion of randomization before sorting to avoid ambiguity. It calculates a phi measure and provides the results of a chi-squared test and a Fisher exact test. The second approach (method = \"minimum\") applies the aforementioned formula. For a multiple case design, overlaps are calculated for each case, summed, and then divided by the total number of measurements. No statistical test is conducted for this method.\n\npand(Parker2007)\n\nPercentage of all non-overlapping data\n\nMethod: sort \n\nPAND = 85.7%\nΦ =  0.713  ; Φ² =  0.508 \n\n28 measurements (13 Phase A, 15 Phase B) in 3 cases\nOverlapping data: n = 4 ; percentage = 14.3 \n\n2 x 2 Matrix of percentages\n         A    B total\nA     39.3  7.1  46.4\nB      7.1 46.4  53.6\ntotal 46.4 53.6 100.0\n\n2 x 2 Matrix of counts\n       A  B total\nA     11  2    13\nB      2 13    15\ntotal 13 15    28\n\n\nChi-Squared test:\nX² = 14.227, df = 1, p = 0.000 \n\nFisher exact test:\nOdds ratio = 29.007, p = 0.000 \n\n\n\npand(Parker2007, method = \"minimum\")\n\nPercentage of all non-overlapping data\n\nMethod: minimum \n\nPAND = 85.7%\n28 measurements (13 Phase A, 15 Phase B) in 3 cases\nOverlapping data: n = 4 ; percentage = 14.3 \n\n\nThe original procedure for computing PAND does not account for ambivalent datapoints (ties). The newer NAP overcomes this problem and has better precision-power (Richard I. Parker, Vannest, & Davis, 2011a)."
  },
  {
    "objectID": "ch_overlapping_indices.html#nonoverlap-of-all-pairs-nap",
    "href": "ch_overlapping_indices.html#nonoverlap-of-all-pairs-nap",
    "title": "8  Overlapping indices",
    "section": "8.7 Nonoverlap of all pairs (NAP)",
    "text": "8.7 Nonoverlap of all pairs (NAP)\n\n\n\n\n\n\nThe nap function call\n\n\n\nnap(data, dvar, pvar, decreasing = FALSE, phases = c(1, 2))\n\n\nThe nap function calculates the nonoverlap of all pairs (Richard I. Parker & Vannest, 2009). NAP summarizes the overlap between all pairs of phase A and phase B data points. If an increase of phase B scores is expected, a non-overlapping pair has a higher phase B data point. The NAP equals number of pairs showing no overlap / number of pairs. Because NAP has values between 0 and 100% where 50% is no effect, a rescaled NAP (ranging between -100 and 100% where 0% is no effect) has been proposed. NAP is equivalent to the the U-test and Wilcox rank sum test. Thus, a Wilcox test is conducted and reported for each case. Additionally, effect sizes d and R squared are reported following Parker and colleagues.\n\nnap(exampleAB)\n\nNonoverlap of All Pairs\n\n     Case NAP NAP Rescaled   w     p   d   R²\n  Johanna 100          100 0.0 &lt;.001 3.5 0.75\n Karolina  97           93 2.5  &lt;.01 2.6 0.62\n     Anja  98           96 1.5 &lt;.001 2.8 0.66"
  },
  {
    "objectID": "ch_overlapping_indices.html#tau-u",
    "href": "ch_overlapping_indices.html#tau-u",
    "title": "8  Overlapping indices",
    "section": "8.8 Tau-U",
    "text": "8.8 Tau-U\n\n\n\n\n\n\nThe tau_u function call\n\n\n\ntau_u(data, dvar, pvar, tau_method = c(“b”, “a”), method = c(“complete”, “parker”), phases = c(1, 2), meta_analyses = TRUE, ci = 0.95, ci_method = c(“z”, “tau”, “s”), meta_weight_method = c(“z”, “tau”), continuity_correction = FALSE, meta_method = NULL)\n\n\nThe Tau-U statistic has been proposed by Richard I. Parker, Vannest, Davis, & Sauber (2011b) and is one of the more broadly used approach for reporting effect sizes of single case data. Unfortunately, various and ambiguous implementations of Tau-U exist (Brossart, Laird, & Armstrong, 2018; Pustejovsky, 2016). The tau_u function tries to cover several of these implementation. It takes an scdf and returns Tau-U calculations for each single-case within that file. Additionally, an overall Tau-U value is calculated for all cases based on a meta-analysis.\n\n8.8.1 Variations of Tau-U\nSeveral arguments an be set to define how Tau-U should be calculated. By setting the argument method = \"parker\", Tau-U is calculated as described in Richard I. Parker et al. (2011b). This procedure could lead to Tau-U values above 1 and below -1 which are difficult to interpret. method = \"complete, which is the default, applies a correction that keeps the values within the -1 to 1 range and should be more appropriate. In the original method proposed by Richard I. Parker et al. (2011b) data, calculations are based on Kendall’s Tau A which does not correct for ties. Alternatively, Kendall’s Tau B has a correction for Tau in the presence of ties. The tau_method can be set to decide on the tau method to use \"a\" for Kendall’s Tau A and \"b\"` for Kendall’s Tau B.\nHere is an example with setting that reconstruct the values from the original example in Richard I. Parker, Vannest, Davis, & Sauber (2011c) :\n\ntau_u(Parker2011, method = \"parker\", tau_method = \"a\", continuity_correction = FALSE)\n\nTau-U\nMethod: parker \nApplied Kendall's Tau-a\n95% CIs for tau are reported.\nCI method: z\n\nCase: Case1 \n                             Tau CI lower CI upper SD_S    Z    p\nA vs. B                     0.80     0.29     0.96 8.16 1.96  .05\nA vs. B - Trend A           0.65    -0.02     0.92 9.59 1.36  .18\nA vs. B + Trend B           0.77     0.21     0.95 9.59 2.40 &lt;.05\nA vs. B + Trend B - Trend A 0.56    -0.17     0.89 9.59 2.09 &lt;.05\n\n\nA different implementation of the method (provided at http://www.singlecaseresearch.org/calculators/tau-u)) uses Kendall’s Tau B:\n\ntau_u(exampleAB$Johanna, method = \"parker\", tau_method = \"b\", continuity_correction = FALSE)\n\nTau-U\nMethod: parker \nApplied Kendall's Tau-b\n95% CIs for tau are reported.\nCI method: z\n\nCase: Johanna \n                             Tau CI lower CI upper  SD_S    Z     p\nA vs. B                     1.00      NaN      NaN 22.91 3.27 &lt;.001\nA vs. B - Trend A           0.59     0.20     0.82 23.26 3.22 &lt;.001\nA vs. B + Trend B           0.79     0.53     0.91 30.53 4.75 &lt;.001\nA vs. B + Trend B - Trend A 0.77     0.49     0.90 30.81 4.71 &lt;.001\n\n\nAnother online calculator created by Rumen Manolov is available at https://manolov.shinyapps.io/Overlap/. It uses an R code developed by Kevin Tarlow to calculate Tau-U. This setting will replicate the results of this approach:\n\ntau_u(exampleAB$Johanna, method = \"complete\", tau_method = \"a\", continuity_correction = FALSE)\n\nTau-U\nMethod: complete \nApplied Kendall's Tau-a\n95% CIs for tau are reported.\nCI method: z\n\nCase: Johanna \n                             Tau CI lower CI upper  SD_S    Z     p\nA vs. B                     1.00      NaN      NaN 22.91 3.27 &lt;.001\nA vs. B - Trend A           0.88     0.72     0.95 30.82 2.43  &lt;.05\nA vs. B + Trend B           0.81     0.56     0.92 30.82 4.70 &lt;.001\nA vs. B + Trend B - Trend A 0.76     0.48     0.90 30.82 4.70 &lt;.001\n\n\nThe standard return of the tau_u function does not display all calculations. If you like to have more details, apply the print function with the additional argument complete = TRUE.\n\ntau_u(exampleAB$Johanna) |&gt; print(complete = TRUE)\n\nTau-U\nMethod: complete \nApplied Kendall's Tau-b\n95% CIs for tau are reported.\nCI method: z\n\nCase: Johanna \n                            pairs pos neg ties   S      D  Tau CI lower\nA vs. B                        75  75   0    0  75  75.00 1.00      NaN\nTrend A                        10   5   5    0   0  10.00 0.00    -0.88\nTrend B                       105  87  17    1  70 104.50 0.67     0.24\nA vs. B - Trend A              85  80   5    0  75 126.75 0.59     0.20\nA vs. B + Trend B             180 162  17    1 145 184.45 0.79     0.53\nA vs. B + Trend B - Trend A   190 167  22    1 145 189.50 0.77     0.49\n                            CI upper  SD_S  VAR_S SE_Tau    Z     p  n\nA vs. B                          NaN 22.91 525.00   0.31 3.27 &lt;.001 20\nTrend A                         0.88  4.08  16.67    NaN 0.00  1.00  5\nTrend B                         0.88 20.21 408.33   0.19 3.46 &lt;.001 15\nA vs. B - Trend A               0.82 23.26 541.22   0.18 3.22 &lt;.001 20\nA vs. B + Trend B               0.91 30.53 932.39   0.17 4.75 &lt;.001 20\nA vs. B + Trend B - Trend A     0.90 30.81 949.00   0.16 4.71 &lt;.001 20\n\n\n\n\n8.8.2 Meta analyses\n\n\n\n\n\n\nNote\n\n\n\nThe procedure for calculating the meta-analyses has changed with scan version 0.55.7. Please make sure you are using the latest scan version.\n\n\nIf you pass multiple cases to the tau-u function, it will calculate a Tau-U table for each case and an overall calculation via a meta-analysis.\n\n\n\n\n\n\nCalculating a Tau-U meta analysis\n\n\n\nThe calculation of the Tau-U-meta-analyses involves the following steps:\n\nThe tau values are Fisher-Z transformed to \\(Tau_z\\).\nThe standard error for each transformed value is calculated as either:\n\\(se_z = {1 \\over \\sqrt{n-3}}\\) (Hotelling, 1953)\nor\n\\(se_z = \\sqrt{0.437 \\over n-4}\\) (Fieller, Hartley, & Pearson, 1957)\nThe average \\(tau_z\\) is the mean of \\(tau_z\\) weighted by \\(1 \\over se_z^2\\)\nThe standard error of the average \\(tau_z\\) is \\(se_{M_{tau_z}} = \\sqrt{\\frac{1}{\\sum{weights}}}\\) (Cooper, Hedges, & Valentine, 2009)\nThe p value is calculated with a Z-test (from \\(Z = \\frac{M_{tau_z}}{se_{M_{tau_z}}}\\) )\nThe overall tau value is derived from an inverse-Fisher-Z-transformation.\n\n\n\n\n\n8.8.3 Confidence intervals\n\n\n\n\n\n\nNote\n\n\n\nThe default method for calculating the confidence interval has changed with scan version 0.55.7. Confidence intervals could have been outside the [-1, 1] in earlier versions. Set ci_method = \"s\" for a replication of results from scan version 0.55.6 or earlier.\n\n\nBy default, 95% confidence intervals are calculated for each tau value. You can specify a different interval with the ci argument (ci = 0.90 will calculate a 90% interval). There are three alternative methods for calculating the confidence intervals. When ci_method = \"z\" is set (the default), a general formula for calculating the standard-error of Fisher-Z values is used (Hotelling, 1953). If ci_method = \"tau\", a specific formula for Fisher-Z transformed tau values is applied (Fieller et al., 1957). Both approaches give similar results. A third approach is derived from the standard deviation of the S statistic1. For this method, set ci_method = \"s\". The S method can give implausible values less than -1 or greater than 1. I recommend using the general “z” method or the accurate “tau” method.å\n\ntau_u(exampleAB, ci = 0.90, ci_method = \"tau\")\n\nTau-U\nMethod: complete \nApplied Kendall's Tau-b\n90% CIs for tau are reported.\nCI method: tau\n\nTau-U meta analyses:\nWeight method: z\n90% CIs are reported.\n\n                       Model Tau_U   se CI lower CI upper   z       p\n                     A vs. B  1.00 0.14     1.00     1.00 Inf 0.0e+00\n           A vs. B - Trend A  0.59 0.14     0.42     0.72 4.8 1.3e-06\n           A vs. B + Trend B  0.75 0.14     0.63     0.83 6.9 4.4e-12\n A vs. B + Trend B - Trend A  0.74 0.14     0.61     0.82 6.7 1.8e-11\n\nCase: Johanna \n                             Tau CI lower CI upper  SD_S    Z     p\nA vs. B                     1.00      NaN      NaN 22.91 3.27 &lt;.001\nA vs. B - Trend A           0.59     0.39     0.74 23.26 3.22 &lt;.001\nA vs. B + Trend B           0.79     0.66     0.87 30.53 4.75 &lt;.001\nA vs. B + Trend B - Trend A 0.77     0.63     0.86 30.81 4.71 &lt;.001\n\nCase: Karolina \n                             Tau CI lower CI upper  SD_S    Z     p\nA vs. B                     0.94     0.90     0.96 22.91 3.06 &lt;.001\nA vs. B - Trend A           0.55     0.34     0.71 23.25 3.01 &lt;.001\nA vs. B + Trend B           0.80     0.69     0.88 30.52 4.85 &lt;.001\nA vs. B + Trend B - Trend A 0.78     0.65     0.87 30.79 4.81 &lt;.001\n\nCase: Anja \n                             Tau CI lower CI upper  SD_S    Z     p\nA vs. B                     0.97     0.94     0.98 22.91 3.14 &lt;.001\nA vs. B - Trend A           0.62     0.43     0.76 23.21 3.36 &lt;.001\nA vs. B + Trend B           0.63     0.43     0.76 30.45 3.74 &lt;.001\nA vs. B + Trend B - Trend A 0.64     0.45     0.78 30.71 3.91 &lt;.001"
  },
  {
    "objectID": "ch_overlapping_indices.html#baseline-corrected-tau",
    "href": "ch_overlapping_indices.html#baseline-corrected-tau",
    "title": "8  Overlapping indices",
    "section": "8.9 Baseline corrected tau",
    "text": "8.9 Baseline corrected tau\n\n\n\n\n\n\nThe corrected_tau function call\n\n\n\ncorrected_tau(data, dvar, pvar, mvar, phases = c(1, 2), alpha = 0.05, continuity = FALSE, repeated = FALSE, tau_method = “b”)\n\n\nThis method has been proposed by Tarlow (2016). The baseline data are checked for a significant autocorrelation (based on Kendalls Tau). If so, a non-parameteric Theil-Sen regression is applied for the baseline data where the dependent values are regressed on the measurement time. The resulting slope information is then used to predict data of the B-phase. The dependent variable is now corrected for this baseline trend and the residuals of the Theil-Sen regression are taken for further calculations. Finally, Kendalls tau is calculated for the dependent variable and the dichotomous phase variable. The function here provides two extensions to this procedure: The alternative Siegel repeated median regression is applied when repeated = TRUE (Siegel, 1982) and a continuity correction is applied when continuity = TRUE (both not the defaults).\nHere is a replication of an example provided by Tarlow (2016) :\n\n\n\n\n\n\ncase &lt;- scdf(\n  c(A = 33, 25, 17, 25, 14, 13,14, \n    B = 14, 15, 15, 4, 6, 9, 5 ,4 ,2 ,2 ,8, 11 ,7)\n)\n\ncorrected_tau(case)\n\nBaseline corrected tau\n\nMethod: Theil-Sen regression\nKendall's tau b applied.\nContinuity correction not applied.\n\nCase1 :\n                           tau     z     p\nBaseline autocorrelation -0.75 -2.31  &lt;.05\nUncorrected tau          -0.58 -2.98  &lt;.01\nBaseline corrected tau    0.69  3.57 &lt;.001\n\nBaseline correction should be applied."
  },
  {
    "objectID": "ch_overlapping_indices.html#reliable-change-index",
    "href": "ch_overlapping_indices.html#reliable-change-index",
    "title": "8  Overlapping indices",
    "section": "8.10 Reliable change index",
    "text": "8.10 Reliable change index\n\n\n\n\n\n\nThe rci function call\n\n\n\nrci(data, dvar, pvar, rel, ci = 0.95, graph = FALSE, phases = c(1, 2))\n\n\nBasically, the reliable change index (rci) shows whether a post-test is above a pre-test value. Based on the reliability of the measurements and the standard deviation, the standard error is calculated. The mean difference between phase A and phase B is divided by the standard error. Several authors have proposed refined methods for calculating the rci.\nThe rci function calculates three indices of reliable change (Wise, 2004) and corresponding descriptive statistics.\n\nrci(exampleAB$Johanna, rel = 0.8, graph = TRUE)\n\n\n\n\nReliable Change Index\n\nMean Difference =  19.53333 \nStandardized Difference =  1.678301 \n\nDescriptives:\n         n     mean       SD       SE\nA-Phase  5 54.60000 2.408319 1.077033\nB-Phase 15 74.13333 8.943207 3.999524\n\nReliability =  0.8 \n\n95 % Confidence Intervals:\n           Lower    Upper\nA-Phase 52.48905 56.71095\nB-Phase 66.29441 81.97226\n\nReliable Change Indices:\n                             RCI\nJacobson et al.         18.13624\nChristensen and Mendoza 12.82426\nHageman and Arrindell   18.49426\n\n\n\n\n\n\nBrossart, D. F., Laird, V. C., & Armstrong, T. W. (2018). Interpreting Kendall’s Tau and Tau-U for single-case experimental designs. Cogent Psychology, 5(1), 1–26. https://doi.org/10.1080/23311908.2018.1518687\n\n\nCooper, H., Hedges, L. V., & Valentine, J. C. (2009). Handbook of research synthesis and meta-analysis, the. Retrieved from https://www.jstor.org/stable/10.7758/9781610441384\n\n\nFieller, E. C., Hartley, H. O., & Pearson, E. S. (1957). Tests for rank correlation coefficients. Biometrika, 44, 470–481.\n\n\nHotelling, H. (1953). New Light on the Correlation Coefficient and its Transforms. Journal of the Royal Statistical Society: Series B (Methodological), 15(2), 193–225. https://doi.org/10.1111/j.2517-6161.1953.tb00135.x\n\n\nParker, Richard I., Hagan-Burke, S., & Vannest, K. (2007). Percentage of All Non-Overlapping Data (PAND) An Alternative to PND. The Journal of Special Education, 40(4), 194–204. Retrieved from http://sed.sagepub.com/content/40/4/194.short\n\n\nParker, Richard I., & Vannest, K. (2009). An improved effect size for single-case research: Nonoverlap of all pairs. Behavior Therapy, 40(4), 357–367. Retrieved from http://www.sciencedirect.com/science/article/pii/S0005789408000816\n\n\nParker, Richard I., Vannest, K. J., & Davis, J. L. (2011a). Effect Size in Single-Case Research: A Review of Nine Nonoverlap Techniques. Behavior Modification, 35(4), 303–322. https://doi.org/10.1177/0145445511399147\n\n\nParker, Richard I., Vannest, K. J., Davis, J. L., & Sauber, S. B. (2011b). Combining Nonoverlap and Trend for Single-Case Research: Tau-U. Behavior Therapy, 42(2), 284–299. https://doi.org/10.1016/j.beth.2010.08.006\n\n\nParker, Richard I., Vannest, K. J., Davis, J. L., & Sauber, S. B. (2011c). Combining nonoverlap and trend for single-case research: Tau-u. Behavior Therapy, 42(2), 284–299. https://doi.org/10.1016/j.beth.2010.08.006\n\n\nPustejovsky, J. E. (2016). What is tau-u? Retrieved from https://www.jepusto.com/what-is-tau-u/\n\n\nScruggs, T. E., Mastropieri, M. A., & Casto, G. (1987). The Quantitative Synthesis of Single-Subject Research Methodology and Validation. Remedial and Special Education, 8(2), 24–33. https://doi.org/10.1177/074193258700800206\n\n\nSiegel, A. F. (1982). Robust Regression Using Repeated Medians. Biometrika, 69(1), 242–244. https://doi.org/10.2307/2335877\n\n\nTarlow, K. R. (2016). An Improved Rank Correlation Effect Size Statistic for Single-Case Designs: Baseline Corrected Tau. Behavior Modification, 41(4), 427–467. https://doi.org/10.1177/0145445516676750\n\n\nWise, E. A. (2004). Methods for analyzing psychotherapy outcomes: A review of clinical significance, reliable change, and recommendations for future directions. Journal of Personality Assessment, 82(1), 50–59. Retrieved from http://www.tandfonline.com/doi/abs/10.1207/s15327752jpa8201_10"
  },
  {
    "objectID": "ch_overlapping_indices.html#footnotes",
    "href": "ch_overlapping_indices.html#footnotes",
    "title": "8  Overlapping indices",
    "section": "",
    "text": "S is the difference between concordant and discordant comparisons in a Kendall’s tau calculation. This is the same statistic used to calculate the p-value.↩︎"
  },
  {
    "objectID": "ch_piecewise_regression.html#the-basic-plm-function",
    "href": "ch_piecewise_regression.html#the-basic-plm-function",
    "title": "9  Piecewise linear regressions",
    "section": "9.1 The basic plm function",
    "text": "9.1 The basic plm function\n\n\n\n\n\n\nThe plm function call\n\n\n\nplm(data, dvar, pvar, mvar, AR = 0, model = c(“W”, “H-M”, “B&L-B”, “JW”), family = “gaussian”, trend = TRUE, level = TRUE, slope = TRUE, contrast = c(“first”, “preceding”), contrast_level = c(NA, “first”, “preceding”), contrast_slope = c(NA, “first”, “preceding”), formula = NULL, update = NULL, na.action = na.omit, r_squared = TRUE, var_trials = NULL, dvar_percentage = FALSE, …)\n\n\nThe basic function for applying a regression analyses to a single-case dataset is plm. This function analyses one single-case. In its simplest way, plm takes one argument with an scdf object and it returns a full piecewise-regression analyses.\n\nplm(exampleAB$Johanna)\n\nPiecewise Regression Analysis\n\nContrast model: W / level = first, slope = first\n\nFitted a gaussian distribution.\nF(3, 16) = 28.69; p = 0.000; R² = 0.843; Adjusted R² = 0.814\n\n                   B   2.5%  97.5%    SE      t     p delta R²\nIntercept     54.400 46.776 62.024 3.890 13.986 0.000         \nTrend mt       0.100 -3.012  3.212 1.588  0.063 0.951    0.000\nLevel phase B  7.858 -3.542 19.258 5.816  1.351 0.195    0.018\nSlope phase B  1.525 -1.642  4.692 1.616  0.944 0.359    0.009\n\nAutocorrelations of the residuals\n lag    cr\n   1 -0.32\n   2 -0.13\n   3 -0.01\nLjung-Box test: X²(3) = 2.84; p = 0.417 \n\nFormula: values ~ 1 + mt + phaseB + interB\n\n\nThe output shows the specific contrast settings for the phase and slope calculation (see below for a detailed explanation). Next, the overall model fit is provided. In this specific example the model fit is quite high explaining more than 80% of the variance of the dependent variable. The following regression table shows the unstandardised estimates (B), the lower and upper boundaries of a 95% confidence interval, the standard-error, the t-test statistic, the p-value, and the delta R squared for each predictor.\nIn this example, the intercept is the score estimation at the beginning of the study (here: at the first session, see Figure 9.2). The trend effect (variable mt) is 0.100. This means that for every one point increase in measurement-time, the score increased by 0.100. As this example has a total of 20 sessions, the overall increase due to the trend is \\(19 * 0.100 = 1.9\\) points. The level effect (variable phase with level B) is 7.858. This means that all scores of phase B are increased by 7.858 points. The slope effect (for phase B) is 1.525. This means that for every one point increase in measurement time after the first phase B session, the score increases by 1.525. As there are 15 Phase B sessions in this example, the total increase due to the slope effect is \\(14 * 1.525 = 21.35\\) points.\nFrom these values each data point can be estimated. For example, the last session (\\(i=20\\)) is estimated to be \\(54.400 + 19 * 0.100 + 7.858 + 14 * 1.525 = 85.508\\).\n\nscplot(exampleAB$Johanna) |&gt; add_statline(\"trend\")\n\n\n\n\nFigure 9.2: Example dataset"
  },
  {
    "objectID": "ch_piecewise_regression.html#adjusting-the-model",
    "href": "ch_piecewise_regression.html#adjusting-the-model",
    "title": "9  Piecewise linear regressions",
    "section": "9.2 Adjusting the model",
    "text": "9.2 Adjusting the model\nThe plm model is a complex model specifically suited for single-case studies. It entails a series of important parameters. Nevertheless, often we have specific theoretical assumption that do no include some of these parameters. We might, for example, only expect an immediate but not a continuous change from a medical intervention. Therefore, it would not be useful to include the slope-effect into our modelling. Vice versa, we could investigate an intervention that will just develop across time without an immediate change with the intervention start. Here we should drop the level-effect from out model. Even the assumption of a trend-effect can be dropped in cases where we do not expect a serial dependency of the data and we do not assume intervention independent changes within the time-frame of the study.\nIt is important to keep in mind, that an overly complex model might have negative effects on the test power of an analyses (that is, the probability of detecting an actually existing effect is diminished) (see Wilbert, Lüke, & Börnert-Ringleb, 2022)\n\n9.2.1 The slope, level, and trend arguments\n\nThe plm function comes with three arguments (slope, level, and trend) to include or drop the respective predictors from the plm model. Buy default, all arguments are set TRUE and a full plm model is applied to the data.\nConsider the following data example:\n\nexample &lt;- scdf(\n   values = c(A = 55, 58, 53, 50, 52, \n              B = 55, 68, 68, 81, 67, 78, 73, 72, 78, 81, 78, 71, 85, 80, 76)\n)\n\nplm(example)\n\nPiecewise Regression Analysis\n\nContrast model: W / level = first, slope = first\n\nFitted a gaussian distribution.\nF(3, 16) = 21.36; p = 0.000; R² = 0.800; Adjusted R² = 0.763\n\n                   B   2.5%  97.5%    SE      t     p delta R²\nIntercept     56.400 48.070 64.730 4.250 13.270 0.000         \nTrend mt      -1.400 -4.801  2.001 1.735 -0.807 0.432    0.008\nLevel phase B 16.967  4.510 29.424 6.356  2.670 0.017    0.089\nSlope phase B  2.500 -0.961  5.961 1.766  1.416 0.176    0.025\n\nAutocorrelations of the residuals\n lag    cr\n   1 -0.28\n   2  0.05\n   3 -0.11\nLjung-Box test: X²(3) = 2.14; p = 0.543 \n\nFormula: values ~ 1 + mt + phaseB + interB\n\n\nThe piecewise regression reveals a significant level effect and two non significant effects for trend and slope. In a further analyses we would like to put the slope effect out of the equation. The easiest way to do this is to set the slope argument to FALSE.\n\nplm(example, slope = FALSE)\n\nPiecewise Regression Analysis\n\nContrast model: W / level = first, slope = first\n\nFitted a gaussian distribution.\nF(2, 17) = 29.30; p = 0.000; R² = 0.775; Adjusted R² = 0.749\n\n                   B   2.5%  97.5%    SE      t     p delta R²\nIntercept     51.572 46.455 56.690 2.611 19.752 0.000         \nTrend mt       1.014  0.364  1.664 0.332  3.057 0.007    0.124\nLevel phase B 10.329  1.674 18.983 4.416  2.339 0.032    0.072\n\nAutocorrelations of the residuals\n lag    cr\n   1 -0.07\n   2  0.06\n   3 -0.17\nLjung-Box test: X²(3) = 0.99; p = 0.804 \n\nFormula: values ~ 1 + mt + phaseB\n\n\nIn the resulting estimations the trend and level effects are now significant. The model estimated a trend effect of 1.01 points for every one point increase in measurement-time and a level effect of 10.33 points. That is, with the beginning of the intervention (the B-phase) the score increases by \\(5 x 1.01 + 10.33 = 15.38\\) points (the measurement-time is increases by five, from one to six, at the first session of phase B)."
  },
  {
    "objectID": "ch_piecewise_regression.html#adding-additional-predictors",
    "href": "ch_piecewise_regression.html#adding-additional-predictors",
    "title": "9  Piecewise linear regressions",
    "section": "9.3 Adding additional predictors",
    "text": "9.3 Adding additional predictors\nIn more complex analyses, additional predictors can be included in the piecewise regression model.\nTo do this, we have to change the regression formula ‘manually’ by applying the update argument. The update argument allows to change the underlying regression formula. To add a new variable named for example newVar, set update = .~. + newVar. The .~. part takes the internally build model formula (based on the number of phases in your model and the setting of the slope, level, and trend arguments) and + newVar adds a variable called newVar to the equation.\nHere is an example adding the control variable cigarrets to the model:\n\nplm(exampleAB_add, update = .~. + cigarrets)\n\nPiecewise Regression Analysis\n\nContrast model: W / level = first, slope = first\n\nFitted a gaussian distribution.\nF(4, 35) = 5.87; p = 0.001; R² = 0.402; Adjusted R² = 0.333\n\n                            B   2.5%  97.5%    SE      t     p delta R²\nIntercept              48.971 43.387 54.555 2.849 17.189 0.000         \nTrend day               0.392 -0.221  1.005 0.313  1.253 0.218    0.027\nLevel phase Medication  3.459 -3.382 10.301 3.490  0.991 0.328    0.017\nSlope phase Medication -0.294 -0.972  0.384 0.346 -0.850 0.401    0.012\ncigarrets              -0.221 -1.197  0.755 0.498 -0.443 0.660    0.003\n\nAutocorrelations of the residuals\n lag    cr\n   1  0.20\n   2 -0.19\n   3 -0.16\nLjung-Box test: X²(3) = 4.33; p = 0.228 \n\nFormula: wellbeing ~ day + phaseMedication + interMedication + cigarrets\n\n\nThe output of the plm-function shows the resulting formula for the regression model that includes the cigarettes variable:\nFormula: wellbeing ~ day + phaseMedication + interMedication + cigarrets"
  },
  {
    "objectID": "ch_piecewise_regression.html#sec-dummy-model",
    "href": "ch_piecewise_regression.html#sec-dummy-model",
    "title": "9  Piecewise linear regressions",
    "section": "9.4 Dummy models",
    "text": "9.4 Dummy models\nThe model argument is used to code the dummy variables. These dummy variables are used to compute the slope and level effects of the phase variable.\nThe phase variable is categorical, identifying the phase of each measurement. Typically, categorical variables are implemented by means of dummy variables. In a piecewise regression model two phase effects have to be estimated: a level effect and a slope effect. The level effect is implemented quite straight forward: for each phase beginning with the second phase a new dummy variable is created with values of zero for all measurements except the measurements of the phase in focus where values of one are set.\n\n\n\n\n\nvalues\nphase\nmt\nlevel B\n\n\n\n\n3\nA\n1\n0\n\n\n6\nA\n2\n0\n\n\n4\nA\n3\n0\n\n\n7\nA\n4\n0\n\n\n5\nB\n5\n1\n\n\n3\nB\n6\n1\n\n\n4\nB\n7\n1\n\n\n6\nB\n8\n1\n\n\n3\nB\n9\n1\n\n\n\n\n\n\n\nFor estimating the slope effect of each phase, another kind of dummy variables have to be created. Like the dummy variables for level effects the values are set to zero for all measurements except the ones of the phase in focus. Here, values start to increase with every measurement until the end of the phase.\nVarious suggestions have been made regarding the way in which these values increase (see Huitema & Mckean, 2000). The B&L-B model starts with a one at the first session of the phase and increases with every session while the H-M model starts with a zero.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nslope B\n\n\n\nvalues\nphase\nmt\nlevel B\nmodel B&L-M\nmodel H-M\n\n\n\n\n3\nA\n1\n0\n0\n0\n\n\n6\nA\n2\n0\n0\n0\n\n\n4\nA\n3\n0\n0\n0\n\n\n7\nA\n4\n0\n0\n0\n\n\n5\nB\n5\n1\n1\n0\n\n\n3\nB\n6\n1\n2\n1\n\n\n4\nB\n7\n1\n3\n2\n\n\n6\nB\n8\n1\n4\n3\n\n\n3\nB\n9\n1\n5\n4\n\n\n\n\n\n\n\nApplying the H-M model will give you a “pure” level-effect while the B&L-B model will provide an estimation of the level-effect that is actually the level-effect plus on times the slope-effect (as the model assumes that the slope variable is 1 at the first measurement of the B-phase). For most studies, the H-M model is more appropriate.\nHowever, there is another aspect to be aware of. Usually, in single case designs, the measurement times are coded as starting with 1 and increasing in integers (1, 2, 3, …). At the same time, the estimation of the trend effect is based on the measurement time variable. In this case, the estimate of the model intercept (usually interpreted as the value at the beginning of the study) actually represents the estimate of the starting value plus one times the trend effect. Therefore, I have implemented the W model (since scan version 0.54.4). Here the trend effect is estimated for a time variable shifted to start at 0. As a result, the intercept represents the estimated value at the first measurement of the study. The W model treats slope estimation in the same way as the H-M model. That is, the measurement-time for calculating the slope effect is set to 0 for the first session of each phase. Since scan version 0.54.4 the W model is the default.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nmt\n\n\nslope\n\n\n\nvalues\nphase\nlevel\nB&L-M and H-M\nW\nB&L-M\nH-M and W\n\n\n\n\n3\nA\n0\n1\n0\n0\n0\n\n\n6\nA\n0\n2\n1\n0\n0\n\n\n4\nA\n0\n3\n2\n0\n0\n\n\n7\nA\n0\n4\n3\n0\n0\n\n\n5\nB\n1\n5\n4\n1\n0\n\n\n3\nB\n1\n6\n5\n2\n1\n\n\n4\nB\n1\n7\n6\n3\n2\n\n\n6\nB\n1\n8\n7\n4\n3\n\n\n3\nB\n1\n9\n8\n5\n4"
  },
  {
    "objectID": "ch_piecewise_regression.html#designs-with-more-than-two-phases-setting-the-right-contrasts",
    "href": "ch_piecewise_regression.html#designs-with-more-than-two-phases-setting-the-right-contrasts",
    "title": "9  Piecewise linear regressions",
    "section": "9.5 Designs with more than two phases: Setting the right contrasts",
    "text": "9.5 Designs with more than two phases: Setting the right contrasts\nFor single case studies with more than two phases, things get a bit more complicated. Applying the models described above to three phases would result in a comparison between each phase and the first phase (usually phase A). That is, the regression weights and significance tests indicate the differences between each phase and the values from phase A. Another common use is to compare the effects of a phase with the previous phase.\nAs of scan version 0.54.4, plm allows to set a contrast argument. contrast = \"first\" (the default) will compare all slope and level-effects to the values in the first phase. contrast = \"preceding\" will compare the slope and level-effects to the preceding phase.\nFor the preceding contrast, the dummy variable for the level-effect is set to zero for all phases preceding the phase in focus and set to one for all remaining measurements. Similarly, the dummy variable for the slope-effect is set to zero for all phases preceding the phase in focus and starts at zero (or one, depending on the model setting, see Section 9.4) for the first measurement of the target phase and increases until the last measurement of the case.\nYou can set the contrast differently for the level and slope effects with the arguments constrast_level and contrast_slope. Both can be either \"first\" or \"preceding\".\n(Note: Prior to scan version 0.54.4, the option model = \"JW\" was identical to model = \"B&L-B\", contrast = \"preceding\").\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ncontrast\nfirst\n\n\ncontrast\npreceeding\n\n\n\n\n\nlevel\n\n\nslope\n\n\nlevel\n\n\nslope\n\n\n\nvalues\nphase\nmt\nB\nC\nB\nC\nB\nC\nB\nC\n\n\n\n\n3\nA\n1\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n6\nA\n2\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n4\nA\n3\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n7\nA\n4\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n5\nB\n5\n1\n0\n1\n0\n1\n0\n1\n0\n\n\n3\nB\n6\n1\n0\n2\n0\n1\n0\n2\n0\n\n\n4\nB\n7\n1\n0\n3\n0\n1\n0\n3\n0\n\n\n6\nB\n8\n1\n0\n4\n0\n1\n0\n4\n0\n\n\n3\nB\n9\n1\n0\n5\n0\n1\n0\n5\n0\n\n\n7\nC\n10\n0\n1\n0\n1\n1\n1\n6\n1\n\n\n5\nC\n11\n0\n1\n0\n2\n1\n1\n7\n2\n\n\n6\nC\n12\n0\n1\n0\n3\n1\n1\n8\n3\n\n\n4\nC\n13\n0\n1\n0\n4\n1\n1\n9\n4\n\n\n8\nC\n14\n0\n1\n0\n5\n1\n1\n10\n5"
  },
  {
    "objectID": "ch_piecewise_regression.html#understanding-and-interpreting-contrasts",
    "href": "ch_piecewise_regression.html#understanding-and-interpreting-contrasts",
    "title": "9  Piecewise linear regressions",
    "section": "9.6 Understanding and interpreting contrasts",
    "text": "9.6 Understanding and interpreting contrasts\nIn this section, we will calculate four plm models with different contrast settings for the same single-case data.\nThe example scdf is the case ‘Marie’ from the exampleABC scdf (exampleABC$Marie)\n\n\n\n\n\nFigure 9.3: Example dataset\n\n\n\n\nThe dark-red lines indicate the intercept and slopes when calculated separately for each phase. They are:\n\n\n\n\nTable 9.1: Intercept, slope, and number of measurements calculated separately for each phase\n\n\n\nintercept\nslope\nn\n\n\n\n\nphase A\n60.618\n-1.915\n10\n\n\nphase B\n74.855\n-0.612\n10\n\n\nphase C\n68.873\n-0.194\n10\n\n\n\n\n\n\n\n\nNow we estimate a plm model with four contrast settings (see Table 9.2):\n\n\n\n\nTable 9.2: Estimates of a piecewise-linear regression with contrast models “first” and “preceding”.\n\n\nContrast level\nContrast slope\nintercept\ntrend\nlevel B\nlevel C\nslope B\nslope C\n\n\n\n\nfirst\nfirst\n60.618\n-1.915\n33.388\n46.558\n1.303\n1.721\n\n\npreceding\npreceding\n60.618\n-1.915\n33.388\n0.139\n1.303\n0.418\n\n\nfirst\npreceding\n60.618\n-1.915\n33.388\n33.527\n1.303\n0.418\n\n\npreceding\nfirst\n60.618\n-1.915\n33.388\n13.170\n1.303\n1.721\n\n\n\n\n\n\n\n\n\n9.6.1 Phase B estimates\nAll regression models in Table 9.2 have the same estimates for intercept and trend. These are not affected by the contrasts and are identical to those for phase A in Table 9.1. In addition, in Table 9.2, the estimates for levelB and slopeC are identical since all models contrast the same phase (the first and the preceding phase are both phase A). The values here can be calculated from Table 9.12:\n\\[\nlevelB = intercept_{phaseB} - (intercept_{phaseA} + n_{PhaseA} * slope_{phaseA})\n\\tag{9.1}\\]\n\\[\n33.388 \\approx  74.855 - (60.618 + 10*-1.915)\n\\]\n\\[\nslopeB = slope_{phaseB} - slope_{phaseA}\n\\tag{9.2}\\]\n\\[\n1.303 \\approx -1.915 - (-0.612)\n\\]\n\n\n9.6.2 Phase C estimates\nThe levelC and slopeC estimates of the regression models in Table 9.2 are different for the various contrast models. Depending on the contrast setting, the estimates “answer” a different question. Table 9.3 provides interpretation help.\n\n\nTable 9.3: Interpretations of the effect estimates in various contrast conditions\n\n\n\n\n\n\n\n\nContrast level\nContrast slope\nInterpretation of level C effect\nInterpretation slope C effect\n\n\n\n\nfirst\nfirst\nWhat would be the value if phase A had continued until to the start of phase C and what is the difference to the actual value at the first measurement of phase C?\nWhat is the difference between the slopes of phase C and A3?\n\n\npreceding\npreceding\nWhat would be the value if phase B had continued to the start of phase C and what is the difference to the actual value at the first measurement of phase C?\nWhat is the difference between the slopes of phase C and B?\n\n\nfirst\npreceding\nWhat would be the value if phase A had continued until the start of phase C (assuming a slope effect but no level effect in phase B)? And what is the difference to the actual value at the first measurement of phase C?\nWhat is the difference between the slopes of phase C and B?\n\n\npreceding\nfirst\nWhat would be the value if phase B had continued until the start of phase C (assuming a level but no slope effect in phase B)? And what is the difference to the actual value at the first measurement of phase C?\nWhat is the difference between the slopes of phase C and A?\n\n\n\n\nAll four models are mathematically equivalent, i.e. they produce the same estimates of the dependent variable. Bellow I will show how the estimates from the piecewise regression models relate to the simple regression estimates from Table 9.1. These are \\(intercept_{phaseC} = 68.873\\) and \\(slope_{phaseC} = -0.194\\).\nLevel first and slope first contrasts\nTable 9.2 estimates a levelC increase of 46.558 compared to phase A (the intercept) and a slopeC increase of 1.721.\n\\[\nlevelC = intercept_{phaseC} - (Intercept_{phaseA} + n_{phaseA+B} * slope_{phaseA})\n\\tag{9.3}\\]\n\\[46.558 \\approx 68.873 - (60.618 + 20*-1.915) \\]\n\\[\nslopeC = slope_{phaseC} - slope_{phaseA}\n\\tag{9.4}\\]\n\\[1.721 \\approx -0.194 - (-1.915)\\]\nLevel preceding and slope preceding contrasts\nTable 9.2 estimates a levelC increase of 0.139 compared to phase B and a slopeC increase of 0.418.\n\\[\nlevelC = intercept_{phaseC} - (intercept_{phaseB} + n_{phaseB} * slope_{phaseB})\n\\tag{9.5}\\]\n\\[0.139 \\approx 68.873 - (74.855 + 10*-0.612)\\]\n\\[\nslopeC = slope_{phaseC} - slope_{phaseB}\n\\tag{9.6}\\]\n\\[0.418 \\approx -0.194 - (-0.612)\\]\nLevel first and slope preceding contrasts\nTable 9.2 estimates a levelC increase of 33.388 compared to phase A and a slopeC increase of 0.418.\n\\[\nlevelC = intercept_{phaseC} - (intercept_{phaseA}  + n_{phaseA} * slope_{phaseA} + n_{phaseB} * slope_{phaseB})\n\\tag{9.7}\\]\n\\[\n33.527 \\approx 68.873 - (60.618 + 10 * -1.915 + 10 * -0.612)\n\\]\n\\[\nslopeC = slope_{phaseC} - slope_{phaseB}\n\\tag{9.8}\\]\n\\[0.418 \\approx -0.194 - (-0.612)\\]\nLevel preceding and slope first contrasts\nTable 9.2 estimates a levelC increase of 13.170 compared to phase B and a slopeC increase of 1.721.\n\\[\nlevelC = intercept_{phaseC} - (intercept_{phaseB} + n_{phaseB} * slope_{phaseA})\n\\tag{9.9}\\]\n\\[\n13.170\\approx 68.873 - (74.855 + 10*-1.915)\n\\]\n\\[\nslopeC = slope_{phaseC} - slope_{phaseA}\n\\tag{9.10}\\]\n\\[\n1.721 \\approx -0.194 - (-1.915)\n\\]\n\n\n\n\n\nHuitema, B. E., & Mckean, J. W. (2000). Design specification issues in time-series intervention models. Educational and Psychological Measurement, 60(1), 38–58. Retrieved from http://epm.sagepub.com/content/60/1/38.short\n\n\nWilbert, J., Lüke, T., & Börnert-Ringleb, M. (2022). Statistical power of piecewise regression analyses of single-case experimental studies addressing behavior problems. Frontiers in Education Educational Psychology, 7. https://doi.org/10.3389/feduc.2022.917944"
  },
  {
    "objectID": "ch_piecewise_regression.html#footnotes",
    "href": "ch_piecewise_regression.html#footnotes",
    "title": "9  Piecewise linear regressions",
    "section": "",
    "text": "The session is the ordinal number (1st, 2nd, 3rd, …) of the measurements. While the measurement-time (the value of the measurement-time variable at a specific session/ measurement) is often identical to the session number, this is not necessarily the case. The measurement-time sometimes represents days since the start of the study or other intervals. Therefore, I refer to session or measurement for the ordinal position of the measurement and measurement-time for the value of the time variable at that session number.↩︎\nDifferences here and in the following calculations are due to rounding errors.↩︎\nThe slope of phase A is the trend effect.↩︎"
  },
  {
    "objectID": "ch_multilevel_plm.html",
    "href": "ch_multilevel_plm.html",
    "title": "10  Multilevel plm analyses",
    "section": "",
    "text": "Note\n\n\n\nRead Chapter 9 before you start with this chapter.\n\n\n\n\n\n\n\n\nThe hplm function call\n\n\n\nhplm(data, dvar, pvar, mvar, model = c(“W”, “H-M”, “B&L-B”, “JW”), contrast = c(“first”, “preceding”), contrast_level = NA, contrast_slope = NA, method = c(“ML”, “REML”), control = list(opt = “optim”), random.slopes = FALSE, lr.test = FALSE, ICC = TRUE, trend = TRUE, level = TRUE, slope = TRUE, fixed = NULL, random = NULL, update.fixed = NULL, data.l2 = NULL, …)\n\n\nMultilevel analyses can take the piecewise-regression approach even further. It allows for\n\nanalyzing the effects between phases for multiple single-cases at once\ndescribing variability between subjects regarding these effects, and\nintroducing variables and factors for explaining the differences.\n\nThe basic function for applying a multilevel piecewise regression analysis is hplm. The hplm function is similar to the plm function, so I recommend that you get familar with plm before applying an hplm.\nHere is a simple example:\n\nhplm(exampleAB_50)\n\nHierarchical Piecewise Linear Regression\n\nEstimation method ML \nContrast model: W / level: first, slope: first\n50 Cases\n\nICC = 0.287; L = 339.0; p = 0.000\n\nFixed effects (values ~ 1 + mt + phaseB + interB)\n\n                   B    SE   df      t p\nIntercept     48.398 1.484 1328 32.611 0\nTrend mt       0.579 0.116 1328  5.006 0\nLevel phase B 14.038 0.655 1328 21.436 0\nSlope phase B  0.902 0.119 1328  7.588 0\n\nRandom effects (~1 | case)\n\n          EstimateSD\nIntercept      9.970\nResidual       5.285\n\n\nHere is an example inlcuding random slopes:\n\nhplm(exampleAB_50, random.slopes = TRUE)\n\nHierarchical Piecewise Linear Regression\n\nEstimation method ML \nContrast model: W / level: first, slope: first\n50 Cases\n\nICC = 0.287; L = 339.0; p = 0.000\n\nFixed effects (values ~ 1 + mt + phaseB + interB)\n\n                   B    SE   df      t p\nIntercept     48.211 1.398 1328 34.497 0\nTrend mt       0.621 0.113 1328  5.516 0\nLevel phase B 13.872 0.894 1328 15.513 0\nSlope phase B  0.864 0.116 1328  7.433 0\n\nRandom effects (~1 + mt + phaseB + interB | case)\n\n              EstimateSD\nIntercept          9.352\nTrend mt           0.096\nLevel phase B      4.537\nSlope phase B      0.126\nResidual           4.974\n\n\n\n10.0.1 Adding additional L2-variables\n\n\n\n\n\n\nThe add_l2 function call\n\n\n\nadd_l2(scdf, data_l2, cvar = “case”)\n\n\nIn some analyses researchers want to investigate whether attributes of the individuals contribute to the effectiveness of an intervention. For example might an intervention on mathematical abilities be less effective for student with a migration background due to too much language related material within the training. Such analyses can also be conducted with scan. Therefore, we need to define a new data frame including the relevant information of the subjects of the single-case studies we want to analyze. This data frame consists of a variable labeled case which has to correspond to the case names of the scfd and further variables with attributes of the subjects. To build a data frame we can use the R function data.frame.\n\nL2 &lt;- data.frame(\n  case = c(\"Antonia\",\"Theresa\", \"Charlotte\", \"Luis\", \"Bennett\", \"Marie\"), \n  age = c(16, 13, 13, 10, 5, 14), \n  sex = c(\"f\",\"f\",\"f\",\"m\",\"m\",\"f\")\n)\nL2\n\n       case age sex\n1   Antonia  16   f\n2   Theresa  13   f\n3 Charlotte  13   f\n4      Luis  10   m\n5   Bennett   5   m\n6     Marie  14   f\n\n\nMultilevel analyses require a high number of Level 2 units. The exact number depends on the complexity of the analyses, the size of the effects, the number of level 1 units, and the variability of the residuals. But surely we need at least about 30 level 2 units. In a single-case design that is, we need at least 30 single-cases (subjects) within the study. After setting the level 2 data frame we can merge it to the scdf with the add_l2() function (alternatively, we can use the data.l2 argument of the hplm function). Then we have to specify the regression function using the update.fixed argument. The level 2 variables can be added just like any other additional variable. For example, we have added a level 2 data-set with the two variables sex and age. update could be construed of the level 1 piecewise regression model .~. plus the additional level 2 variables of interest + sex + age. The complete argument is update.fixed = .~. + sex + age. This analyses will estimate a main effect of sex and age on the overall performance. In case we want to analyze an interaction between the intervention effects and for example the sex of the subject we have to add an additional interaction term (a cross-level interaction). An interaction is defined with a colon. So sex:phase indicates an interaction of sex and the level effect in the single case study. The complete formula now is update.fixed = .~. + sex + age + sex:phase.\nscan includes an example single-case study with 50 subjects example50 and an additional level 2 data-set example50.l2. Here are the first 10 cases of example50.l2.\n\n\n\n\n\ncase\nsex\nage\n\n\n\n\nRoman\nm\n12\n\n\nBrennen\nm\n10\n\n\nIsmael\nm\n13\n\n\nDonald\nm\n11\n\n\nRicardo\nm\n13\n\n\nIzayah\nm\n11\n\n\nIgnacio\nm\n12\n\n\nXavier\nm\n12\n\n\nArian\nm\n10\n\n\nPaul\nm\n10\n\n\n\n\n\n\n\nAnalyzing the data with hplm could look like this:\n\nexampleAB_50 %&gt;%\n  add_l2(exampleAB_50.l2) %&gt;%\n  hplm(update.fixed = .~. + sex + age)\n\nHierarchical Piecewise Linear Regression\n\nEstimation method ML \nContrast model: W / level: first, slope: first\n50 Cases\n\nICC = 0.287; L = 339.0; p = 0.000\n\nFixed effects (values ~ mt + phaseB + interB + sex + age)\n\n                   B     SE   df      t     p\nIntercept     44.878 11.926 1328  3.763 0.000\nTrend mt       0.581  0.116 1328  5.026 0.000\nLevel phase B 14.023  0.655 1328 21.405 0.000\nSlope phase B  0.900  0.119 1328  7.569 0.000\nsexm          -6.440  2.727   47 -2.362 0.022\nage            0.603  1.073   47  0.562 0.577\n\nRandom effects (~1 | case)\n\n          EstimateSD\nIntercept      9.446\nResidual       5.284\n\n# Alternatively:\n# hplm(exampleAB_50, data.l2 = exampleAB_50.l2, update.fixed = .~. + sex + age)\n\nsex is a factor with the levels f and m. So sexm is the effect of being male on the overall performance. age does not seem to have any effect. So we drop age out of the equation and add an interaction of sex and phase to see whether the sex effect is due to a weaker impact of the intervention on males.\n\nexampleAB_50 %&gt;%\n  add_l2(exampleAB_50.l2) %&gt;%\n  hplm(update.fixed = .~. + sex + sex:phaseB)\n\nHierarchical Piecewise Linear Regression\n\nEstimation method ML \nContrast model: W / level: first, slope: first\n50 Cases\n\nICC = 0.287; L = 339.0; p = 0.000\n\nFixed effects (values ~ mt + phaseB + interB + sex + phaseB:sex)\n\n                        B    SE   df       t    p\nIntercept          48.573 1.968 1327  24.676 0.00\nTrend mt            0.609 0.109 1327   5.573 0.00\nLevel phase B      17.726 0.684 1327  25.922 0.00\nSlope phase B       0.884 0.112 1327   7.868 0.00\nsexm               -0.593 2.741   48  -0.216 0.83\nLevel phase B:sexm -7.732 0.609 1327 -12.699 0.00\n\nRandom effects (~1 | case)\n\n          EstimateSD\nIntercept      9.494\nResidual       4.989\n\n\nNow the interaction phase:sexm is significant and the main effect is no longer relevant. It looks like the intervention effect is \\(7.7\\) points lower for male subjects. While the level-effect is \\(17.7\\) points for female subjects it is \\(17.7\\) - \\(7.7\\) = \\(10\\) for males."
  },
  {
    "objectID": "ch_multivariate_plm.html",
    "href": "ch_multivariate_plm.html",
    "title": "11  Multivariate piecewise regression",
    "section": "",
    "text": "Note\n\n\n\nRead Chapter 9 before you start with this chapter.\n\n\n\n\n\n\n\n\nThe mplm function call\n\n\n\nmplm(data, dvar, mvar, pvar, model = c(“W”, “H-M”, “B&L-B”, “JW”), contrast = c(“first”, “preceding”), contrast_level = c(NA, “first”, “preceding”), contrast_slope = c(NA, “first”, “preceding”), trend = TRUE, level = TRUE, slope = TRUE, formula = NULL, update = NULL, na.action = na.omit, …)\n\n\n\nfit &lt;- mplm(exampleAB_add, dvar = c(\"wellbeing\", \"depression\"))\nfit\n\nMultivariate piecewise linear model\n\nDummy model: W level = first, slope = first\n\nCoefficients: \n                       wellbeing depression\n(Intercept)               48.417      4.200\nday                        0.379      0.114\nLevel Phase Medication     3.588     -0.945\nSlope Phase Medication    -0.275     -0.165\n\nFormula: y ~ 1 + day + phaseMedication + interMedication\n\nType III MANOVA Tests: Pillai test statistic\n                       Df test stat approx F num Df den Df Pr(&gt;F)    \n(Intercept)             1     0.915    188.9      2     35 &lt;2e-16 ***\nday                     1     0.055      1.0      2     35   0.38    \nLevel Phase Medication  1     0.033      0.6      2     35   0.56    \nSlope Phase Medication  1     0.039      0.7      2     35   0.50    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nThe following variables were used in this analysis:\n'wellbeing/ depression' as dependent variable, 'phase' as phase variable, and 'day' as measurement-time variable.\n\n\n\nprint(fit, std = TRUE)\n\nMultivariate piecewise linear model\n\nDummy model: W level = first, slope = first\n\nCoefficients: \n                       wellbeing depression\n(Intercept)               48.417      4.200\nday                        0.379      0.114\nLevel Phase Medication     3.588     -0.945\nSlope Phase Medication    -0.275     -0.165\n\nStandardized coefficients: \n                       wellbeing depression\n(Intercept)                0.000      0.000\nday                        0.694      0.441\nLevel Phase Medication     0.276     -0.153\nSlope Phase Medication    -0.356     -0.449\n\nFormula: y ~ 1 + day + phaseMedication + interMedication\n\nType III MANOVA Tests: Pillai test statistic\n                       Df test stat approx F num Df den Df Pr(&gt;F)    \n(Intercept)             1     0.915    188.9      2     35 &lt;2e-16 ***\nday                     1     0.055      1.0      2     35   0.38    \nLevel Phase Medication  1     0.033      0.6      2     35   0.56    \nSlope Phase Medication  1     0.039      0.7      2     35   0.50    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nThe following variables were used in this analysis:\n'wellbeing/ depression' as dependent variable, 'phase' as phase variable, and 'day' as measurement-time variable."
  },
  {
    "objectID": "ch_randomization_test.html#arguments-of-the-rand_test-function",
    "href": "ch_randomization_test.html#arguments-of-the-rand_test-function",
    "title": "12  Randomization tests",
    "section": "12.1 Arguments of the rand_test() function",
    "text": "12.1 Arguments of the rand_test() function\nThe statistics argument defines the statistics on which the phase comparison is based. The following comparisons can be made:\n\n\n\n\n\n\n\nMean A-B\nUses the difference between the mean of Phase A and the mean of Phase B. This is appropriate if a decrease in scores is expected for Phase B.\n\n\nMean B-A\nUses the difference between the mean of Phase B and the mean of Phase A. This is appropriate if a increase in scores is expected for Phase B.\n\n\nMean |A-B|\nUses the absolute value of the difference between the means of Phases A and B.\n\n\nMedian A-B\nThe same as Mean A-B, but based on the median.\n\n\nMedian B-A\nThe same as Mean B-A, but based on the median.\n\n\n\nFurther argument are\n\nArguments of the randomization test function.\n\n\n\n\n\n\nArgument\nWhat it does …\n\n\n\n\nnumber\nSample size of the randomization distribution. The exactness of the p-value can not exceed 1/number (i.e., number = 100 results in p-values with an exactness of one percent). Default is number = 500. For faster processing use number = 100. For more precise p-values set number = 1000.\n\n\ncomplete\nIf TRUE, the distribution is based on a complete permutation of all possible starting combinations. This setting overwrites the number Argument. The default setting is FALSE.\n\n\nlimit\nMinimal number of data points per phase in the sample. The first number refers to the A-phase and the second to the B-phase (e.g., limit = c(5, 3)). If only one number is given, this number is applied to both phases. Default is limit = 5.\n\n\nstartpoints\nAlternative to the limit-parameter, startpoints exactly defines the possible start points of phase B (e.g., startpoints = 4:9 restricts the phase B start points to measurements 4 to 9. startpoints overwrite the limit-parameter.\n\n\nexclude.equal\nIf set to FALSE, which is the default, random distribution values equal to the observed distribution are counted as null-hypothesis conform. That is, they decrease the probability of rejecting the null-hypothesis (increase the p-value). exclude.equal should be set to TRUE if you analyse one single-case design (not a multiple baseline data set) to reach a sufficient power. But be aware, that it increases the chance of an alpha-error."
  },
  {
    "objectID": "ch_randomization_test.html#example",
    "href": "ch_randomization_test.html#example",
    "title": "12  Randomization tests",
    "section": "12.2 Example",
    "text": "12.2 Example\n\nres &lt;- rand_test(exampleAB)\nres\n\nRandomization Test\n\nCombined test for three cases.\n\nComparing phase 1 against phase 2 \nStatistic:  Mean B-A \n\nMinimal length of each phase: A = 5 , B = 5 \nObserved statistic =  20.55556 \n\nDistribution based on a random sample of all 1331 possible combinations.\nn   =  500 \nM   =  18.53912 \nSD  =  1.125025 \nMin =  16.08836 \nMax =  21.34493 \n\nProbability of observed statistic based on distribution:\np   =  0.038 \n\nShapiro-Wilk Normality Test: W = 0.979; p = 0.000  (Hypothesis of normality rejected)\n\nProbabilty of observed statistic based on the assumption of normality:\nz = 1.7923, p = 0.0365 (single sided)\n\n\n\n\n\n\n\n\nThe plot_rand function call\n\n\n\nplot_rand(object, xlab = NA, ylab = “Frequency”, title = “Random distribution”, text_observed = “observed”, color = “lightgrey”, …)\n\n\nThe print_rand() function plots a distribution of the random sample against the observed statistic:\n\nplot_rand(res)\n\n\n\n\n\n\n\n\nBulté, I., & Onghena, P. (2008). An r package for single-case randomization tests. Behavior Research Methods, 40(2), 467–478. https://doi.org/10.3758/BRM.40.2.467\n\n\nBulté, I., & Onghena, P. (2009). Randomization tests for multiple-baseline designs: An extension of the SCRT-r package. Behavior Research Methods, 41(2), 477–485. https://doi.org/10.3758/BRM.41.2.477"
  },
  {
    "objectID": "ch_powertest.html#the-idea-of-a-power-test",
    "href": "ch_powertest.html#the-idea-of-a-power-test",
    "title": "13  Power analyses",
    "section": "13.1 The idea of a power-test",
    "text": "13.1 The idea of a power-test\nThe powert_test() function provides the alpha error probability and power when analyzing a specific effect of a single-case design with a given statistical method.\nFor example, you have a one case design with phase length A = 10 and B = 20. You assume a strong level effect of d = 1 and you expect a slight trend effect of d = 0.02 (per measurement). You might be interested to answer two questions:\n\nHow suitable is a plm model for detecting the level-effect? (also: what is the power to detect the level effect?).\nWhat if I had the same design but without a level-effect. How often would the plm falsely find a significant level-effect? (also: how large is the alpha-error probability for the level-effect?).\n\nIn principle, power_test() takes a single case design and repeatedly generates random cases based on that design. Each case is now analyzed with a given statistical method. The proportion of significant effects in these analyses is an estimator of the test-power. In a second step the design is stripped of the target effect and again multiple cases are generated on this changed design and analyzed with the same method. Now, the proportion of significant effects is the estimator for the alpha-error probability."
  },
  {
    "objectID": "ch_powertest.html#set-up-a-single-case-design",
    "href": "ch_powertest.html#set-up-a-single-case-design",
    "title": "13  Power analyses",
    "section": "13.2 Set up a single-case design",
    "text": "13.2 Set up a single-case design\n\n\n\n\n\n\nThe design function call\n\n\n\ndesign(n = 1, phase_design = list(A = 5, B = 15), trend = 0, level = list(0), slope = list(0), start_value = 50, s = 10, rtt = 0.8, extreme_prop = list(0), extreme_range = c(-4, -3), missing_prop = 0, distribution = c(“normal”, “gaussian”, “poisson”, “binomial”), n_trials = NULL, mt = NULL, B_start = NULL, m, MT)\n\n\nThe design function sets up a single-case design. You can define various parameters of that design:\n\n\n\n\nTable 13.1: Core arguments of the design function\n\n\nArgument\nWhat it does ...\n\n\n\n\nn\nNumber of cases to be created (Default is n = 1).\n\n\nphase_design\nA list defining the length and label of each phase. E.g., phase.length = list(A1 = 10, B1 = 10, A2 = 10, B2 = 10). Use vectors if you want to define different values for each case phase.length = list(A = c(10, 15), B = c(10, 15).\n\n\ntrend\nDefines the effect size of a trend added incrementally to each measurement across the whole data-set. To assign different trends to several single-cases, use a vector of values (e.g. trend = c(.1, .3, .5)). If the number of cases exceeds the length of the vector, values are recycled. When using a 'gaussian' distribution, the trend parameters indicate effect size d changes. When using a binomial or poisson distribution, trend indicates an increase in points / counts per measurement.\n\n\nlevel\nA list that defines the level increase (effect size d) at the beginning of each phase relative to the previous phase (e.g. list(A = 0, B = 1)). The first element must be zero as the first phase of a single-case has no level effect (if you have one less list element than the number of phases, scan will add a leading element with 0 values). Use vectors to define variable level effects for each case (e.g. list(A = c(0, 0), B = c(1, 2))). When using a 'gaussian' distribution, the level parameters indicate effect size d changes. When using a binomial or poisson distribution, level indicates an increase in points / counts with the onset of each phase.\n\n\nslope\nA list that defines the increase per measurement for each phase compared to the previous phase. slope = list(A = 0, B = .1 generates an incremental increase of 0.1 per measurement starting at the B phase. The first list element must be zero as the first phase of a single-case has no slope effect (if you have one less list element than the number of phases, scan will add a leading element with 0 values). Use vectors to define variable slope effects for each case (e.g. list(A = c(0, 0), B = c(0.1, 0.2))). If the number of cases exceeds the length of the vector, values are recycled. When using a 'gaussian' distribution, the slope parameters indicate effect size d changes per measurement. When using a binomial or poisson distribution, slope indicates an increase in points / counts per measurement.\n\n\nrtt\nReliability of the underlying simulated measurements. Set rtt = .8 by default. To assign different reliabilities to several single-cases, use a vector of values (e.g. rtt = c(.6, .7, .8)). If the number of cases exceeds the length of the vector, values are repeated. rtt has no effect when you're using binomial or poisson distributed scores.\n\n\nstart_value\nStarting value at the first measurement. Default is 50. To assign different start values to several single-cases, use a vector of values (e.g. c(50, 42, 56)). If the number of cases exceeds the length of the vector, values are recycled.\n\n\ns\nStandard deviation used to calculate absolute values from level, slope, trend effects and to calculate and error distribution from the rtt values. Set to 10 by default. To assign different variances to several single-cases, use a vector of values (e.g. s = c(5, 10, 15)). If the number of cases exceeds the length of the vector, values are recycled. if the distribution is 'poisson' or 'binomial' s is not applied.\n\n\nextreme_prop\nProbability of extreme values. extreme.p = .05 gives a five percent probability of an extreme value. A vector of values assigns different probabilities to multiple cases. If the number of cases exceeds the length of the vector, values are repeated.\n\n\nextreme_range\nRange for extreme values, expressed as effect size d. extreme.d = c(-7,-6) uses extreme values within a range of -7 and -6 standard deviations. In case of a binomial or poisson distribution, extreme.d indicates points / counts. Caution: the first value must be smaller than the second, otherwise the procedure will fail.\n\n\nmissing_prop\nPortion of missing values. missing.p = 0.1 creates 10% of all values as missing). A vector of values assigns different probabilities to multiple cases. If the number of cases exceeds the length of the vector, values are repeated.\n\n\ndistribution\nDistribution of the scores. Default is distribution = 'normal'. Possible values are 'normal' (or 'gaussian'), 'binomial', and 'poisson'.\n\n\nprob\nIf distribution is set 'binomial', prob passes the probability of occurrence."
  },
  {
    "objectID": "ch_powertest.html#conducting-a-power-test",
    "href": "ch_powertest.html#conducting-a-power-test",
    "title": "13  Power analyses",
    "section": "13.3 Conducting a power-test",
    "text": "13.3 Conducting a power-test\nWhen conduction a power test, you firstly need to define a design which you like to be tested.\n\ndesign &lt;- design(\n  n = 1,\n  phase_design = list(A = 10, B = 20),\n  level = list(A = 0, B = 1),\n  trend = 0.02,\n  distribution = \"normal\"\n)\n\nThen you have to choose the statistical method. The power_test function applies three methods by default: plm, randomization test, and Tau U. These default values are only suitable when your design is a one case single-case study.\nLet us start with the defaults and conduct a power analysis for our previously set design: (This might take some time. Even in the default setting with 100 simulations you might wait a few seconds. For more precise estimations I recommend 1000 simulations - or even higher.)\n\nres &lt;- power_test(design)\nres\n\nTest-Power in percent:\n\n    Method Power Alpha Error Alpha:Beta Correct\n plm_level    80           8      1:2.5      86\n      rand    65           4      1:8.8      80\n      tauU   100          18      1:0.0      91\n\n\nThe results show that the plm test and the randomization test have similar power and alpha-error probabilities (the differences here may be due to outliers of the random samples. A more intensive computation with 1000 simulations shows slightly better values for the plm). The tau U test has an unacceptably high alpha-error which is due to the trend we put into the design. Alpha:Beta depicts the relation of the Alpha and Beta error (power = 1 - Beta). Correct is the overall proportion of correct categorizations and p is the results of a binomial-test of Correct against 50%."
  },
  {
    "objectID": "ch_powertest.html#statistical-methods",
    "href": "ch_powertest.html#statistical-methods",
    "title": "13  Power analyses",
    "section": "13.4 Statistical methods",
    "text": "13.4 Statistical methods\nThe method argument takes a list where each element depicts a statistical method. Currently, the following character strings are predefined:\n\n\n\n\nTable 13.2: Statistical methods\n\n\nName\nSingle/ multiple cases\nWhat it means ...\n\n\n\n\nplm_level\nsingle\nA complete plm model for normal distributed dependent variables. It checks for the level effect.\n\n\nplm_slope\nsingle\nA complete plm model for normal distributed dependent variables. It checks for the slope effect.\n\n\nplm_poisson_level\nsingle\nLike plm_level but for poisson distributed dependent variables.\n\n\nplm_poisson_slope\nsingle\nLike plm_slope but for poisson distributed dependent variables.\n\n\nhplm_level\nmultiple\nA complete hplm model for normal distributed dependent variables. It checks for the level effect.\n\n\nhplm_slope\nmultiple\nA complete hplm model for normal distributed dependent variables. It checks for the slope effect.\n\n\ntauU\nsinlge\nA tauU test with method complete and taub estimations. It checks the 'A vs. B - Trend A' variation.\n\n\ntauU_slope\nsinlge\nA tauU test with method complete and taub estimations. It checks the 'A vs. B - Trend A + Trend B' variation.\n\n\ntauU_meta\nmultiple\nLike 'TauU' but with the results from a meta analyses (fixed effects). Very slow.\n\n\ntauU_slope_meta\nmultiple\nLike 'TauU_slope' but with the results from a meta analyses (fixed effects). Very slow.\n\n\nbase_tau\nsingle\nA baseline corrected tau test.\n\n\nrand\nsingle and multiple\nA randomization test for 'Mean B-A' with 100 permutations."
  },
  {
    "objectID": "ch_powertest.html#confidence-intervals-and-binomial-tests",
    "href": "ch_powertest.html#confidence-intervals-and-binomial-tests",
    "title": "13  Power analyses",
    "section": "13.5 Confidence intervals and binomial tests",
    "text": "13.5 Confidence intervals and binomial tests\nWith only 100 simulations you will have quite large confidence intervals for the power, alpha error probability, and correct estimations. You can calculate these intervals by setting the ci argument. For 95% CI’s set ci = 0.95 for 99% ci = 0.99.\n\npower_test(design, ci = 0.95)\n\nTest-Power in percent:\n\n    Method Power 2.5% 97.5% Alpha Error 2.5% 97.5% Alpha:Beta Correct 2.5%\n plm_level    65   55    74           6  2.2    13      1:5.8      80   73\n      rand    58   48    68           2  0.2     7     1:21.0      78   72\n      tauU   100   96   100          29 20.4    39      1:0.0      86   80\n 97.5%\n    85\n    84\n    90\n\n\nYou can also test the power, alpha error, and correct estimates against predefined values. In order to do that, set binom_test = TRUE. The power will be tested against being greater or equal to 80%, the alpha error against being less or equal 5%, and the correct proportion against being greater equal 87.5%.\n\npower_test(design, binom_test = TRUE)\n\nTest-Power in percent:\n\n    Method Power Alpha Error Alpha:Beta Correct p Power&gt;=80 p Alpha Error&lt;=5\n plm_level    66           2     1:17.0      82           1                0\n      rand    57           3     1:14.3      77           1                0\n      tauU   100          31      1:0.0      84           0                1\n p Correct&gt;=87.5\n             1.0\n             1.0\n             0.9\n\n\nIf you want to define individual values for the three tests, set the binom_test_power. binom_test_alpha, and/or, the binom_test_correct arguments."
  },
  {
    "objectID": "ch_powertest.html#advanced-methods",
    "href": "ch_powertest.html#advanced-methods",
    "title": "13  Power analyses",
    "section": "13.6 Advanced methods",
    "text": "13.6 Advanced methods\nNote: You need specific knowledge on how to create functions in R and on data structures to follow all aspects of this section.\nInstead of one of the predefined character strings you can also create you own functions and implement these. You function must take an scdf as the first argument and return a single numeric p-value.\nHere is an example that implements a method for the significance of a NAP (nonoverlap of all pairs) test. This is statistically identical to a U-Test comparing phase A and B.\n\nset.seed(1) # only needed to make this example replicable\n\nmcmethod_nap &lt;- function(scdf) {\n  nap(scdf)$nap[1, \"p\"]\n}\n\npower_test(design, method = list(nap = mcmethod_nap, \"rand\", \"plm_level\"))\n\nTest-Power in percent:\n\n    Method Power Alpha Error Alpha:Beta Correct\n       nap   100          47      1:0.0      76\n      rand    65           5      1:7.0      80\n plm_level    73           3      1:9.0      85\n\n\nHere is another example for a fast plm function for poisson distributed data based on the fastglm package:\n\nplm_fast &lt;- function(data) {\n  data &lt;- unlist(data, recursive = FALSE)\n  y  &lt;- data$values\n  n1 &lt;- sum(data$phase == \"A\")\n  n2 &lt;- sum(data$phase == \"B\")\n  D &lt;- c(rep(0, n1), rep(1, n2))\n  mt &lt;- data$mt\n  inter &lt;- (mt - mt[n1]) * D\n  x &lt;- matrix(\n    c(rep(1, n1 + n2), mt, D, inter),\n    nrow = n1 + n2,\n    ncol = 4\n  )\n  full &lt;- fastglm::fastglm(x = x, y = y, family = \"poisson\", method = 2)\n  summary(full)$coef[3, 4]\n}\n\n\npower_test(design, method = list(\"fast plm\" = plm_fast))"
  },
  {
    "objectID": "ch_powertest.html#computation-duration",
    "href": "ch_powertest.html#computation-duration",
    "title": "13  Power analyses",
    "section": "13.7 Computation duration",
    "text": "13.7 Computation duration\nYou can print the returning object of the power_test function with added computation duration time by setting duration = TRUE\n\nprint(res, duration = TRUE)\n\nTest-Power in percent:\n\n    Method Power Alpha Error Alpha:Beta Correct\n plm_level    80           8      1:2.5      86\n      rand    65           4      1:8.8      80\n      tauU   100          18      1:0.0      91\n\nComputation duration is 0.9 seconds.\n\n\nThe duration depends heavily on the applied test methods. Regressions are faster than randomization tests and tau U tests are quiet slow:\n\nres1 &lt;- power_test(design, method = \"plm_level\")\nres2 &lt;- power_test(design, method = \"rand\")\nres3 &lt;- power_test(design, method = \"tauU\")\n\n# Elapsed time in seconds for each procedure\nattr(res1, \"computation_duration\")[3]\n## elapsed \n##   0.076\nattr(res2, \"computation_duration\")[3]\n## elapsed \n##   0.257\nattr(res3, \"computation_duration\")[3]\n## elapsed \n##   0.484\n\n… and what about our new fast-glm function?\n\nset.seed(1)\ndesign &lt;- design(\n  n = 1,\n  phase_design = list(A = 10, B = 20),\n  level = list(A = 0, B = 1),\n  trend = 0.02,\n  distribution = \"poisson\"\n)\n\nres1 &lt;- power_test(design, method = list(\"fast plm\" = plm_fast))\nres2 &lt;- power_test(design, method = \"plm_poisson_level\")\n\nattr(res1, \"computation_duration\")[3]\n\nelapsed \n  0.119 \n\nattr(res2, \"computation_duration\")[3]\n\nelapsed \n  0.135"
  },
  {
    "objectID": "ch_export.html#single-case-data-files",
    "href": "ch_export.html#single-case-data-files",
    "title": "14  Exporting scan results",
    "section": "14.1 Single case data files",
    "text": "14.1 Single case data files\n\n\n\n\n\n\nThe scdf export function call\n\n\n\nexport(object, caption = NA, footnote = NA, filename = NA, kable_styling_options = list(), kable_options = list(), cols, …)\n\n\n\nexport(exampleA1B1A2B2_zvt)\n\n\nSingle case data frame with three cases\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTick\n\n\nTrick\n\n\nTrack\n\n\n\nzvt\nd2\nday\npart\nzvt\nd2\nday\npart\nzvt\nd2\nday\npart\n\n\n\n\n47\n131\n1\nA1\n51\n100\n1\nA1\n54\n89\n1\nA1\n\n\n58\n134\n2\nA1\n58\n126\n2\nA1\n57\n116\n2\nA1\n\n\n76\n141\n3\nA1\n70\n130\n3\nA1\n51\n114\n3\nA1\n\n\n63\n141\n4\nB1\n65\n130\n4\nB1\n61\n131\n4\nB1\n\n\n71\n140\n5\nB1\n67\n137\n5\nB1\n57\n132\n5\nB1\n\n\n59\n140\n6\nB1\n63\n133\n6\nB1\n53\n130\n6\nB1\n\n\n64\n138\n7\nA2\n64\n136\n7\nA2\n58\n128\n7\nA2\n\n\n69\n140\n8\nA2\n70\n137\n8\nA2\n57\n131\n8\nA2\n\n\n72\n141\n9\nA2\n70\n135\n9\nA2\n60\n130\n9\nA2\n\n\n77\n140\n10\nB2\n68\n128\n10\nB2\n55\n129\n10\nB2\n\n\n76\n138\n11\nB2\n69\n137\n11\nB2\n58\n118\n11\nB2\n\n\n73\n140\n12\nB2\n70\n138\n12\nB2\n58\n131\n12\nB2"
  },
  {
    "objectID": "ch_export.html#descriptive-stats",
    "href": "ch_export.html#descriptive-stats",
    "title": "14  Exporting scan results",
    "section": "14.2 Descriptive stats",
    "text": "14.2 Descriptive stats\n\n\n\n\n\n\nThe describe export function call\n\n\n\nexport(object, caption = NA, footnote = NA, filename = NA, kable_styling_options = list(), kable_options = list(), flip = FALSE, …)\n\n\n\nres &lt;- describe(GruenkeWilbert2014)\nexport(res)\n\n\nDescriptive statistics\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nn\n\n\nMissing\n\n\nM\n\n\nMedian\n\n\nSD\n\n\nMAD\n\n\nMin\n\n\nMax\n\n\nTrend\n\n\n\nCase\nDesign\nA\nB\nA\nB\nA\nB\nA\nB\nA\nB\nA\nB\nA\nB\nA\nB\nA\nB\n\n\n\n\nAnton\nA-B\n4\n14\n0\n0\n5.00\n9.14\n5\n9\n0.82\n0.77\n0.74\n1.48\n4\n8\n6\n10\n-0.40\n0.03\n\n\nBob\nA-B\n7\n11\n0\n0\n3.00\n8.82\n3\n9\n0.82\n0.87\n1.48\n0.00\n2\n7\n4\n10\n0.04\n0.04\n\n\nPaul\nA-B\n6\n12\n0\n0\n3.83\n8.83\n4\n9\n0.75\n0.72\n0.74\n0.74\n3\n8\n5\n10\n-0.26\n0.02\n\n\nRobert\nA-B\n8\n10\n0\n0\n4.12\n8.90\n4\n9\n0.83\n0.99\n1.48\n1.48\n3\n7\n5\n10\n-0.06\n-0.14\n\n\nSam\nA-B\n5\n13\n0\n0\n4.60\n9.08\n5\n9\n0.55\n0.86\n0.00\n1.48\n4\n8\n5\n10\n0.10\n0.03\n\n\nTim\nA-B\n4\n14\n0\n0\n3.00\n9.00\n3\n9\n0.82\n0.96\n0.74\n1.48\n2\n7\n4\n10\n-0.60\n0.00\n\n\n\nNote:   n = Number of measurements; Missing = Number of missing values; M = Mean; Median = Median; SD = Standard deviation; MAD = Median average deviation; Min = Minimum; Max = Maximum; Trend = Slope of dependent variable regressed on measurement-time."
  },
  {
    "objectID": "ch_export.html#standardized-mean-differences",
    "href": "ch_export.html#standardized-mean-differences",
    "title": "14  Exporting scan results",
    "section": "14.3 Standardized mean differences",
    "text": "14.3 Standardized mean differences\n\n\n\n\n\n\nThe smd export function call\n\n\n\nexport(object, caption = NA, footnote = NA, filename = NA, select = c(“Case”, Mean A = “mA”, Mean B = “mB”, SD A = “sdA”, SD B = “sdB”, SD Cohen = “sd cohen”, SD Hedges = “sd hedges”, “Glass’ delta”, “Hedges’ g”, “Hedges’ g correction”, “Hedges’ g durlak correction”, “Cohen’s d”), kable_styling_options = list(), kable_options = list(), round = 2, flip = FALSE, …)\n\n\n\nsmd(exampleAB) |&gt; export(flip = TRUE)\n\n\nStandardizes mean differences. Comparing phase 1 against phase 2\n\n\nStatistic\nJohanna\nKarolina\nAnja\n\n\n\n\nMean B\n74.13\n73.47\n74.07\n\n\nSD A\n2.41\n6.83\n3.05\n\n\nSD B\n8.94\n9.76\n7.57\n\n\nSD Cohen\n6.55\n8.43\n5.77\n\n\nSD Hedges\n7.97\n9.19\n6.83\n\n\nGlass' delta\n8.11\n3.17\n6.71\n\n\nHedges' g\n2.45\n2.36\n3.00\n\n\nHedges' g correction\n2.35\n2.26\n2.87\n\n\nHedges' g durlak correction\n2.23\n2.14\n2.72\n\n\nCohen's d\n2.98\n2.57\n3.55\n\n\n\nNote:   SD Cohen = unweigted average of the variance of both phases; SD Hedges = weighted average of the variance of both phases with a degrees of freedom correction; Glass' delta = mean difference divided by the standard deviation of the A-phase; Hedges' g = mean difference divided by SD Hedges; Hedges' g (durlak) correction = approaches for correcting Hedges' g for small sample sizes; Cohens d = mean difference divided by SD Cohen."
  },
  {
    "objectID": "ch_export.html#trend-analysis",
    "href": "ch_export.html#trend-analysis",
    "title": "14  Exporting scan results",
    "section": "14.4 Trend analysis",
    "text": "14.4 Trend analysis\n\n\n\n\n\n\nThe trend export function call\n\n\n\nexport(object, caption = NA, footnote = NA, filename = NA, kable_styling_options = list(), kable_options = list(), round = 2, …)\n\n\n\nexampleABC$Marie %&gt;%\n  trend() %&gt;%\n  export()\n\n\nTrend analysis\n\n\nPhase\nIntercept\nB\nBeta\n\n\n\n\nLinear\n\n\nALL\n55.16\n0.61\n0.39\n\n\nA\n60.62\n-1.92\n-0.70\n\n\nB\n74.85\n-0.61\n-0.16\n\n\nC\n68.87\n-0.19\n-0.05\n\n\nQuadratic\n\n\nALL\n59.13\n0.02\n0.33\n\n\nA\n57.94\n-0.21\n-0.71\n\n\nB\n73.22\n-0.04\n-0.10\n\n\nC\n68.49\n-0.02\n-0.04"
  },
  {
    "objectID": "ch_export.html#overlap-indices",
    "href": "ch_export.html#overlap-indices",
    "title": "14  Exporting scan results",
    "section": "14.5 Overlap indices",
    "text": "14.5 Overlap indices\n\n\n\n\n\n\nThe overlap export function call\n\n\n\nexport(object, caption = NA, footnote = NULL, filename = NA, kable_styling_options = list(), kable_options = list(), round = 2, flip = FALSE, …)\n\n\n\nexampleA1B1A2B2_zvt %&gt;%\n  select_phases(A = c(1,3), B = c(2,4)) %&gt;%\n  overlap() %&gt;%\n  export(flip = TRUE)\n\n\nOverlap indices. Comparing phase 1 against phase 2\n\n\nStatistic\nTick\nTrick\nTrack\n\n\n\n\nPND\n16.67\n0.00\n16.67\n\n\nPEM\n66.67\n50.00\n50.00\n\n\nPET\n66.67\n33.33\n33.33\n\n\nNAP\n68.06\n51.39\n58.33\n\n\nNAP-R\n36.11\n2.78\n16.67\n\n\nPAND\n66.67\n50.00\n66.67\n\n\nIRD\n0.33\n0.33\n0.17\n\n\nTau-U (A + B - trend A)\n0.07\n-0.16\n-0.04\n\n\nTau-U (A + B - trend A + trend B)\n0.14\n0.03\n-0.03\n\n\nBase Tau\n0.27\n-0.25\n0.13\n\n\nDelta M\n5.50\n3.17\n0.83\n\n\nDelta Trend\n-0.31\n-1.10\n-0.74\n\n\nSMD\n0.52\n0.40\n0.26\n\n\nHedges g\n0.56\n0.50\n0.26\n\n\n\nNote:   PND = Percentage Non-Overlapping Data; PEM = Percentage Exceeding the Median; PET = Percentage Exceeding the Trend; NAP = Nonoverlap of all pairs; NAP-R = NAP rescaled; PAND = Percentage all nonoverlapping data; IRD = Improvement rate difference; Tau U (A + B - trend A) = Parker's Tau-U; Tau U (A + B - trend A + trend B) = Parker's Tau-U; Base Tau = Baseline corrected Tau; Delta M = Mean difference between phases; Delta Trend = Trend difference between phases; SMD = Standardized Mean Difference; Hedges g = Corrected SMD.\n\n\n\n\n\n\n\n\n\n\n\ntau_u(exampleAB_decreasing) |&gt; export()"
  },
  {
    "objectID": "ch_export.html#tau-u",
    "href": "ch_export.html#tau-u",
    "title": "14  Exporting scan results",
    "section": "14.6 Tau-U",
    "text": "14.6 Tau-U\n\n\n\n\n\n\nThe tauu export function call\n\n\n\nexport(object, caption = NA, footnote = NA, filename = NA, select = “auto”, kable_styling_options = list(), kable_options = list(), meta = TRUE, …)\n\n\nSet the argument meta = TRUE (the default) to get the results of the meta analysis or set meta = FALSE to get a table with each case.\n\ntau_u(exampleAB_decreasing) |&gt; export()\n\n\nOverall Tau-U\n\n\nModel\nTau U\nse\nCI lower\nCI upper\nz\np\n\n\n\n\nA vs. B\n-0.98\n0.14\n-0.99\n-0.96\n-16.04\n&lt;.001\n\n\nA vs. B - Trend A\n-0.59\n0.14\n-0.74\n-0.38\n-4.82\n&lt;.001\n\n\nA vs. B + Trend B\n-0.58\n0.14\n-0.73\n-0.37\n-4.72\n&lt;.001\n\n\nA vs. B + Trend B - Trend A\n-0.53\n0.14\n-0.70\n-0.31\n-4.21\n&lt;.001\n\n\n\nNote:   Method is ' complete '. Analyses based on Kendall's Tau b . 95 % CIs for tau are reported.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ntau_u(exampleAB_decreasing) |&gt; export(meta = FALSE)\n\n\nTau-U analyses\n\n\nCase\nModel\nTau\nCI lower\nCI upper\nZ\np\n\n\n\n\nPeter\n\n\n\n\n\n\n\n\n\nA vs. B\n-0.98\n-0.99\n-0.94\n-3.53\n&lt;.001\n\n\n\nTrend A\n-0.29\n-0.86\n0.59\n-0.90\n.36\n\n\n\nTrend B\n-0.10\n-0.62\n0.47\n-0.49\n.62\n\n\n\nA vs. B - Trend A\n-0.57\n-0.81\n-0.18\n-3.18\n&lt;.01\n\n\n\nA vs. B + Trend B\n-0.55\n-0.80\n-0.14\n-3.23\n&lt;.01\n\n\n\nA vs. B + Trend B - Trend A\n-0.48\n-0.76\n-0.05\n-2.96\n&lt;.01\n\n\nTony\n\n\n\n\n\n\n\n\n\nA vs. B\n-0.98\n-0.99\n-0.95\n-3.63\n&lt;.001\n\n\n\nTrend A\n-0.18\n-0.79\n0.60\n-0.62\n.53\n\n\n\nTrend B\n-0.09\n-0.63\n0.51\n-0.41\n.68\n\n\n\nA vs. B - Trend A\n-0.58\n-0.82\n-0.19\n-3.28\n&lt;.01\n\n\n\nA vs. B + Trend B\n-0.57\n-0.81\n-0.18\n-3.37\n&lt;.001\n\n\n\nA vs. B + Trend B - Trend A\n-0.50\n-0.77\n-0.08\n-3.09\n&lt;.01\n\n\nBruce\n\n\n\n\n\n\n\n\n\nA vs. B\n-0.98\n-0.99\n-0.94\n-3.38\n&lt;.001\n\n\n\nTrend A\n0.07\n-0.79\n0.83\n0.19\n.85\n\n\n\nTrend B\n-0.33\n-0.73\n0.24\n-1.64\n.10\n\n\n\nA vs. B - Trend A\n-0.61\n-0.83\n-0.22\n-3.34\n&lt;.001\n\n\n\nA vs. B + Trend B\n-0.62\n-0.83\n-0.24\n-3.69\n&lt;.001\n\n\n\nA vs. B + Trend B - Trend A\n-0.60\n-0.82\n-0.21\n-3.67\n&lt;.001\n\n\n\nNote:   Method is ' complete '. Analyses based on Kendall's Tau b . 95 % CIs for tau are reported."
  },
  {
    "objectID": "ch_export.html#piecewise-linear-models",
    "href": "ch_export.html#piecewise-linear-models",
    "title": "14  Exporting scan results",
    "section": "14.7 Piecewise linear models",
    "text": "14.7 Piecewise linear models\n\n\n\n\n\n\nThe plm export function call\n\n\n\nexport(object, caption = NA, footnote = NA, filename = NA, kable_styling_options = list(), kable_options = list(), nice = TRUE, …)\n\n\n\nres &lt;- plm(exampleA1B1A2B2$Pawel)\nexport(res)\n\n\nPiecewise-regression model predicting variable 'values'\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCI(95%)\n\n\n\n\nParameter\nB\n2.5%\n97.5%\nSE\nt\np\ndelta R²\n\n\n\n\nIntercept\n12.69\n6.18\n19.20\n3.32\n3.82\n&lt;.01\n\n\n\nTrend mt\n0.22\n-1.00\n1.44\n0.62\n0.36\n.72\n0.001\n\n\nLevel phase B1\n16.28\n6.31\n26.26\n5.09\n3.20\n&lt;.01\n0.118\n\n\nLevel phase A2\n1.48\n-18.81\n21.76\n10.35\n0.14\n.88\n0.000\n\n\nLevel phase B2\n11.46\n-20.49\n43.40\n16.30\n0.70\n.48\n0.006\n\n\nSlope phase B1\n-1.41\n-3.13\n0.32\n0.88\n-1.60\n.12\n0.029\n\n\nSlope phase A2\n-1.10\n-2.83\n0.62\n0.88\n-1.25\n.21\n0.018\n\n\nSlope phase B2\n-1.08\n-2.81\n0.64\n0.88\n-1.23\n.22\n0.017\n\n\n\nNote:   F(7, 32) = 7.86; p &lt;.001; R² = 0.632; Adjusted R² = 0.552"
  },
  {
    "objectID": "ch_export.html#hierarchical-piecewise-regressions",
    "href": "ch_export.html#hierarchical-piecewise-regressions",
    "title": "14  Exporting scan results",
    "section": "14.8 Hierarchical piecewise regressions",
    "text": "14.8 Hierarchical piecewise regressions\n\n\n\n\n\n\nThe hplm export function call\n\n\n\nexport(object, caption = NA, footnote = NA, filename = NA, kable_styling_options = list(), kable_options = list(), round = 2, nice = TRUE, …)\n\n\n\nexampleAB_50 %&gt;%\n  add_l2(exampleAB_50.l2) %&gt;%\n  hplm(lr.test = TRUE, random.slopes = TRUE) %&gt;%\n  export()\n\n\nHierarchical Piecewise Linear Regression predicting variable 'values'\n\n\n\n\n\n\n\n\n\n\nPredictors\nB\nSE\ndf\nt\np\n\n\n\n\nIntercept\n48.21\n1.4\n1328\n34.5\n&lt;.001\n\n\nTrend mt\n0.62\n0.11\n1328\n5.52\n&lt;.001\n\n\nLevel phase B\n13.87\n0.89\n1328\n15.51\n&lt;.001\n\n\nSlope phase B\n0.86\n0.12\n1328\n7.43\n&lt;.001\n\n\n\nRandom effects\n\n\n\nSD\nL\ndf\np\n\n\n\nIntercept\n9.35\n348.85\n4\n&lt;.001\n\n\n\nTrend mt\n0.1\n0.83\n4\n.93\n\n\n\nLevel phase B\n4.54\n42.82\n4\n&lt;.001\n\n\n\nSlope phase B\n0.13\n0.76\n4\n.94\n\n\n\nResidual\n4.97\nNA\nNA\nNA\n\n\n\n\nModel\n\n\nAIC\n8693.2\n\n\n\n\n\n\nBIC\n8771.7\n\n\n\n\n\n\nICC\n0.29\nL = 339\np &lt;.001\n\n\n\n\n\nNote:   Estimation method ML; Slope estimation method: W; first; 50 cases."
  },
  {
    "objectID": "ch_export.html#power-analyses",
    "href": "ch_export.html#power-analyses",
    "title": "14  Exporting scan results",
    "section": "14.9 Power analyses",
    "text": "14.9 Power analyses\n\n\n\n\n\n\nThe power_test export function call\n\n\n\nexport(object, caption = NA, footnote = NA, filename = NA, kable_styling_options = list(), kable_options = list(), …)\n\n\n\ndesign &lt;- design(\n  n = 1, phase_design = list(A = 6, B = 9),\n  rtt = 0.8, level = 1.4, trend = 0.05\n)\nset.seed(124)\npower_test(design, n_sim = 10) |&gt; export()\n\n\nTest power in percent\n\n\nMethod\nPower\nAlpha Error\nAlpha:Beta\nCorrect\n\n\n\n\nplm_level\n80\n10\n1:2.0\n85\n\n\nrand\n90\n30\n1:0.3\n80\n\n\ntauU\n100\n10\n1:0.0\n95"
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Brossart, D. F., Laird, V. C., & Armstrong, T. W. (2018).\nInterpreting Kendall’s Tau and Tau-U for single-case\nexperimental designs. Cogent Psychology, 5(1), 1–26.\nhttps://doi.org/10.1080/23311908.2018.1518687\n\n\nBulté, I., & Onghena, P. (2008). An r package for single-case\nrandomization tests. Behavior Research Methods, 40(2),\n467–478. https://doi.org/10.3758/BRM.40.2.467\n\n\nBulté, I., & Onghena, P. (2009). Randomization tests for\nmultiple-baseline designs: An extension of the SCRT-r package.\nBehavior Research Methods, 41(2), 477–485. https://doi.org/10.3758/BRM.41.2.477\n\n\nCooper, H., Hedges, L. V., & Valentine, J. C. (2009). Handbook\nof research synthesis and meta-analysis, the. Retrieved from https://www.jstor.org/stable/10.7758/9781610441384\n\n\nFieller, E. C., Hartley, H. O., & Pearson, E. S. (1957). Tests for\nrank correlation coefficients. Biometrika, 44,\n470–481.\n\n\nHotelling, H. (1953). New Light on the Correlation Coefficient and its\nTransforms. Journal of the Royal Statistical Society: Series B\n(Methodological), 15(2), 193–225. https://doi.org/10.1111/j.2517-6161.1953.tb00135.x\n\n\nHuitema, B. E., & Mckean, J. W. (2000). Design specification issues\nin time-series intervention models. Educational and Psychological\nMeasurement, 60(1), 38–58. Retrieved from http://epm.sagepub.com/content/60/1/38.short\n\n\nIglewicz, B., & Hoaglin, D. C. (1993). How to detect and handle\noutliers. Milwaukee, Wis. : ASQC Quality Press.\n\n\nParker, Richard I., Hagan-Burke, S., & Vannest, K. (2007).\nPercentage of All Non-Overlapping\nData (PAND) An\nAlternative to PND. The Journal of Special\nEducation, 40(4), 194–204. Retrieved from http://sed.sagepub.com/content/40/4/194.short\n\n\nParker, Richard I., & Vannest, K. (2009). An improved effect size\nfor single-case research: Nonoverlap of all pairs.\nBehavior Therapy, 40(4), 357–367. Retrieved from http://www.sciencedirect.com/science/article/pii/S0005789408000816\n\n\nParker, Richard I., Vannest, K. J., & Davis, J. L. (2011). Effect\nSize in Single-Case\nResearch: A Review of\nNine Nonoverlap Techniques.\nBehavior Modification, 35(4), 303–322. https://doi.org/10.1177/0145445511399147\n\n\nParker, Richard I., Vannest, K. J., Davis, J. L., & Sauber, S. B.\n(2011b). Combining Nonoverlap and Trend for\nSingle-Case Research:\nTau-U. Behavior Therapy,\n42(2), 284–299. https://doi.org/10.1016/j.beth.2010.08.006\n\n\nParker, Richard I., Vannest, K. J., Davis, J. L., & Sauber, S. B.\n(2011a). Combining nonoverlap and trend for single-case research: Tau-u.\nBehavior Therapy, 42(2), 284–299. https://doi.org/10.1016/j.beth.2010.08.006\n\n\nPustejovsky, J. E. (2016). What is tau-u? Retrieved from https://www.jepusto.com/what-is-tau-u/\n\n\nR Core Team. (2023). R: A language and environment for statistical\ncomputing. Retrieved from https://www.R-project.org/\n\n\nRStudio Team. (2018). RStudio: Integrated development environment\nfor r. Retrieved from http://www.rstudio.com/\n\n\nScruggs, T. E., Mastropieri, M. A., & Casto, G. (1987). The\nQuantitative Synthesis of\nSingle-Subject Research\nMethodology and Validation. Remedial and\nSpecial Education, 8(2), 24–33. https://doi.org/10.1177/074193258700800206\n\n\nSiegel, A. F. (1982). Robust Regression Using Repeated\nMedians. Biometrika, 69(1), 242–244. https://doi.org/10.2307/2335877\n\n\nTarlow, K. R. (2016). An Improved Rank Correlation Effect Size\nStatistic for Single-Case Designs:\nBaseline Corrected Tau. Behavior Modification,\n41(4), 427–467. https://doi.org/10.1177/0145445516676750\n\n\nWilbert, Juergen. (2023). Scplot: Plot function for single-case data\nframes.\n\n\nWilbert, Juergen, & Lueke, T. (2023). Scan: Single-case data\nanalyses for single and multiple baseline designs.\n\n\nWilbert, Jürgen, Lüke, T., & Börnert-Ringleb, M. (2022). Statistical\npower of piecewise regression analyses of single-case experimental\nstudies addressing behavior problems. Frontiers in Education\nEducational Psychology, 7. https://doi.org/10.3389/feduc.2022.917944\n\n\nWise, E. A. (2004). Methods for analyzing psychotherapy outcomes:\nA review of clinical significance, reliable change, and\nrecommendations for future directions. Journal of Personality\nAssessment, 82(1), 50–59. Retrieved from http://www.tandfonline.com/doi/abs/10.1207/s15327752jpa8201_10"
  },
  {
    "objectID": "app_some_things.html#basic-r",
    "href": "app_some_things.html#basic-r",
    "title": "Appendix A — Some things about R",
    "section": "A.1 Basic R",
    "text": "A.1 Basic R\nR is a script language. That is, you type in text and let R execute the commands you wrote down. Either you work in a console or a textfile. In a console the command will be executed every time you press the RETURN-key. In a textfile you type down your code, mark the part you like to be executed, and run that code (with a click or a certain key). The latter text files can be saved and reused for later R sessions. Therefore, usually you will work in a text file.\nA value is assigned to a variable with the &lt;- operator. Which should be read as an arrow rather than a less sign and a minus sign. A # is followed by a comment to make your code more understandable. So, what follows a # is not interpreted by R. A vector is a chain of several values. With a vector you could describe the values of a measurement series. The c function is used to build a vector (e.g., c(1, 2, 3, 4)). If you like to see the content of a variable you could use the print function. print(x) will display the content of the variable x. A shortcut for this is just to type variable name (and press return) x.\n\n# x is assigned the value 10:\nx &lt;- 10\n\n# See what's inside of x:\nx\n\n[1] 10\n\n# x is assigned a vector with three values:\nx &lt;- c(10, 11, 15)\n\n# ... and display the content of x:\nx\n\n[1] 10 11 15\n\n\nTwo important concepts in R are functions and arguments. A function is the name for a procedure that does something with the arguments that are provided by you. For example, the function mean calculated the mean. mean has an argument x which “expects” that you provide a vector (a series of values) from which it will calculate the mean. mean( x = c(1, 3, 5) ) will compute the mean of the values 1, 3, and 5 and return the result 3. Some functions can take several arguments. mean for example also takes the argument trim. For calculating a trimmed mean. mean( x = c(1, 1, 3, 3, 5, 6, 7, 8, 9, 9), trim = 0.1) will calculate the 10% trimmed mean of the provided values. The name of the first argument could be dropped. That is, mean( c(1, 3, 5) ) will be interpreted by R as mean( x = c(1, 3, 5) ). You could also provide a variable to an argument.\n\nvalues &lt;- c(1, 4, 5, 6, 3, 7, 7, 5)\nmean(x = values)\n\n[1] 4.75\n\n# or shorter:\nmean(values)\n\n[1] 4.75\n\n\nThe return value of a function can be assigned to a new variable instead:\n\ny &lt;- c(1, 4, 5, 6, 3, 7, 7, 5)\nres &lt;- mean(y)\n#now res contains the mean of y:\nres\n\n[1] 4.75\n\n\nEvery function in R has a help page written by the programmers. You can retrieve these pages with the help function or the short cut ?. help(\"mean\") will display the help page for the mean function. The quotation marks are necessary here because you do not provide a variable with the name mean but a word ‘mean’. The shortcut works ?mean. A bit confusingly, you do not need the quotation marks here."
  },
  {
    "objectID": "app_changes.html#important-changes-with-version-0.53",
    "href": "app_changes.html#important-changes-with-version-0.53",
    "title": "Appendix B — Changes",
    "section": "B.1 Important changes with version 0.53",
    "text": "B.1 Important changes with version 0.53\n\nB.1.1 Single-case studies with cases of varying phase design\nSometimes it is necessary to combine single-cases with different phase-designs into one single-case study (for instance when some cases include an extension phase and others do not). Various functions in scan now can handle such a data structure.\n\n\nB.1.2 Piping\nThe concept of piping is great for writing clean and intelligible code that is easier to debug. We imported the pipe function %&gt;% from the magrittr package. Since version 4.1, R has its own pipe operator implementation |&gt;. This is great and works fine with the scan package. But since the |&gt; Operator is not backwards compatible for R prior versions 4.1, we will stick with the %&gt;% for a while.\nTo allow for smooth “piping” we began adding some functions select_phases, subset, select_cases, set_var, set_dvar, set_mvar, set_pvar, and add_l2."
  },
  {
    "objectID": "app_changes.html#important-changes-with-version-0.50",
    "href": "app_changes.html#important-changes-with-version-0.50",
    "title": "Appendix B — Changes",
    "section": "B.2 Important changes with version 0.50",
    "text": "B.2 Important changes with version 0.50\n\nB.2.1 New function names\nWith version 0.50 scan introduced new names for its functions. The old function names are still usable but they will return a “deprecated” warning telling you to use the new function names.\n(rtbl-aliases?) shows the changes.\n\n\n\n\nTable B.1: Scan previous and current function names\n\n\nCurrent function name\nPrevious function name\n\n\n\n\nautocorr\nautocorrSC\n\n\ncorrected_tau\ncorrected_tauSC\n\n\ndescribe [since v0.52]\ndescribeSC\n\n\nfill_missing\nfillmissingSC\n\n\noutlier\noutlierSC\n\n\noverlap\noverlapSC\n\n\npower_test\npower_testSC\n\n\nrand_test\nrandSC; rand.test\n\n\nranks\nrankSC\n\n\nrci\nrCi; rciSC\n\n\nshift\nshiftSC\n\n\nsmooth_cases\nsmoothSC\n\n\nstyle_plot\nstyle.plotSC; style_plotSC\n\n\ntau_u\ntauUSC\n\n\ntrend\ntrendSC\n\n\ntruncate_phase\ntruncateSC\n\n\n\n\n\n\n\n\n\n\nB.2.2 Change target variables in functions\nAll functions in R that analyze data now allow for temporarily changing dependent, phase, and measurement-time variables by adding three argument:\ndvar sets the dependent variable.\npvar sets the phase variable.\nmvar sets the measurement-time variable.\nFor example, overlap(exampleAB_add, dvar = \"depression\") will report overlap parameters for the variable depression while overlap(exampleAB_add) while take wellbeing as the dependent variable (as defined in the scdf).\nAfter finishing the analysis, the variables are set back to their original values as defined in the scdf."
  },
  {
    "objectID": "app_default_settings.html",
    "href": "app_default_settings.html",
    "title": "Appendix C — Default settings",
    "section": "",
    "text": "Some of the default settings of scan can be changed with the options() argument. Table C.1 shows a complete list of options and their default values.\n\n# get the current value of an option\ngetOption(\"scan.print.rows\")\n\n[1] 15\n\n# set option to a different value\noptions(scan.print.rows = 5, scan.print.scdf.name = FALSE)\nprint(exampleAB)\n\n#A single-case data frame with three cases\n\n values mt phase ｜ values mt phase ｜ values mt phase ｜\n     54  1     A ｜     41  1     A ｜     55  1     A ｜\n     53  2     A ｜     59  2     A ｜     58  2     A ｜\n     56  3     A ｜     56  3     A ｜     53  3     A ｜\n     58  4     A ｜     51  4     A ｜     50  4     A ｜\n     52  5     A ｜     52  5     A ｜     52  5     A ｜\n# ... up to 15 more rows\n\noptions(scan.print.rows = 15, scan.print.scdf.name = TRUE)\nprint(exampleAB)\n\n#A single-case data frame with three cases\n\n Johanna: values mt phase ｜ Karolina: values mt phase ｜ Anja: values mt phase\n              54  1     A ｜               41  1     A ｜           55  1     A\n              53  2     A ｜               59  2     A ｜           58  2     A\n              56  3     A ｜               56  3     A ｜           53  3     A\n              58  4     A ｜               51  4     A ｜           50  4     A\n              52  5     A ｜               52  5     A ｜           52  5     A\n              61  6     B ｜               57  6     B ｜           55  6     B\n              62  7     B ｜               56  7     B ｜           68  7     B\n              71  8     B ｜               67  8     B ｜           68  8     B\n              66  9     B ｜               75  9     B ｜           81  9     B\n              64 10     B ｜               66 10     B ｜           67 10     B\n              78 11     B ｜               69 11     B ｜           78 11     B\n              70 12     B ｜               68 12     B ｜           73 12     B\n              74 13     B ｜               73 13     B ｜           72 13     B\n              82 14     B ｜               77 14     B ｜           78 14     B\n              77 15     B ｜               79 15     B ｜           81 15     B\n ｜\n ｜\n ｜\n ｜\n ｜\n ｜\n ｜\n ｜\n ｜\n ｜\n ｜\n ｜\n ｜\n ｜\n ｜\n ｜\n# ... up to five more rows\n\n\n\n\n\n\nTable C.1: Scan Options\n\n\nOption\nDefault\nWhat it does ...\n\n\n\n\nscan.print.cases\n\"fit\"\nMax number of cases printed for scdf objects\n\n\nscan.print.rows\n15\nMax number of rows printed for scdf objects\n\n\nscan.print.cols\n\"all\"\nMax number of columns printed for scdf objects\n\n\nscan.print.digits\n2\nMax number of digits printed for scdf objects\n\n\nscan.print.long\nFALSE\nIf TRUE, prints scdf objects in long format\n\n\nscan.print.scdf.name\nTRUE\nIf TRUE, prints case names of scdf\n\n\nscan.deprecated.warning\nFALSE\nWhen TRUE returns information on deprecated functions\n\n\nscan.export.kable\nlist(digits = 2, linesep = \"\", booktab = TRUE)\nList with default arguments for the kable argument of the export function\n\n\nscan.export.kable_styling\nlist(bootstrap_options = c(\"bordered\", \"condensed\"), full_width = FALSE, position = \"left\", latex_options = \"hold_position\", htmltable_class = \"lightable-classic\")\nList with default arguments for the kable_styling argument of the export function\n\n\nscan.plot.style\n\"grid\"\nNA\n\n\nscan.print.bar\n\"｜\"\nNA\n\n\n\n\n\n\n\n\n\nD Example datasets\n\n\n\n\nTable D.1: Example scdfs\n\n\nName\nInfo\nAuthor\n\n\n\n\nBeretvas2008\n\nBeretvas, S., & Chung, H. (2008). An evaluation of modified R2-change effect size indices for single-subject experimental designs. Evidence-Based Communication Assessment and Intervention, 2, 120-128.\n\n\nBorckardt2014\n\nBorckardt, J. J., & Nash, M. R. (2014). Simulation modelling analysis for small sets of single-subject data collected over time. Neuropsychological Rehabilitation, 24(3-4), 492-506.\n\n\nGrosche2011\nDirect instruction intervention on reading accuracy.\nGrosche, M. (2011). Effekte einer direkt-instruktiven Förderung der Lesegenauigkeit. Empirische Sonderpädagogik, 3(2), 147-161.\n\n\nGrosche2014\nData from a multiple material multi person intervention study on reading.\nMichael Grosche, Timo Lueke, and Juergen Wilbert\n\n\nGruenkeWilbert2014\nData from an intervention study on text comprehension.\nGruenke, M., Wilbert, J., & Stegemann-Calder, K. (2013). Analyzing the effects of story mapping on the reading comprehension of children with low intellectual abilities. Learning Disabilities: A Contemporary Journal, 11(2), 51-64.\n\n\nHuber2014\nBehavioral data (compliance in percent).\nChristian Huber\n\n\nHuitema2000\nExample from Huitema, B. E., & Mckean, J. W. (2000). Design specification issues in time-series intervention models. Educational and Psychological Measurement, 60(1), 38-58.\n\n\n\nLeidig2018\n\nLeidig, T., Casale, G., Wilbert, J., Hennemann, T., Volpe, R. J., Briesch, A., & Grosche, M. (2022). Individual, generalized, and moderated effects of the good behavior game on at-risk primary school students: A multilevel multiple baseline study using behavioral progress monitoring. Frontiers in Education, 7. https://www.frontiersin.org/articles/10.3389/feduc.2022.917138\n\n\nLeidig2018_l2\n\n\n\n\nLenz2013\n\nLenz, A. S. (2013). Calculating Effect Size in Single-Case Research: A Comparison of Nonoverlap Methods. Measurement and Evaluation in Counseling and Development, 46(1), 64-73.\n\n\nParker2007\n\nParker, R. I., Hagan-Burke, S., & Vannest, K. (2007). Percentage of All Non-Overlapping Data (PAND) An Alternative to PND. The Journal of Special Education, 40(4), 194-204.\n\n\nParker2009\n\nParker, R. I., Vannest, K. J., & Brown, L. (2009). The improvement rate difference for single-case research. Exceptional Children, 75(2), 135-150.\n\n\nParker2011\n\nParker, R. I., Vannest, K. J., Davis, J. L., & Sauber, S. B. (2011). Combining Nonoverlap and Trend for Single-Case Research: Tau-U. Behavior Therapy, 42(2), 284-299.\n\n\nParker2011b\n\nParker, R. I., Vannest, K. J., & Davis, J. L. (2011). Effect Size in Single-Case Research: A Review of Nine Nonoverlap Techniques. Behavior Modification, 35(4), 303-322. https://doi.org/10.1177/0145445511399147\n\n\nSSDforR2017\nExample from the SSDforR package.\nCharles Auerbach, PhD & Wendy Zeitlin, PhD; Yeshiva University, Wurzweiler school of social work.\n\n\nTarlow2017\n\nTarlow, K. R. (2017). An Improved Rank Correlation Effect Size Statistic for Single-Case Designs: Baseline Corrected Tau. Behavior Modification, 41(4), 427-467. https://doi.org/10.1177/0145445516676750\n\n\nWaddell2011\nExample from Waddell, D. E., Nassar, S. L., & Gustafson, S. A. (2011). Single-Case Design in Psychophysiological Research: Part II: Statistical Analytic Approaches. Journal of Neurotherapy, 15(2), 160 - 169.\n\n\n\nbyHeart2011\nData from university students learning vocabulary by heart and checking their progress with 20 flashcards each session.\nJuergen Wilbert, 2011\n\n\nexampleA1B1A2B2\n\n\n\n\nexampleA1B1A2B2_zvt\n\n\n\n\nexampleAB\nRandomly created data with normal distributed dependent variable.\n\n\n\nexampleABAB\nRandomly created data with uniform distribution.\n\n\n\nexampleABC\n\n\n\n\nexampleABC_150\nRandom data-set for testing out hplm. Level and slope effects vary.\n\n\n\nexampleABC_outlier\nRandom data-set based on exampleABC but with outliers.\n\n\n\nexampleAB_50\n\n\n\n\nexampleAB_50.l2\n\n\n\n\nexampleAB_add\nRandom data-set for testing out plm with additional variables.\n\n\n\nexampleAB_decreasing\nRandom data-set from a poisson distribution. Level effect is negative.\n\n\n\nexampleAB_mpd\nA multiple phase design study.\nJuergen Wilbert\n\n\nexampleAB_score\nRandom data-set for binomial data.\n\n\n\nexampleAB_simple\nA simple multiple baseline AB Design.\nJuergen Wilbert\n\n\nexample_A24\nNumber of injuries on a German autobahn before and after implementation of a speedlimit (130km/h).\nMinisterium fuer Infrastruktur und Landesplanung. Land Brandenburg."
  },
  {
    "objectID": "app_supseded_functions.html#sec-plotsc",
    "href": "app_supseded_functions.html#sec-plotsc",
    "title": "Appendix D — Supseded functions",
    "section": "D.1 The plotSC plotting function",
    "text": "D.1 The plotSC plotting function\n\n\n\n\n\n\nThe plotSC function call\n\n\n\nplotSC(data, dvar, pvar, mvar, ylim = NULL, xlim = NULL, xinc = 1, lines = NULL, marks = NULL, phase.names = NULL, xlab = NULL, ylab = NULL, main = ““, case.names = NULL, style = getOption(”scan.plot.style”), …)\n\n\nAfter you build an scdf the plot command helps to visualize the data. When the scdf includes more than one case a multiple baseline figure is provided. Various arguments can be set to customize the appearance of the plot. Table D.1 gives an overview of all available arguments.\n\nplot(exampleA1B1A2B2_zvt)\n\n\n\n\nA simple plot does not need much."
  },
  {
    "objectID": "app_supseded_functions.html#plot-axis",
    "href": "app_supseded_functions.html#plot-axis",
    "title": "Appendix D — Supseded functions",
    "section": "D.2 Plot axis",
    "text": "D.2 Plot axis\nLabels of the axes and for the phases can be changed with the xlab, ylab, and the phase.names arguments. The x- and y-scaling of the graphs are by default calculated as the minimum and the maximum of all included single cases. The xlim and the ylim argument are used to set specific values. The argument takes a vector of two numbers. The first for the lower and the second for the upper limit of the scale. In case of multiple single cases an NA sets the individual minimum or maximum for each case. Assume for example the study contains three single cases ylim = c(0, NA) will set the lower limit for all three single cases to 0 and the upper limit individually at the maximum of each case. The argument xinc sets the incremental steps for the x-axis ticks with corresponding values. For example xinc = 1 will set a tick for every measurement time increase of 1 while xinc = 5 will only set every ffith tick.\n\nplot(\n  exampleABC,\n  phase.names = c(\"Baseline\", \"Intervention\", \"Follow-Up\"),\n  case.names = c(\"First\", \"Second\", \"Third\"),\n  ylab = \"Frequency\",\n  xlab = \"Days\",\n  main = \"An example\",\n  ylim = c(0, 120),\n  xinc = 2\n)\n\n\n\n\nA plot with various axis specidications.\n\n\n\n\n\n\n\n\nTable D.1: Arguments of the plot function\n\n\nArgument\nWhat it does ...\n\n\n\n\ndata\nA single-case data frame.\n\n\nylim\nLower and upper limits of the y-axis\n\n\nxlim\nLower and upper limits of the x-axis.\n\n\nstyle\nA specific design for displaying the plot.\n\n\nlines\nA character or list defining one or more lines or curves to be plotted.\n\n\nmarks\nA list of parameters defining markings of certain data points.\n\n\nmain\nA figure title\n\n\nphase.names\nBy default phases are labeled as given in the phase variable. Use this argument to specify different labels: `phase.names = c('Baseline', 'Intervention')`.\n\n\ncase.names\nCase names. If not provided, names are taken from the scdf or left blank if the scdf does not contain case names.\n\n\nxlab\nThe label of the x-axis. The default is taken from the name of the measurement variable as provided by the scdf.\n\n\nylab\nThe labels of the y-axis. The default is taken from the name of the dependent variable as provided by the scdf.\n\n\nxinc\nAn integer. Increment of the x-axis. 1 : each mt value will be printed, 2 : every other value, 3 : every third values etc."
  },
  {
    "objectID": "app_supseded_functions.html#adding-lines",
    "href": "app_supseded_functions.html#adding-lines",
    "title": "Appendix D — Supseded functions",
    "section": "D.3 Adding lines",
    "text": "D.3 Adding lines\nExtra lines can be added to the plot using the lines argument. The lines argument takes several separate sub-arguments which have to be provided in a list. In its most simple form this list contains one element. lines = list(type = 'median') adds a line with the median of each phase to the plot. Additional arguments like col or lwd help to format these lines. For adding red thick median lines use the command lines = list(type = 'median', col = 'red', lwd = '2').\n\n\n\n\nTable D.2: Values of the lines argument\n\n\nArgument\nWhat it does ...\n\n\n\n\nmedian\nseparate lines for the medians of each phase\n\n\nmean\nseparate lines for the means of each phase. By default it is 10%-trimmed. Other trims can be set using a second parameter (e.g., `lines = list(type = 'mean', trim = 0.2)` draws a 20%-trimmed mean line).\n\n\ntrend\nSeparate lines for the trend of each phase.\n\n\ntrendA\nTrend line for phase A, extrapolated throughout the other phases\n\n\nmaxA\nLine at the level of the highest phase A score.\n\n\nminA\nLine at the level of the lowest phase A score.\n\n\nmedianA\nLine at the phase A median score.\n\n\nmeanA\nLine at the phase A 10%-trimmed mean score. Apply a different trim, by using the additional argument (e.g., `lines = list(type = 'meanA', trim = 0.2)`).\n\n\nmovingMean\nDraws a moving mean curve, with a specified lag: `lines = list(type = 'movingMean', lag = 2)`. Default is a lag 1 curve.\n\n\nmovingMedian\nDraws a moving median curve, with a specified lag: `lines = list(type = 'movingMedian', lag = 3).` Default is a lag 1 curve.\n\n\nloreg\nDraws a non-parametric local regression line. The proportion of data influencing each data point can be specified using `lines = list(type = 'loreg', f = 0.66)`. The default is 0.5.\n\n\nlty\nLine type. Examples are: 'solid','dashed', 'dotted'.\n\n\nlwd\nLine thickness, e.g., `lwd = 4`.\n\n\ncol\nLine colour, e.g., `col = 'red'`.\n\n\n\n\n\n\n\n\n\nplot(\n  exampleAB, \n  lines = list(\n    list(type = \"median\", col = \"red\", lwd = 0.5),\n    list(type = \"trend\", col = \"blue\", lty = \"dashed\", lwd = 2),\n    list(type = \"loreg\", f = 0.2, col = \"green\", lty = \"solid\", lwd = 1)\n  )\n)\n\n\n\n\nA plot with various visual aids"
  },
  {
    "objectID": "app_supseded_functions.html#mark-data-points",
    "href": "app_supseded_functions.html#mark-data-points",
    "title": "Appendix D — Supseded functions",
    "section": "D.4 Mark data points",
    "text": "D.4 Mark data points\nSpecific data points can be highlighted using the marks argument. A list defines the measurement times to be marked, the marking color and the size of the marking. marks = list(position = c(1,5,6)) marks the first, fifth, and sixth measurement time. If the scdf contains more than one data-set marking would be the same for all data sets in this example. In case you define a list Containing vectors, marking can be individually defined for each data set. Assume, for example, we have an scdf comprising three data sets, then marks = list(position = list(c(1,2), c(3,4), c(5,6))) will highlight measurement times one and two for the first data set, three and four for the second and five and six for the third. pch, col and cex define symbol, colour and size of the markings.\n\n# plot with marks in a red circles 2.5 times larger than the standard symbol \n# size. exampleAB is an example scdf included in the scan package\nmarks &lt;- list(\n  positions = list( c(8, 9), c(17, 19), c(7, 18) ), \n  col = 'red', cex = 2.5, pch = 1\n)\nplot(exampleAB, marks = marks, style = \"sienna\")\n\n\n\n\nA plot with highlighted data-points"
  },
  {
    "objectID": "app_supseded_functions.html#graphical-styles-of-a-plot",
    "href": "app_supseded_functions.html#graphical-styles-of-a-plot",
    "title": "Appendix D — Supseded functions",
    "section": "D.5 Graphical styles of a plot",
    "text": "D.5 Graphical styles of a plot\n\n\n\n\n\n\nThe style_plot function call\n\n\n\nstyle_plot(style = “default”, …)\n\n\nThe style argument of the plot function allows to specify a specific design of a plot. By default, the grid style is applied. scan includes some further predefined styles. default, yaxis, tiny, small, big, chart, ridge, annotate, grid, grid2, dark, nodot, and sienna. The name of a style is provided as a character string (e.g., style = \"grid\").\nSome styles only address specific elements (e.g., “small” or “tiny” just influence text and line sizes). These styles lend themselves to be combined with other styles. This could be achieved by providing several style names to the plot argument: style = c(\"grid\", \"annotate\", \"small\"). A style overwrites the settings of all previously included style.\nBeyond predefined styles, styles can be individually modified and created. New styles are provided as a list of several design parameters that are passed to the style argument of the plot function. Table D.3 shows all design parameter that could be defined.\nTo define a new style, first create a list containing a plain design. The style_plot function returns such a list with the default values for a plain design (e.g., mystyle &lt;- style_plot()). Single design parameters can now be set by assigning a specific value within the list. For example, newstyle$fill &lt;- \"grey90\" will set the fill parameter to \"grey90\". Alternatively, changes to the plain design can already by defined within the style_plot function. To set a light-blue background color and also an orange grid, create the style style_plot(fill.bg = \"lightblue\", grid = \"orange\"). If you do not want to start with the plain design but a different of the predefined styles, set the style argument. If, for example, you like to have the grid combined with the big style but want to change the color of the grid to orange type style_plot(style = c(\"grid\", \"big\"), col.grid = \"orange\"). plot(mydata, style = mystyle) will apply the new style in a plot. Please note that the new style is not passed in quotation marks.\n\n\n\n\nTable D.3: Arguments of the style plot function\n\n\nArgument\nWhat it does ...\n\n\n\n\nfill\nIf TRUE area under the line is filled.\n\n\ncol.fill\nSets the color of the area under the line.\n\n\ngrid\nIf TRUE a grid is included.\n\n\ncol.grid\nSets the color of the grid.\n\n\nlty.grid\nSets the line type of the grid.\n\n\nlwd.grid\nSets the line thikness of the grid.\n\n\nfill.bg\nIf not NA the backgorund of the plot is filled with the given color. If multiple colours are provided, the colours change with phases (e.g., `fill.bg = c('aliceblue', 'mistyrose1', 'honeydew')`\n\n\nannotations\nA list of parameters defining annotations to each data point. This adds the score of each MT to your plot. `'pos'` Position of the annotations: 1 = below, 2 = left, 3 = above, 4 = right. `'col'` Color of the annotations. `'cex'` Size of the annotations. `'round'` rounds the values to the specified decimal. `annotations = list(pos = 3, col = 'brown', round = 1)` adds scores rounded to one decimal above the data point in brown color to the plot.\n\n\ntext.ABlag\nBy default a vertical line separates phases A and B in the plot. Alternatively, you could print a character string between the two phases using this argument: `text.ABlag = 'Start'`.\n\n\nlwd\nWidth of the plot line. Default is `lwd = 2`.\n\n\npch\nPoint type. Default is `pch = 17` (triangles). Other options are for example: 16 (filled circles) or 'A' (uses the letter A).\n\n\ncol.lines\nThe color of the lines. If set to an empty string no lines are drawn.\n\n\ncol.dots\nThe color of the dots. If set to an empty string no dots are drawn.\n\n\nmai\nSets the margins of the plot.\n\n\n...\nFurther arguments passed to the plot command.\n\n\n\n\n\n\n\n\nThe width of the lines are set with the lwd argument, col is used to set the line colour and pch sets the symbol for a data point. The pch argument can take several values for defining the symbol in which data points are plotted.\n\n\n\n\n\nSome of the possible symbols and their pch values.\n\n\n\n\nHere is an example customizing a plot with several additional graphic parameters\n\nnewstyle &lt;- style_plot(\n  fill = \"grey95\",\n  fill.bg = c('aliceblue', 'mistyrose1', 'honeydew'),\n  names = list(col = \"brown\", cex = 2, font = 3, side = 3),\n  annotations = list(col = \"brown\"),\n  col.dots = \"blue\",\n  grid = \"lightblue\", \n  pch = 16)\n\nplot(exampleABAB, style = newstyle)\n\n\n\n\nA plot with a customized style."
  },
  {
    "objectID": "app_supseded_functions.html#sec-smoothing",
    "href": "app_supseded_functions.html#sec-smoothing",
    "title": "Appendix D — Supseded functions",
    "section": "D.6 Smoothing data",
    "text": "D.6 Smoothing data\n\n\n\n\n\n\nThe smooth_cases function call\n\n\n\nsmooth_cases(data, dvar, mvar, method = “mean”, intensity = NULL, FUN = NULL)\n\n\nThe smooth_cases function provides procedures to smooth single-case data and eliminate noise. A moving average function (mean- or median-based) replaces each data point by the average of the surrounding data points step-by-step. A lag defines the number of measurements before and after the calculation is based on. So a lag-1 will take the average of the proceeding and following value and lag-2 the average of the two proceeding and two following measurements. With a local regression function, each data point is regressed by its surrounding data points. Here, the proportion of measurements surrounding a value is usually defined. So an intensity of 0.2 will take the surrounding 20% of data as the basis for a regression.\nThe function returns am scdf with smoothed data points.\n\n## Use the three different smoothing functions and compare the results\nberta_mmd &lt;- smooth_cases(Huber2014$Berta)\nberta_mmn &lt;- smooth_cases(Huber2014$Berta, FUN = \"movingMean\")\nberta_lre &lt;- smooth_cases(Huber2014$Berta, FUN = \"localRegression\")\nnew_study &lt;- c(Huber2014$Berta, berta_mmd, berta_mmn, berta_lre)\nnames(new_study) &lt;- c(\"Original\", \"Moving Median\", \"Moving Mean\", \"Local Regression\")\nplot(new_study, style = \"grid2\")\n\n\n\n\nHere is the syntax for the upcoming scplot() function (see Chapter 5):\n\nscplot(new_study) %&gt;% add_ridge()"
  },
  {
    "objectID": "app_author.html",
    "href": "app_author.html",
    "title": "Appendix E — About the author",
    "section": "",
    "text": "Currently, I am a professor for research methods and diagnostics at the department of inclusive education at the University of Potsdam in Germany. I studied education sciences at the University of Cologne where I also did my PhD in psychology. Thereafter, I got a tenured position as a senior researcher at the department of special education (also University of Cologne). Later I did my habilitation on “Pedagogic and psychology in learning disabilities” at the Carl von Ossietzky University Oldenburg.\nMy current work focuses on:\n\nSingle-case research designs, analyzing single case data, and reporting single-case based results.\nSocial inclusion and social participation in classrooms.\nImplementation of Open Science and Data Science concepts into special education research.\n\nYou can find more information about me on my homepage:\nhttps://jazznbass.github.io/homepage/"
  }
]