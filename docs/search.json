[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Analyzing Single-Case Data with R and scan",
    "section": "",
    "text": "Preface\nVersion 25.09 (build 8)\nWelcome dear readers!\nI am delighted to see that you have discovered this book - it suggests that you interested in single-case research and have begun to explore the scan package. This book pursues two maingoals: First, it is a practical manual for using the scan package in R. Second, it offers a comprehensive overview of methods and approaches to analysze single-case data.\nWhile scan has undergone extensive development, this book is still a work in progress (approximately 62% complete) and I am continuously expanding its content. At present, there is no official release version - only this draft, which may contain errors.\nIf you have any suggestions for improving the book, wish to report a bug, or provide comments, feedback, and more, please visit the GitHub repository of this book and contribute here:\nhttps://github.com/jazznbass/scan-Book/discussions.\nThank you very much!\nJürgen\n15 September 2025",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#general-references",
    "href": "index.html#general-references",
    "title": "Analyzing Single-Case Data with R and scan",
    "section": "General references",
    "text": "General references\nThis book has been created using Quarto within the RStudio (RStudio Team, 2018) environment. The analyses have been conducted with the R package scan at version 0.67.0 (Wilbert & Lueke, 2025) and scplot at version 0.6.0 (Wilbert, 2025). R version 4.5.1 (2025-06-13) was used (R Core Team, 2025).\nNote: The cover has been designed by Tony Wilbert and Henry Ritter.\nThanx for that!\n\n\n\n\nR Core Team. (2025). R: A language and environment for statistical computing. Retrieved from https://www.R-project.org/\n\n\nRStudio Team. (2018). RStudio: Integrated development environment for r. Retrieved from http://www.rstudio.com/\n\n\nWilbert, J. (2025). Scplot: Plot function for single-case data frames. https://doi.org/10.32614/CRAN.package.scplot\n\n\nWilbert, J., & Lueke, T. (2025). Scan: Single-case data analyses for single and multiple baseline designs. Retrieved from https://github.com/jazznbass/scan/",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "ch_introduction.html",
    "href": "ch_introduction.html",
    "title": "1  Introduction",
    "section": "",
    "text": "1.1 A teaser\nSingle case research has become an important and widely accepted method for gaining insight into educational processes. In particular, the field of special education has embraced single case research as an appropriate method for evaluating the effectiveness of an intervention or the developmental processes underlying problems in the acquisition of academic skills. Single case studies are also popular with teachers and educators who are interested in evaluating the learning progress of their students. Despite their usefulness, standards for conducting single-case studies, analysing the data, and presenting the results are less well developed than for group-based research designs. Furthermore, while there is a wealth of software available to help analyse data, most of it is designed to analyse group-based datasets. Visualising single case data sets often means fiddling with spreadsheets, and analysis becomes a cumbersome endeavour. This book fills that gap. It has been written around a specialised software tool for managing, visualising and analysing single case data. This tool is an extension package for the R software (R Core Team, 2025) called scan, an acronym for single case analysis.\nBefore going into the details of how scan works, I would like to give you an example of what you can do with scan. It is meant as a teaser to get you motivated to tackle the steep learning curve associated with using R (but there is a land of milk and honey behind that curve!). So, do not worry if you do not understand every detail of this example, it will all be explained and obvious to you once you become familiar with scan.\nLet us set a fictional context. Suppose you are researching a method to improve the arithmetic skills of struggling fourth-grade students. You have developed an intervention programme called KUNO. In a pilot study, you want to get some evidence about the effectiveness of this new method, and you set up a multi-baseline single-case study with three students who participated in the KUNO programme over a period of ten weeks. during the course, you regularly measured each student’s numeracy skills 20 times using a reliable test. You also carried out a follow-up after eight weeks with five additional measures. I am now going to make up some data for this fictitious KUNO study, because it would be too difficult to carry out a real study and actually develop a real intervention method.\nWe use the scan package to code the data. Each case has 25 measurements. We have three phases: before the intervention (A), during the intervention (B), and a follow-up (C). Phases A and B are of different lengths. The cases are named and combined into a single object called strange_study.\nlibrary(scan)\nlibrary(scplot)\n\ncase1 &lt;- scdf(\n  c(A = 3, 2, 4, 6, 4, 3, \n    B = 6, 5, 4, 6, 7, 5, 6, 8, 6, 7, 8, 9, 7, 8, \n    C = 6, 6, 8, 5, 7), \n  name = \"Dustin\"\n)\ncase2 &lt;- scdf(\n  c(A = 0, 1, 3, 1, 4, 2, 1, \n    B = 2, 1, 4, 3, 5, 5, 7, 6, 3, 8, 6, 4, 7, \n    C = 6, 5, 6, 8, 6), \n  name = \"Mike\"\n)\ncase3 &lt;- scdf(\n  c(A = 7, 5, 6, 4, 4, 7, 5, 7, 4,\n    B = 8, 9, 11, 13, 12, 15, 16, 13, 17, 16, 18,\n    C = 17, 20, 22, 18, 20), \n  name = \"Will\"\n)\nstrange_study &lt;- c(case1, case2, case3)\nNow we visualize the cases:\nscplot(strange_study) %&gt;%\n  set_ylabel(\"Correct\") %&gt;%\n  set_xlabel (\"Days\") %&gt;%\n  add_statline(\"lowess\", color = \"red\") %&gt;%\n  set_phasenames(c(\"Baseline\", \"Intervention\", \"Follow-up\")) %&gt;%\n  set_yaxis(limits = c(0, 30)) %&gt;%\n  set_xaxis(increment = 2) %&gt;%\n  add_ridge(color = \"lightblue\") %&gt;%\n  set_theme(\"basic\")\nNow we need some descriptive statistics:\ndescribe(strange_study)\nTable\n\n\nDescriptive statistics\n\n\nParameter\nDustin\nMike\nWill\n\n\n\n\nDesign\nA-B-C\nA-B-C\nA-B-C\n\n\nn A\n6\n7\n9\n\n\nn B\n14\n13\n11\n\n\nn C\n5\n5\n5\n\n\nMissing A\n0\n0\n0\n\n\nMissing B\n0\n0\n0\n\n\nMissing C\n0\n0\n0\n\n\nm A\n3.67\n1.71\n5.44\n\n\nm B\n6.57\n4.69\n13.45\n\n\nm C\n6.4\n6.2\n19.4\n\n\nmd A\n3.5\n1.0\n5.0\n\n\nmd B\n6.5\n5.0\n13.0\n\n\nmd C\n6\n6\n20\n\n\nsd A\n1.37\n1.38\n1.33\n\n\nsd B\n1.40\n2.10\n3.27\n\n\nsd C\n1.14\n1.10\n1.95\n\n\nmad A\n0.74\n1.48\n1.48\n\n\nmad B\n1.48\n2.97\n4.45\n\n\nmad C\n1.48\n0.00\n2.97\n\n\nMin A\n2\n0\n4\n\n\nMin B\n4\n1\n8\n\n\nMin C\n5\n5\n17\n\n\nMax A\n6\n4\n7\n\n\nMax B\n9\n8\n18\n\n\nMax C\n8\n8\n22\n\n\nTrend A\n0.23\n0.21\n-0.08\n\n\nTrend B\n0.25\n0.36\n0.91\n\n\nTrend C\n0.1\n0.3\n0.4\n\n\n\nNote. n = Number of measurements; Missing = Number of missing values; M = Mean; Median = Median; SD = Standard deviation; MAD = Median average deviation; Min = Minimum; Max = Maximum; Trend = Slope of dependent variable regressed on measurement-time.\nSingle-case data are often analysed using overlap indices. Let us get an overview by comparing phases A and B:\noverlap(strange_study)\nTable\n\n\nOverlap indices. Comparing phase 1 against phase 2\n\n\nStatistic\nDustin\nMike\nWill\n\n\n\n\nPND\n50.00\n53.85\n100.00\n\n\nPEM\n100.00\n92.31\n100.00\n\n\nPET\n71.43\n61.54\n100.00\n\n\nNAP\n92.86\n87.91\n100.00\n\n\nNAP-R\n85.71\n75.82\n100.00\n\n\nPAND\n90.00\n80.00\n100.00\n\n\nIRD\n0.76\n0.56\n1.00\n\n\nTau-U (A + B - trend A)\n0.53\n0.45\n0.67\n\n\nTau-U (A + B - trend A + trend B)\n0.66\n0.56\n0.80\n\n\nBase Tau\n0.60\n0.55\n0.74\n\n\nDelta M\n2.90\n2.98\n8.01\n\n\nDelta Trend\n0.02\n0.14\n0.99\n\n\nSMD\n2.13\n2.16\n6.01\n\n\nHedges g\n2.00\n1.51\n2.96\n\n\n\nNote. PND = Percentage Non-Overlapping Data; PEM = Percentage Exceeding the Median; PET = Percentage Exceeding the Trend; NAP = Nonoverlap of all pairs; NAP-R = NAP rescaled; PAND = Percentage all nonoverlapping data; IRD = Improvement rate difference; Tau U (A + B - trend A) = Parker’s Tau-U; Tau U (A + B - trend A + trend B) = Parker’s Tau-U; Base Tau = Baseline corrected Tau; Delta M = Mean difference between phases; Delta Trend = Trend difference between phases; SMD = Standardized Mean Difference; Hedges g = Corrected SMD.\nHow do the changes hold up against the follow-up? Let us compare phases A and C:\noverlap(strange_study, phases = c(\"A\", \"C\"))\nTable\n\n\nOverlap indices. Comparing phase A against phase C\n\n\nStatistic\nDustin\nMike\nWill\n\n\n\n\nPND\n40.00\n100.00\n100.00\n\n\nPEM\n100.00\n100.00\n100.00\n\n\nPET\n0.00\n60.00\n100.00\n\n\nNAP\n93.33\n100.00\n100.00\n\n\nNAP-R\n86.67\n100.00\n100.00\n\n\nPAND\n81.82\n100.00\n100.00\n\n\nIRD\n0.82\n1.00\n1.00\n\n\nTau-U (A + B - trend A)\n0.48\n0.50\n0.61\n\n\nTau-U (A + B - trend A + trend B)\n0.46\n0.51\n0.61\n\n\nBase Tau\n0.67\n0.76\n0.74\n\n\nDelta M\n2.73\n4.49\n13.96\n\n\nDelta Trend\n−0.13\n0.09\n0.48\n\n\nSMD\n2.00\n3.25\n10.47\n\n\nHedges g\n1.97\n3.25\n8.34\n\n\n\nNote. PND = Percentage Non-Overlapping Data; PEM = Percentage Exceeding the Median; PET = Percentage Exceeding the Trend; NAP = Nonoverlap of all pairs; NAP-R = NAP rescaled; PAND = Percentage all nonoverlapping data; IRD = Improvement rate difference; Tau U (A + B - trend A) = Parker’s Tau-U; Tau U (A + B - trend A + trend B) = Parker’s Tau-U; Base Tau = Baseline corrected Tau; Delta M = Mean difference between phases; Delta Trend = Trend difference between phases; SMD = Standardized Mean Difference; Hedges g = Corrected SMD.\nFinally, we conduct regression analyses for each cases with a piecewise regression model:\nplm(strange_study$Dustin)\nplm(strange_study$Mike)\nplm(strange_study$Will)\nTable\n\n\nPiecewise-regression model predicting ‘values’\n\n\nParameter\nB\n\nCI(95%)\n\nSE\nt\np\ndelta R²\n\n\nLL\nUL\n\n\n\n\nIntercept\n3.10\n1.46\n4.73\n0.83\n3.72\n&lt;.001\n\n\n\nTrend (mt)\n0.23\n-0.31\n0.77\n0.28\n0.83\n.42\n0.01\n\n\nLevel phase B (phaseB)\n0.50\n-1.89\n2.90\n1.22\n0.41\n.68\n0.00\n\n\nLevel phase C (phaseC)\n-1.47\n-11.11\n8.17\n4.92\n-0.30\n.77\n0.00\n\n\nSlope phase B (interB)\n0.02\n-0.54\n0.58\n0.29\n0.06\n.95\n0.00\n\n\nSlope phase C (interC)\n-0.13\n-1.02\n0.77\n0.46\n-0.28\n.78\n0.00\n\n\n\nNote. F(5, 19) = 7.88; p = 0.000; R² = 0.675; Adjusted R² = 0.589; AIC = 85; LL = lower limit; UL = upper limit; Slope estimation method = W; Contrasts for the level and slope effects are coded with the first phase as the reference.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTable\n\n\nPiecewise-regression model predicting ‘values’\n\n\nParameter\nB\n\nCI(95%)\n\nSE\nt\np\ndelta R²\n\n\nLL\nUL\n\n\n\n\nIntercept\n1.07\n-0.95\n3.09\n1.03\n1.04\n.31\n\n\n\nTrend (mt)\n0.21\n-0.35\n0.78\n0.29\n0.75\n.46\n0.01\n\n\nLevel phase B (phaseB)\n-0.02\n-2.97\n2.93\n1.51\n-0.01\n.99\n0.00\n\n\nLevel phase C (phaseC)\n0.24\n-9.63\n10.12\n5.04\n0.05\n.96\n0.00\n\n\nSlope phase B (interB)\n0.14\n-0.46\n0.75\n0.31\n0.46\n.65\n0.00\n\n\nSlope phase C (interC)\n0.09\n-1.01\n1.18\n0.56\n0.15\n.88\n0.00\n\n\n\nNote. F(5, 19) = 8.00; p = 0.000; R² = 0.678; Adjusted R² = 0.593; AIC = 99; LL = lower limit; UL = upper limit; Slope estimation method = W; Contrasts for the level and slope effects are coded with the first phase as the reference.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTable\n\n\nPiecewise-regression model predicting ‘values’\n\n\nParameter\nB\n\nCI(95%)\n\nSE\nt\np\ndelta R²\n\n\nLL\nUL\n\n\n\n\nIntercept\n5.78\n3.96\n7.59\n0.93\n6.23\n&lt;.001\n\n\n\nTrend (mt)\n-0.08\n-0.46\n0.30\n0.19\n-0.43\n.67\n0.00\n\n\nLevel phase B (phaseB)\n3.88\n1.16\n6.60\n1.39\n2.80\n&lt;.05\n0.02\n\n\nLevel phase C (phaseC)\n14.49\n7.89\n21.08\n3.37\n4.31\n&lt;.001\n0.05\n\n\nSlope phase B (interB)\n0.99\n0.52\n1.47\n0.24\n4.10\n&lt;.001\n0.05\n\n\nSlope phase C (interC)\n0.48\n-0.53\n1.49\n0.52\n0.94\n.36\n0.00\n\n\n\nNote. F(5, 19) = 68.16; p = 0.000; R² = 0.947; Adjusted R² = 0.933; AIC = 99; LL = lower limit; UL = upper limit; Slope estimation method = W; Contrasts for the level and slope effects are coded with the first phase as the reference.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "ch_introduction.html#a-teaser",
    "href": "ch_introduction.html#a-teaser",
    "title": "1  Introduction",
    "section": "",
    "text": "R Core Team. (2025). R: A language and environment for statistical computing. Retrieved from https://www.R-project.org/",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "ch_scan_package.html",
    "href": "ch_scan_package.html",
    "title": "2  The scan package",
    "section": "",
    "text": "2.1 Installing the scan package\nYou can use the install.packages function to install scan.\ninstall.packages(\"scan\") will install the stable version.\nThe current stable release is version 0.67.0. Please refer to the Software Reference section to see which version of scan was used to create this book, and make sure you have this or a newer version installed.\nR contains many packages, and it would slow things down considerably if all packages were loaded into memory at the beginning of each R session. Therefore, after installing scan, you need to enable it at the beginning of each session in which you use R. Normally, a session starts when you start the R program and ends when you quit it.\nTo activate a package, you need the library function. In this case, library(scan). You should get something like\nscan 0.67.0 (2025-09-11)\nindicating that everything went smoothly and scan is ready for work.\nFor creating single-case plots, please install the add-on package scplot with install.packages(\"scplot\").",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>The scan package</span>"
    ]
  },
  {
    "objectID": "ch_scan_package.html#development-version-of-scan",
    "href": "ch_scan_package.html#development-version-of-scan",
    "title": "2  The scan package",
    "section": "2.2 Development version of scan",
    "text": "2.2 Development version of scan\nAlternatively, you can compile the development version of scan yourself. This may be necessary if the stable version has some bugs or missing features that have been fixed.\nYou may need some computer knowledge to get the development version running. It is hosted on gitHub at &lt;https://github.com/jazznbass/scan&gt;.\nFor installation, you can apply the install_github function from the devtools package (make sure you have installed the devtools package before):\ndevtools::install_github(\"jazznbass/scan\", dependencies = TRUE)\nIf you are using a Windows operating system, you will probably need to install Rtools first. Rtools contains additional programs (e.g. compilers) needed to compile R source packages.\nYou can find Rtools here: &lt;https://cran.r-project.org/bin/windows/Rtools/&gt;",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>The scan package</span>"
    ]
  },
  {
    "objectID": "ch_scan_package.html#reporting-issues-with-scan-and-suggesting-enhancements",
    "href": "ch_scan_package.html#reporting-issues-with-scan-and-suggesting-enhancements",
    "title": "2  The scan package",
    "section": "2.3 Reporting issues with scan and suggesting enhancements",
    "text": "2.3 Reporting issues with scan and suggesting enhancements\nThe scan gitHub repository at &lt;https://github.com/jazznbass/scan&gt; is the ideal place to report bugs, problems, or ideas for enhancing scan. Please use the issue tool (direct link: &lt;https://github.com/jazznbass/scan/issues&gt;).\nWe are very thankful for any feedback, corrections, or whatever helps to improve scan!",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>The scan package</span>"
    ]
  },
  {
    "objectID": "ch_scan_package.html#functions-overview",
    "href": "ch_scan_package.html#functions-overview",
    "title": "2  The scan package",
    "section": "2.4 Functions overview",
    "text": "2.4 Functions overview\nThe functions of the scan package can be divided into the following categories:\nManage data, analyze, manipulate, simulate, and depict.\nThe following tables give an overview of the central functions. Furthermore, you can see the current life cycle stage of a function. The life cycle stage categorization is based on the tidyverse package and described in detail here https://lifecycle.r-lib.org/articles/stages.html.\n\n2.4.1 Management\n\n\n\nTable 2.1: Functions for data management\n\n\n\n\n\n\n\n\n\n\n\nFunction\nWhat it does …\nLifecycle stage\nChapter\n\n\n\n\nscdf\nCreates a single-case data-frame\nStable\nSection 3.2\n\n\nselect_cases\nSelects specific cases of an scdf\nStable\nSection 4.1\n\n\nselect_phases\nSelects and/or recombines phases\nStable\nSection 9.1.1\n\n\nsubset\nSelects specific measurements or variables of an scdf\nstable\nSection 4.2\n\n\nread_scdf\nLoads external data into an scdf\nStable\nSection 3.4.1\n\n\nwrite_scdf\nWrites scdf into an external file\nStable\nSection 3.4.3\n\n\nconvert\nConverts an scdf object into R syntax\nStable\nSection 3.5\n\n\nset_var\n(Re)sets dependent, measurement, and phase variable of an scdf\nStable\nChapter 6\n\n\nadd_l2\nAdds level-two data to an scdf\nStable\nSection 11.0.4\n\n\nas_scdf\nTransforms a data.frame into an scdf\nStable\n-\n\n\nas.data.frame/as.data.frame.scdf\nTransforms an scdf into a data frame\nStable\n-\n\n\n\n\n\n\n\n\n2.4.2 Depiction\n\n\n\n\nTable 2.2: Functions for data depiction/visualisation\n\n\n\n\n\n\n\n\n\n\n\n\nFunction\nWhat it does …\nLifecycle stage\nChapter\n\n\n\n\nscplot\nAdd-on package scplot. Creates advanced ggplot2 plots\nStable\nChapter 5\n\n\nprint/print.scdf\nPrints an scdf\nStable\nSection 3.6\n\n\nsummary/summary.scdf\nSummaizes an scdf\nStable\nSection 3.6.1\n\n\nexport\nCreates html or latex tables from the output of various scan functions\nStable\nChapter 16\n\n\nplot/plot.scdf\nCreates plots of single cases\nSuperseded\n-\n\n\nstyle_plot\nDefines single-case plot graphical styles\nSuperseded\n-\n\n\nplot_rand\nCreate a distribution plot from a randomization test obejct\nSuperseded\n-\n\n\n\n\n\n\n\n\n\n\n2.4.3 Analysis\n\n\n\n\nTable 2.3: Functions for data analysis\n\n\n\n\n\n\n\n\n\n\n\n\nFunction\nWhat it does …\nLifecycle stage\nChapter\n\n\n\n\nautocorr\nAutocorrelations for each phase of each case\nStable\nSection 8.2\n\n\ndescribe\nDescriptive statistics for each phase of each case\nStable\nSection 8.1\n\n\nird\nImprovement rate difference\nStable\nSection 9.7\n\n\noverlap\nAn overview of overlap indeces for each case\nStable\nChapter 9\n\n\nsmd\nVarious standardized mean differences between phase A and B\nStable\nSection 9.10\n\n\nrci\nReliable change index\nStable\nSection 9.12\n\n\nrand_test\nRandomization test\nStable\nChapter 14\n\n\ntrend\nTrend analyses for each case\nStable\nSection 8.3\n\n\nplm\nPiecewise linear regression model\nStable\nChapter 10\n\n\nmplm\nMultivariate piecewise linear regression model\nExperimental\nChapter 12\n\n\nhplm\nHierarchical piecewise linear regression model\nStable\nChapter 11\n\n\nbplm\nBayesian (multilevel) piecewise linear regression model\nExperimental\nChapter 13\n\n\nbetween_smd\nBetween case standardized mean difference\nStable\nSection 9.11\n\n\nanova\nModelcomparison via likelihood ratio test for plm and hplm\nStable\nSection 10.7\n\n\ntau_u\nTau-U for each case and all cases\nStable\nSection 9.8\n\n\ncorrected_tau\nBaseline corrected tau\nStable\nSection 9.9\n\n\nnap\nNon-overlap of all pairs for each case\nStable\nSection 9.6\n\n\npnd\nPercentage of non overlapping data for each case\nStable\nSection 9.2\n\n\npand\nPercentage of all non overlapping data for all cases\nStable\nSection 9.5\n\n\npem\nPercantage exceeding the mean for each case\nStable\nSection 9.3\n\n\npet\nPercentage exceeding the trend for each case\nStable\nSection 9.4\n\n\ncdc\nConservative dual-criterion test\nStable\n?sec-cdc\n\n\noutlier\nDetect outliers for all cases\nStable\nSection 7.2\n\n\n\n\n\n\n\n\n\n\n2.4.4 Manipulation\n\n\n\n\nTable 2.4: Functions for data manipulation\n\n\n\n\n\n\n\n\n\n\n\n\nFunction\nWhat it does …\nLifecycle stage\nChapter\n\n\n\n\ntransform\nCalculate new and change existing variables\nStable\nSection 4.3\n\n\nall_cases\nHelper function for ‘transform()’ that executes an expression across all cases of an scdf\nStable\nSection 4.3.1\n\n\nacross_cases\nHelper function for ‘transform()’ that calculates a variable for all cases of an scdf\nStable\n-\n\n\nmoving_mean\nHelper function for ‘transform()’ to smooth with moving means\nStable\nSection 4.3.2\n\n\nmoving_media\nHelper function for ‘transform()’ to smooth with moving medians\nStable\nSection 4.3.2\n\n\nlocal_regression\nHelper function for ‘transform()’ to smooth with local regressions\nStable\nSection 4.3.2\n\n\nfill_missing\nInterpolate missign values or missing measurement times\nStable\nSection 7.1\n\n\nrescale\nStandardizes variables across cases\nStable\nSection 10.3\n\n\nranks\nCovert data into ranked data across all cases\nSuperseded\n-\n\n\nsmooth_cases\nSmoothes time series data\nSuperseded\n-\n\n\ntruncate_phase\nDeletes measurements of phases\nSuperseded\n-\n\n\nstandardize\nStandardizes or centers variables across cases\nSuperseded\n-\n\n\n\n\n\n\n\n\n\n\n2.4.5 Simulation\n\n\n\n\nTable 2.5: Functions for data simulation\n\n\n\n\n\n\n\n\n\n\n\n\nFunction\nWhat it does …\nLifecycle stage\nChapter\n\n\n\n\ndesign\nDefines a design of one or multiple single-cases\nStable\n-\n\n\npower_test\nCalculates power and alpha error of a specific analyzes for a specific single-case design\nStable\nSection 16.16\n\n\nrandom_scdf\nCreats random single-case studies from a single-case design\nStable\n-\n\n\nestimate_design\nExtraxt a deisgn template from an existing scdf\nExperimental\n-\n\n\nmcscan\nAdd-on package mcscan. Create Monte-Carlo designs and analyses with scan\n(Upcoming not yet functioning)\n-",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>The scan package</span>"
    ]
  },
  {
    "objectID": "ch_create_display_and_store_scdfs.html",
    "href": "ch_create_display_and_store_scdfs.html",
    "title": "3  Create, display, and store single-case data",
    "section": "",
    "text": "3.1 A single-case data frame\nScan provides its own data-class for encoding single-case data: the single-case data frame (short scdf). An scdf is an object that contains one or multiple single-case data sets and is optimized for managing and displaying these data. Think of an scdf as a file including a separate datasheet for each single case. Each datasheet is made up of at least three variables: The measured values, the phase identifier for each measured value, and the measurement time (mt) of each measure. Optionally, scdfs could include further variables for each single-case (e.g., control variables), and also a name for each case.\nSeveral functions are available for creating, transforming, merging, and importing/exporting scdfs.",
    "crumbs": [
      "The single-case data frame",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Create, display, and store single-case data</span>"
    ]
  },
  {
    "objectID": "ch_create_display_and_store_scdfs.html#a-single-case-data-frame",
    "href": "ch_create_display_and_store_scdfs.html#a-single-case-data-frame",
    "title": "3  Create, display, and store single-case data",
    "section": "",
    "text": "Note\n\n\n\nTechnically, an scdf object is a list containing data frames. It is of the class c(\"scdf\",\"list\"). Additionally, an scdf entails an attribute scdf with a list with further attributes. var.values, var.phase, and var.mt contain the names of the values, phase, and the measurement time variable. By default, these names are set to values, phase, and mt.",
    "crumbs": [
      "The single-case data frame",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Create, display, and store single-case data</span>"
    ]
  },
  {
    "objectID": "ch_create_display_and_store_scdfs.html#sec-scdf",
    "href": "ch_create_display_and_store_scdfs.html#sec-scdf",
    "title": "3  Create, display, and store single-case data",
    "section": "3.2 Create single-case data frames",
    "text": "3.2 Create single-case data frames\n\n\n\n\n\n\nThe scdf function call:\n\n\n\nscdf(\n  …,\n  B_start = NULL,\n  phase_starts = NULL,\n  phase_design = NULL,\n  name = NULL,\n  dvar = “values”,\n  pvar = “phase”,\n  mvar = “mt”\n)\n\n\nThe scdf() function is the basic tool for creating a single-case data frame. Basically, you have to provide the measurement values and the phase structure and an scdf object is build. There are three different ways of defining the phase structure. First, defining the beginning of each phase with the phase_starts argument, second, defining a design with the phase_design argument and third, setting parameters in a named vector of the dependent variable.\n\n### Three ways to code the same scdf\nscdf(values = c(2,2,4,5,8,7,6,9,8,7), phase_starts = c(A = 1, B = 5))\nscdf(values = c(2,2,4,5,8,7,6,9,8,7), phase_design = c(A = 4, B = 6))\nscdf(values = c(A = 2,2,4,5, B = 8,7,6,9,8,7))\n\nThe phase_starts argument is a named vector with the starts of each phase. The number assigned to phase_starts indicate the measurement-time as defined in the mt argument. That is, assume a vector for the measurement times mt = c(1,3,7,10,15,17,18,20) and phase_starts = c(A = 1, B = 15) then the first measurement of the B-phase will start with the fifth measurement at which mt = 15.\nThe phase_design argument is a named vector with the name and length of each phase. Here, the values indicate the numbers of measurements (including explicit missing values, the mt argument is not taken into account).\nWhen the vector of the dependent variable includes named values, a phase_design structure is created automatically. Each named value sets the beginning of a new phase. For example c(A = 3,2,4, B = 5,4,3, C = 6,7,6,5) will create an ABC-phase design with 3, 3, and 4 values per phase.\nThe phase names can be set arbitrary, although I recommend to use capital letters (A, B, C, …) for each phase followed by, when indicated, a number if the phases repeat (A1, B1, A2, B2, …). Although it is possible to give the same name to more than one phase (A, B, A, B) this might lead to some confusion and errors when coding analyzes with scan.\n\n\n\n\n\n\nNote\n\n\n\nA deprecated argument is B_start which is only applicable when the single-case consists of a single A-phase followed by a B-phase. It is a remnant from the time when scan could only handle one-case designs with two phases. The number assigned to B_start indicates the measurement-time as defined in the mt argument.\n\n\nIf no measurement times are specified, they are automatically created as a series 1, 2, 3, …, N, where N is the number of measurements. in some circumstances it might be useful to define individual measurement times for each measurement. For example, if you want to include the days since the beginning of the study as time intervals between measurements are widely varying you might get more valid results this way when analyzing the data in a regression approach.\n\n# example of a more complex design \nscdf(\n  values = c(2,2,4,5, 8,7,6,9,8,7, 12,11,13), \n  mt = c(1,2,3,6, 8,9,11,12,16,18, 27,28,29),\n  phase_design = c(A = 4, B = 6, C = 3)\n)\n\n#A single-case data frame with one case\n\n [case #1]: values mt phase\n                 2  1     A\n                 2  2     A\n                 4  3     A\n                 5  6     A\n                 8  8     B\n                 7  9     B\n                 6 11     B\n                 9 12     B\n                 8 16     B\n                 7 18     B\n                12 27     C\n                11 28     C\n                13 29     C\n\n\nMissing values could be coded using NA (not available).\n\nscdf(values = c(A = 2,2,NA,5, B = 8,7,6,9,NA,7))\n\nMore variables are implemented by adding new variable names with a vector containing the values. Please be aware that a new variable must never have the same name as one of the arguments of the function (i.e. phase_starts, phase_design, name, dvar, pvar, mvar).\n\nscdf(\n  values = c(A = 2,2,3,5, B = 8,7,6,9,7,7), \n  teacher = c(0,0,1,1,0,1,1,1,0,1), \n  hour = c(2,3,4,3,3,1,6,5,2,2)\n)\n\n#A single-case data frame with one case\n\n [case #1]: values teacher hour mt phase\n                 2       0    2  1     A\n                 2       0    3  2     A\n                 3       1    4  3     A\n                 5       1    3  4     A\n                 8       0    3  5     B\n                 7       1    1  6     B\n                 6       1    6  7     B\n                 9       1    5  8     B\n                 7       0    2  9     B\n                 7       1    2 10     B\n\n\nTable 3.1 shows a complete list of arguments that could be passed to the function.\n\n\n\n\nTable 3.1: Arguments of the scdf function\n\n\n\n\n\n\nArgument\nWhat it does ...\n\n\n\n\nvalues\nThe default vector with values for the dependent variable. It can be changed with the dvar argument.\n\n\nphase\nUsually, this variable is not defined manually and will be created by the function. It is the default vector with values for the phase variable. It can be changed with the pvar argument.\n\n\nmt\nThe default vector with values for the measurement-time variable. It can be changed with the mvar argument.\n\n\nphase_design\nA named vector defining the length and label of each phase.\n\n\nphase_starts\nA named vector defining the startpoint of each phase with respect to the measurement-time.\n\n\n(deprecated) B_start\nThe first measurement of phase B (simple coding if design is strictly AB).\n\n\nname\nA name for the case.\n\n\ndvar\nThe name of the dependent variable. By default this is 'values'.\n\n\npvar\nThe name of the variable containing the phase information. By default this is 'phase'.\n\n\nmvar\nThe name of the variable with the measurement-time. The default is 'mt'.\n\n\n...\nAny number of variables with a vector asigned to them.\n\n\n\n\n\n\n\n\nIf you want to create a dataset comprising several single cases, the easiest way is to first create an scdf for each case and then merge them into a new scdf using the c command:\n\ncase1 &lt;- scdf(\n  values = c(A = 5, 7, 10, 5, 12, B = 7, 10, 18, 15, 14, 19), \n  name = \"Charlotte\"\n)\ncase2 &lt;- scdf(\n  values = c(A = 3, 4, 3, 5, B = 7, 4, 7, 9, 8, 10, 12), \n  name = \"Theresa\"\n)\ncase3 &lt;- scdf(\n  values = c(A = 9, 8, 8, 7, 5, 7, B = 6, 14, 15, 12, 16), \n  name = \"Antonia\"\n)\nmbd &lt;- c(case1, case2, case3)\n\nIf you want to use other than the default variable names (“values”, “phase” and “mt”), you can define them with the arguments dvar (for the dependent variable), pvar (the variable specifying the phase) and mvar (the measurement time variable).\n\n# Example: Using a different name for the dependent variable\ncase &lt;- scdf(\n  score = c(A = 5, 7, 10, 5, 12, B = 7, 10, 18, 15, 14, 19), \n  dvar = \"score\"\n)\n\n# Example: Using new names for the dependent and the phase variables\ncase &lt;- scdf(\n  score = c(A = 3, 4, 3, 5, B = 7, 4, 7, 9, 8, 10, 12), \n  dvar = \"score\", pvar = \"section\"\n)\n\n# Example: Using new names for dependent, phase, and measurement-time variables\ncase &lt;- scdf(\n  score = c(A = 9, 8, 8, 7, 5, 7, B = 6, 14, 15, 12, 16), \n  name = \"Antonia\", dvar = \"score\", pvar = \"section\", mvar = \"day\"\n)\n\nsummary(case)\n\n#A single-case data frame with one case\n\n         Measurements Design\n Antonia           11    A-B\n\nVariable names:\nscore &lt;dependent variable&gt;\nsection &lt;phase variable&gt;\nday &lt;measurement-time variable&gt;",
    "crumbs": [
      "The single-case data frame",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Create, display, and store single-case data</span>"
    ]
  },
  {
    "objectID": "ch_create_display_and_store_scdfs.html#sec-save-scdf",
    "href": "ch_create_display_and_store_scdfs.html#sec-save-scdf",
    "title": "3  Create, display, and store single-case data",
    "section": "3.3 Save and read single-case data frames",
    "text": "3.3 Save and read single-case data frames\nNormally, it is not necessary to save an scdf in a separate file on your computer. In most cases, you can keep the coding of the scdf as described above and run it again each time you work with your data. However, for large files, it is sometimes more convenient to save the data separately in a file for later use.\nThe easiest way is to use the R base functions saveRDS and readRDS for this purpose. saveRDS takes at least two arguments: The first is the object you want to save, and the second is a filename for the resulting file. If you have an scdf named study1, you can use saveRDS(study1, \"study1.rds\") to save the scdf to your drive. You can read this file with study1 &lt;- readRDS(\"study1.rds\"). With getwd() you get the path to the current active folder you are saving and reading data from.",
    "crumbs": [
      "The single-case data frame",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Create, display, and store single-case data</span>"
    ]
  },
  {
    "objectID": "ch_create_display_and_store_scdfs.html#import-and-export-single-case-data-frames",
    "href": "ch_create_display_and_store_scdfs.html#import-and-export-single-case-data-frames",
    "title": "3  Create, display, and store single-case data",
    "section": "3.4 Import and export single-case data frames",
    "text": "3.4 Import and export single-case data frames\n\n3.4.1 Import data\n\n\n\n\n\n\nThe read_scdf function call:\n\n\n\nread_scdf(\n  file,\n  cvar = “case”,\n  pvar = “phase”,\n  dvar = “values”,\n  mvar = “mt”,\n  sort_cases = FALSE,\n  phase_names = NULL,\n  type = NA,\n  na = c(““,”NA”),\n  …\n)\n\n\nWhen you are working with other programs besides R you need to export and import the scdf into a common file format. read_scdf imports a comma-separated-variable (csv) file and converts it into an scdf object. By default, the csv-file has to contain the columns case, phase, and values. Optionally, a further column named mt could be provided. The csv file should be build up like this:\n\n\n\n\n\n\nFigure 3.1: How to format a single-case file in a spreadsheet program for importing into scan\n\n\n\nIn case your variables names differ from the standard (i.e. “case”, “values”, “phase”, and “mt” ), you could set additional arguments to fit your file. read_scdf(\"example.csv\", cvar = \"name\", dvar = \"wellbeing\", pvar = \"intervention\", mvar = \"time\") for example will set the variables attributes of the resulting scdf. Cases will be split by the variable \"name\", \"wellbeing\" is set as the dependent variable (default is values), phase information are in the variable \"intervention\", and measurement times in the variable \"time\". You could also reassign the phase names within the phase variable by setting the argument phase_names. Assume for example your file contains the values 0 and 1 to identify the two phases I recommend to set them to “A” and “B” with read_scdf(\"example.csv\", phase_names = c(\"A\", \"B\")).\nFor some reasons, computer systems with a German (and some other) language setups export csv-files by default with a comma as a decimal point and a semicolon as a separator between values. In these cases you have to set two extra arguments to import the data:\nread_scdf(\"example.csv\", dec = \",\", sep = \";\")\n\n\n3.4.2 Other data formats\nread_scdf also allows for directly importing Microsoft Excel .xlsx or .xls files. You need to have the library readxl installed in your R setup for this to work. Excel files will be automatically detected by the filename extension xlsor xlsx or by explicitly setting the type argument (e.g. type = \"xlsx\").\n\ndat &lt;- read_scdf(\n  \"example2.xlsx\", cvar = \"name\", pvar = \"intervention\", \n  dvar = \"wellbeing\", mvar = \"time\", phase_names = c(\"A\",\"B\")\n)\n\nImported 20 cases\n\nsummary(dat)\n\n#A single-case data frame with 20 cases\n\n          Measurements Design\n Charles            20    A-B\n Kolten             20    A-B\n Annika             20    A-B\n Kaysen             20    A-B\n Urijah             20    A-B\n Leila              20    A-B\n Leia               20    A-B\n Aleigha            20    A-B\n Greta              20    A-B\n Alijah             20    A-B\n... [skipped 10 cases]\n\nVariable names:\nwellbeing &lt;dependent variable&gt;\nintervention &lt;phase variable&gt;\ntime &lt;measurement-time variable&gt;\nage\ngender\ngym\n\n\nBasically, you can import data from any file format with the help of additional R packages and the as_scdf() function. Here are two examples:\n\n# Open document example. You need to have the readODS package installed.\ndf &lt;- readODS::read_ods(\"filename.ods\")\nscdf &lt;- as_scdf(df)\n\n# SPSS example. You need to have the haven package installed.\ndf &lt;- haven::read_sav(\"filename.sav\")\nscdf &lt;- as_scdf(df)\n\n\n\n3.4.3 Export data\n\n\n\n\n\n\nThe write_scdf function call:\n\n\n\nwrite_scdf(data, filename = NULL, sep = “,”, dec = “.”, …)\n\n\nwrite_scdf() exports an scdf object as a comma-separated-variables file (csv) which can be imported into any other software for data analyses (MS OFFICE, Libre Office etc.). The scdf object is converted into a single data frame with a case variable identifying the rows for each subject. The first argument of the command identifies the scdf to be exported and the second argument (file) the name of the resulting csv-file. If no file argument is provided, a dialog box is opened to choose a file interactively. By default, write_scdf exports into a standard csv-format with a dot as the decimal point and a comma for separating variables. If your system expects a comma instead of a point for decimal numbers you may use the dec and the sep arguments. For example, write_scdf(example, file = \"example.csv\", dec = \",\", sep = \";\") exports a csv variation usually used for example in Germany.\n\n\n3.4.4 Other data formats\nThe R system has many add on packages that allow to write data to almost any file format available. If you like to export an scdf in those formats, you firstly need to convert the scdf into a standard R data-frame with the as.data.frame() function. Now you can export the resulting data-frame applying the respective function from another package.\n\ndf &lt;- as.data.frame(exampleABC)\n\n# Open document example. You need to have the readODS package installed.\nreadODS::write_ods(df, \"filename.ods\")\n\n# Excel example. You need to have the openxlsx package installed.\nopenxlsx::write.xlsx(df, \"filename.xlsx\")\n\n# SPSS example. You need to have the haven package installed.\nhaven::write_sav(df, \"filename.sav\")",
    "crumbs": [
      "The single-case data frame",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Create, display, and store single-case data</span>"
    ]
  },
  {
    "objectID": "ch_create_display_and_store_scdfs.html#sec-convert",
    "href": "ch_create_display_and_store_scdfs.html#sec-convert",
    "title": "3  Create, display, and store single-case data",
    "section": "3.5 Convert an scdf object back to scan syntax",
    "text": "3.5 Convert an scdf object back to scan syntax\n\n\n\n\n\n\nThe convert function call:\n\n\n\nconvert(scdf, file = ““, study_name =”study”, case_name = “case”, inline = FALSE, indent = 2, silent = FALSE)\n\n\nYou can also reconvert an scdf object back to “raw” scan syntax. This is a convenient way when you imported data from an Excel or csv file and want to keep everything clean and transparent within your R syntax files.\nHere is an example:\n\nconvert(exampleABC)\n\ncase1 &lt;- scdf(\n  values = c(\n    58, 56, 60, 63, 51, 45, 44, 59, 45, 39, 83, 65, 70, 83, 70, 85, 47, 66,\n    77, 75, 51, 87, 80, 68, 70, 56, 52, 70, 83, 63\n  ),\n  phase_design = c(A = 10, B = 10, C = 10),\n  name = \"Marie\"\n)\n\ncase2 &lt;- scdf(\n  values = c(\n    47, 41, 47, 52, 54, 65, 55, 37, 51, 60, 60, 65, 55, 46, 49, 54, 77, 73,\n    97, 64, 84, 71, 66, 74, 78, 68, 52, 76, 63, 54\n  ),\n  phase_design = c(A = 15, B = 8, C = 7),\n  name = \"Rosalind\"\n)\n\ncase3 &lt;- scdf(\n  values = c(\n    50, 45, 63, 53, 66, 57, 35, 45, 74, 63, 47, 45, 47, 36, 51, 55, 35, 66,\n    59, 55, 73, 60, 85, 62, 79, 69, 87, 76, 90, 48\n  ),\n  phase_design = c(A = 20, B = 7, C = 3),\n  name = \"Lise\"\n)\n\nstudy &lt;- c(\n  case1, case2, case3\n) \n\n\nSet inline = TRUE if you prefer the phase definition in a named vector:\n\nconvert(exampleABC$Marie, inline = TRUE)\n\nstudy1 &lt;- scdf(\n  values = c(\n    A = 58, 56, 60, 63, 51, 45, 44, 59, 45, 39,\n    B = 83, 65, 70, 83, 70, 85, 47, 66, 77, 75,\n    C = 51, 87, 80, 68, 70, 56, 52, 70, 83, 63\n  ),\n  name = \"Marie\"\n)\n\n \n\n\nNow you can copy and past the output into your R file or you set the file argument to save the output into an R file convert(exampleABC, file = \"scdf.R\").",
    "crumbs": [
      "The single-case data frame",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Create, display, and store single-case data</span>"
    ]
  },
  {
    "objectID": "ch_create_display_and_store_scdfs.html#sec-print",
    "href": "ch_create_display_and_store_scdfs.html#sec-print",
    "title": "3  Create, display, and store single-case data",
    "section": "3.6 Display single-case data frames",
    "text": "3.6 Display single-case data frames\n\n\n\n\n\n\nThe print scdf function call:\n\n\n\nprint(\n  x,\n  cases = getOption(“scan.print.cases”),\n  rows = getOption(“scan.print.rows”),\n  cols = getOption(“scan.print.cols”),\n  long = getOption(“scan.print.long”),\n  digits = getOption(“scan.print.digits”),\n  …\n)\n\n\nscdf are displayed by just typing the name of the object.\n\n#Beretvas2008 is an example scdf included in scan\nBeretvas2008\n\n#A single-case data frame with one case\n\n [case #1]: values mt phase\n               0.7  1     A\n               1.6  2     A\n               1.4  3     A\n               1.6  4     A\n               1.9  5     A\n               1.2  6     A\n               1.3  7     A\n               1.6  8     A\n                10  9     B\n              10.8 10     B\n              11.9 11     B\n                11 12     B\n                13 13     B\n              12.7 14     B\n                14 15     B\n\n\nThe print command allows you to specify the output. Some possible arguments are cases (the number of cases to display; three by default), rows (the maximum number of rows to display; fifteen by default), and digits (the number of digits). cases = 'all' and rows = 'all' prints all cases and rows.\n\n# Huber2014 is an example scdf included in scan\nprint(Huber2014, cases = 2, rows = 10)\n\n#A single-case data frame with four cases\n\n Adam: compliance mt phase ｜ Berta: compliance mt phase ｜\n               25  1     A ｜                25  1     A ｜\n             20.8  2     A ｜              20.8  2     A ｜\n             39.6  3     A ｜              39.6  3     A ｜\n               75  4     A ｜                75  4     A ｜\n               45  5     A ｜                45  5     A ｜\n             39.6  6     A ｜              14.6  6     A ｜\n             54.2  7     A ｜              45.8  7     A ｜\n               50  8     A ｜              33.3  8     A ｜\n             28.1  9     A ｜              31.3  9     A ｜\n               40 10     A ｜              32.5 10     A ｜\n# ... up to 66 more rows\n#  two more cases\n\n\nThe argument long = TRUE prints each case one after the other instead of side by side (e.g., print(exampleAB, long = TRUE)).\n\n3.6.1 Summary\n\n\n\n\n\n\nThe summary scdf function call:\n\n\n\nsummary(object, all_cases = FALSE, …)\n\n\nA short description of the scdf is provided by the summary command. The results are pretty much self explaining:\nsummary() gives a very concise overview of an scdf\n\nsummary(Huber2014)\n\n#A single-case data frame with four cases\n\n           Measurements Design\n Adam                37    A-B\n Berta               29    A-B\n Christian           76    A-B\n David               76    A-B\n\nVariable names:\ncompliance &lt;dependent variable&gt;\nphase &lt;phase variable&gt;\nmt &lt;measurement-time variable&gt;\n\nNote: Behavioral data (compliance in percent). \n\nAuthor of data: Christian Huber",
    "crumbs": [
      "The single-case data frame",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Create, display, and store single-case data</span>"
    ]
  },
  {
    "objectID": "ch_working_with_scdfs.html",
    "href": "ch_working_with_scdfs.html",
    "title": "4  Working with single-case data frames",
    "section": "",
    "text": "4.1 Select cases\nYou can extract one or more single-cases from an scdf with multiple cases in two ways.\nThe first method follows the basic rules of the R syntax. If the case has a name, you can address it with the $ operator\nHuber2014$David\nor you can use squared brackets to select by the number (its position) of a case\nHuber2014[1] #extracts case 1\nHuber2014[2:3] #extracts cases 2 and 3\nnew.huber2014 &lt;- Huber2014[c(1, 4)] #extracts cases 1 and 4\nnew.huber2014\n\n#A single-case data frame with two cases\n\n Adam: compliance mt phase ｜ David: compliance mt phase ｜\n               25  1     A ｜              65.6  1     A ｜\n             20.8  2     A ｜              37.5  2     A ｜\n             39.6  3     A ｜              58.3  3     A ｜\n               75  4     A ｜              72.9  4     A ｜\n               45  5     A ｜              33.3  5     A ｜\n             39.6  6     A ｜              59.4  6     A ｜\n             54.2  7     A ｜              77.1  7     A ｜\n               50  8     A ｜              54.2  8     A ｜\n             28.1  9     A ｜              68.8  9     A ｜\n               40 10     A ｜              43.8 10     A ｜\n             52.1 11     B ｜              62.5 11     B ｜\n             31.3 12     B ｜              64.6 12     B ｜\n             15.6 13     B ｜              60.4 13     B ｜\n             29.2 14     B ｜              81.3 14     B ｜\n             43.8 15     B ｜              79.2 15     B ｜\n# ... up to 61 more rows\nThe second method is to use the select_cases function.\nThe select_cases() function takes case names and/or numbers for selecting cases:\n# With pipes:\nHuber2014 %&gt;%\n  select_cases(Adam, Berta, 4) %&gt;%\n  summary()\n\n#A single-case data frame with three cases\n\n       Measurements Design\n Adam            37    A-B\n Berta           29    A-B\n David           76    A-B\n\nVariable names:\ncompliance &lt;dependent variable&gt;\nphase &lt;phase variable&gt;\nmt &lt;measurement-time variable&gt;\n\nNote: Behavioral data (compliance in percent). \n\nAuthor of data: Christian Huber \n\n# 1. Take the scdf Huber2014,\n# 2. select the cases Adam, Berta and case number four,\n# 3. show a summary of the remaining cases in the study.\nCase names can also be defined within a specific range by the colon operator:\nHuber2014 %&gt;%\n  select_cases(Berta:David) %&gt;%\n  summary()\n\n#A single-case data frame with three cases\n\n           Measurements Design\n Berta               29    A-B\n Christian           76    A-B\n David               76    A-B\n\nVariable names:\ncompliance &lt;dependent variable&gt;\nphase &lt;phase variable&gt;\nmt &lt;measurement-time variable&gt;\n\nNote: Behavioral data (compliance in percent). \n\nAuthor of data: Christian Huber",
    "crumbs": [
      "The single-case data frame",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Working with *single-case data frames*</span>"
    ]
  },
  {
    "objectID": "ch_working_with_scdfs.html#sec-select-cases",
    "href": "ch_working_with_scdfs.html#sec-select-cases",
    "title": "4  Working with single-case data frames",
    "section": "",
    "text": "The select_cases function call:\n\n\n\nselect_cases(scdf, …)\n\n\n\n\n\n\n\n\nNote\n\n\n\nSince version 0.53, scan includes functions to work with pipe-operators. scan imports the pipe operator %&gt;% from the magrittr package. Alternatively, you can use R’s native pipe operator |&gt;.",
    "crumbs": [
      "The single-case data frame",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Working with *single-case data frames*</span>"
    ]
  },
  {
    "objectID": "ch_working_with_scdfs.html#sec-subset",
    "href": "ch_working_with_scdfs.html#sec-subset",
    "title": "4  Working with single-case data frames",
    "section": "4.2 Select measurements",
    "text": "4.2 Select measurements\n\n\n\n\n\n\nThe subset function call:\n\n\n\nsubset(x, …)\n\n\n\n\n\n\n\n\nNote\n\n\n\nThe subset function is a method for the generic subset function. To call the help file you have to add the class to the function name: ?subset.scdf\n\n\nThe subset() function helps to extract measurements (or rows) from an scdf according to specific criteria.\nSubset takes an scdf as the first argument and a logical expression (filter) as the second argument. Only measurements for which the logical argument is true are included in the returned scdf object.\nFor example, the scdf Huber2014 has a variable compliance and we want to keep measurements where compliance is greater than 10 because we assume the others are outliers:\n\nHuber2014 %&gt;%\n  subset(compliance &gt; 10) %&gt;%\n  summary()\n\n#A single-case data frame with four cases\n\n           Measurements Design\n Adam                37    A-B\n Berta               20    A-B\n Christian           76    A-B\n David               76    A-B\n\nVariable names:\ncompliance &lt;dependent variable&gt;\nphase &lt;phase variable&gt;\nmt &lt;measurement-time variable&gt;\n\nNote: Behavioral data (compliance in percent). \n\nAuthor of data: Christian Huber \n\n\nIn a more complex example, we want to keep only values less than 60 when they are in phase A, or values equal to or greater than 60 when they are in phase B:\n\nexampleAB %&gt;%\n  subset((values &lt; 60 & phase == \"A\") | (values &gt;= 60 & phase == \"B\")) %&gt;%\n  summary()\n\n#A single-case data frame with three cases\n\n          Measurements Design\n Johanna            20    A-B\n Karolina           18    A-B\n Anja               19    A-B\n\nVariable names:\nvalues &lt;dependent variable&gt;\nphase &lt;phase variable&gt;\nmt &lt;measurement-time variable&gt;\n\nNote: Randomly created data with normal distributed dependent variable.",
    "crumbs": [
      "The single-case data frame",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Working with *single-case data frames*</span>"
    ]
  },
  {
    "objectID": "ch_working_with_scdfs.html#sec-transform",
    "href": "ch_working_with_scdfs.html#sec-transform",
    "title": "4  Working with single-case data frames",
    "section": "4.3 Change and create variables",
    "text": "4.3 Change and create variables\n\n\n\n\n\n\nThe transform function call:\n\n\n\ntransform(*_data, …*)\n\n\nWith the help of the transform() function, you can add new variables or change existing variables for each case of an scdf. This can be useful if you want to\n\nz-standardize a variable,\ncalculate a new variable as the sum of two existing variables\nconvert a frequency to a percentage,\n\nor in many other cases.\n\n\n\n\n\n\nNote\n\n\n\nThe transform function is a method for the generic transform function. To call the help file you have to add the class to the function name: ?transform.scdf\n\n\nHere is an example of standardizing the dependent variable “values”:\n\nexampleAB_z &lt;- transform(\n  exampleAB, values = (values-mean(values)) / sd(values)\n)\n\n# note: alternatively for the same result:\n# exampleAB_z &lt;- transform(exampleAB, values = scale(values))\n\nHere is an example where a new percentage variable is added and the measurement times shifted to start with 0:\n\nexampleAB_score %&gt;%\n  transform(\n    percentage = values / trials * 100,\n    mt = mt - mt[1]\n  )\n\n#A single-case data frame with three cases\n\n Christiano: values trials mt phase percentage\n                  1     20  0     A          5\n                  3     20  1     A         15\n                  3     20  2     A         15\n                  3     20  3     A         15\n                  5     20  4     A         25\n                  3     20  5     A         15\n                  0     20  6     A          0\n                  2     20  7     A         10\n                  4     20  8     A         20\n                  3     20  9     A         15\n                 12     20 10     B         60\n                 13     20 11     B         65\n                 15     20 12     B         75\n                 11     20 13     B         55\n                 15     20 14     B         75\n# ... up to 15 more rows\n#  two more cases\n\n\n\n4.3.1 all_cases\nThe all_cases helper function returns the values of a variable across all cases. This allows for calculations where you need values within a case and values across cases, for example when you want to standardize a variable based on all cases:\n\nexampleAB %&gt;%\n  transform(\n    values = (values - mean(all_cases(values))) / sd(all_cases(values))\n  ) %&gt;%\n  setNames(paste0(names(exampleAB), \"_z\")) %&gt;%\n  c(exampleAB) %&gt;%\n  smd()\n\nStandardized mean differences\n\n                            Johanna_z Karolina_z Anja_z Johanna Karolina  Anja\nmA                             -1.194     -1.431 -1.279   54.60    51.80 53.60\nmB                              0.454      0.398  0.449   74.13    73.47 74.07\nsdA                             0.203      0.577  0.257    2.41     6.83  3.05\nsdB                             0.755      0.824  0.639    8.94     9.76  7.57\nsd cohen                        0.553      0.711  0.487    6.55     8.43  5.77\nsd hedges                       0.673      0.776  0.577    7.97     9.19  6.83\nGlass' delta                    8.111      3.171  6.711    8.11     3.17  6.71\nHedges' g                       2.451      2.357  2.996    2.45     2.36  3.00\nHedges' g correction            2.348      2.258  2.869    2.35     2.26  2.87\nHedges' g durlak correction     2.227      2.142  2.722    2.23     2.14  2.72\nCohen's d                       2.983      2.572  3.545    2.98     2.57  3.54\n\n# 1. Take the exampleAB scdf,\n# 2. Z-standardise the values of each case based on all measurements,\n# 3. rename the cases by adding a \"_z\" suffix,\n# 4. add the original untransformed cases,\n# 5. analyse the data by calculating measures of standardized mean differences.\n\n\n\n4.3.2 Smoothing\nFor smoothing the data dependent variable, transform has a number of helper functions:\n\nmoving_mean calculates the moving median of a series of values. The lag argument specifies the number of values from which to calculate the mean (the default is 1, where the mean is calculated from a value and a measurement before and after that value),\nmoving_median is the same as before, but calculates the median instead of the mean,\nlocal_regression regresses each value on the surrounding values. The argument f defines the fraction of the values (the default f = 0.2 considers the surrounding 20% of the values). You must also provide the measurement time variable with the argument mt.\n\n\ntransform(Huber2014,\n  \"compliance (moving median)\" = moving_median(compliance),\n  \"compliance (moving mean)\" = moving_mean(compliance),\n  \"compliance (local regression)\" = local_regression(compliance, mt = mt)\n)\n\n#A single-case data frame with four cases\n\n Adam: compliance mt phase compliance (moving median) compliance (moving mean)\n               25  1     A                         25                       25\n             20.8  2     A                         25                    28.47\n             39.6  3     A                       39.6                    47.69\n               75  4     A                         45                     55.9\n               45  5     A                         45                    46.83\n             39.6  6     A                         45                    46.88\n             54.2  7     A                         50                    50.36\n               50  8     A                         50                    42.82\n             28.1  9     A                         40                    36.97\n               40 10     A                         40                    43.02\n             52.1 11     B                         40                    42.14\n             31.3 12     B                       31.3                    29.68\n             15.6 13     B                       29.2                    24.83\n             29.2 14     B                       29.2                    32.61\n             43.8 15     B                       29.2                     33.8\n compliance (local regression)\n                         22.51\n                         28.81\n                         34.49\n                         40.55\n                         44.56\n                         46.31\n                         46.14\n                         43.98\n                         42.21\n                         40.06\n                         37.24\n                         33.11\n                         29.56\n                         29.11\n                         28.94\n# ... up to 61 more rows\n#  three more cases\n\n\n\n\n4.3.3 Transform values at the begining of a phase\nThe first_of helper function is specifically designed to replace values at or around the beginning of a phase. The first argument is a logical vector defining a selection criterion. The positions argument is a vector of positions to be addressed. Negative numbers refer to positions before and positive numbers to positions after the selection criteria. This is useful, for example, if you want to discard the first two measurements of a phase.\nHere is an example that replaces the values at the beginning of phase A and the value after that to missing (NA), and also replaces the values at the beginning of phase B and the value before that to NA:\n\nbyHeart2011 %&gt;%\n  transform(\n    values = replace(values, first_of(phase == \"A\", 0:1), NA),\n    values = replace(values, first_of(phase == \"B\", -1:0), NA)\n  )\n\n#A single-case data frame with 11 cases\n\n Lisa (Turkish): values mt phase ｜ Patrick (Spanish): values mt phase ｜\n                   &lt;NA&gt;  1     A ｜                      &lt;NA&gt;  1     A ｜\n                   &lt;NA&gt;  2     A ｜                      &lt;NA&gt;  2     A ｜\n                      0  3     A ｜                         3  3     A ｜\n                      0  4     A ｜                         0  4     A ｜\n                   &lt;NA&gt;  5     A ｜                      &lt;NA&gt;  5     A ｜\n                   &lt;NA&gt;  6     B ｜                      &lt;NA&gt;  6     B ｜\n                      5  7     B ｜                         8  7     B ｜\n                      6  8     B ｜                         8  8     B ｜\n                      7  9     B ｜                         8  9     B ｜\n                     10 10     B ｜                        12 10     B ｜\n                     10 11     B ｜                        13 11     B ｜\n                     15 12     B ｜                        13 12     B ｜\n                     16 13     B ｜                        15 13     B ｜\n                     14 14     B ｜                        14 14     B ｜\n                     17 15     B ｜                        15 15     B ｜\n# ... up to 11 more rows\n#  nine more cases",
    "crumbs": [
      "The single-case data frame",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Working with *single-case data frames*</span>"
    ]
  },
  {
    "objectID": "ch_scplot.html",
    "href": "ch_scplot.html",
    "title": "5  Creating a single-case data plot",
    "section": "",
    "text": "5.1 Install scplot\nPlotting the data is a first important approach to analysis. After you create a scdf, the scplot() command helps you visualize the data. If the scdf contains more than one case, a multiple baseline plot is provided.\nscplot is an add-on package to scan for visualizing single-case data. It replaces the plot.scdf() (or: plotSC()) function already included in scan (see Section D.1). For the time being, the “old” plot.scdf will be kept in future versions of scan.\nsplot is available from the CRAN repository. Execute install.packages(\"scplot\") to install it.\nIf you are more adventures you can install the developmental version of scplot from github.The project is hosted at https://github.com/jazznbass/scplot. You can install it with devtools::install_github(\"jazznbass/scplot\") from your R console. Make sure you have the package devtools installed before. The scplot package has to be compiled. When you are running R on a Windows machine you also have to install Rtools. Rtools is not an R package and can be downloaded from CRAN at https://cran.r-project.org/bin/windows/Rtools/.\nThe following chapter has been written with scplot version 0.6.0. If you have problems replicating the examples, please update to this version.",
    "crumbs": [
      "The single-case data frame",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Creating a single-case data plot</span>"
    ]
  },
  {
    "objectID": "ch_scplot.html#basic-principal",
    "href": "ch_scplot.html#basic-principal",
    "title": "5  Creating a single-case data plot",
    "section": "5.2 Basic principal",
    "text": "5.2 Basic principal\nYou start by providing an scdf object (see Section 3.2) to the scplot() function (e.g. scplot(exampleAB)). This already creates a default plot.\n\nscplot(exampleAB)\n\n\n\n\n\n\n\n\nNow you use a series of pipe-operators (%&gt;% or |&gt;) to apply functions that add elements and change characteristics of the resulting plot. For example:\n\nscplot(exampleABC) |&gt;\n  add_title(\"My plot\") |&gt;\n  set_xlabel(\"Days\", color = \"red\", size = 1.3)\n\nHere is an overview of possible functions:\n\n\n\n\nTable 5.1: scplot functions\n\n\n\n\n\n\nFunction\nWhat it does ...\n\n\n\n\nset_dataline\nChange the default dataline/ add an additional dataline\n\n\nadd_statline\nAdd a line or curve representing statistical parameters\n\n\nadd_arrow\nAdd an arrow to a specific case at a specific position\n\n\nadd_line\nAdd a line to a specific case\n\n\nadd_grid\nAdd a grid to the plot pannel\n\n\nadd_labels\nAdd value labels to each data-point\n\n\nadd_legend\nAdd a plot legend\n\n\nadd_marks\nMark specific data points of specific cases\n\n\nadd_ridge\nColour the area below the dataline\n\n\nadd_text\nAdd text to a specific case at a specific position\n\n\nadd_title\nAdd a title above the plot\n\n\nadd_caption\nAdd a caption below the plot\n\n\nset_xlabel/ set_ylabel\nChange and style axis labels\n\n\nset_xaxis/ set_yaxis\nSet the value range, increments etc. of the x- and y-axis\n\n\nset_background\nSet colour and texture of the plot background\n\n\nset_panel\nSet colour and texture of the plot panel\n\n\nset_phasenames\nRename and style the phases\n\n\nset_casenames\nRename and style the phases\n\n\nset_separator\nStyle the vertical separator line between phases\n\n\nset_theme\nApply a predefined visual theme\n\n\nset_theme_element\nStyle specific elements of the plot\n\n\nas_ggplot\nReturn a ggplot2 object for further processing\n\n\nnew_theme\nCreate/define a new visual theme\n\n\n\n\n\n\n\n\nAll text, line, dot, and area elements have a set of arguments to change visual characteristics.\nText arguments can be applied to the following functions: add_caption(), add_labels(), add_legend(), add_text(), add_title(), set_xlabel(), set_ylabel(), set_phasenames(), set_casenames().\nPossible arguments are:\n\n\n\n\nTable 5.2: Arguments for text elements\n\n\n\n\n\n\nArgument\nWhat it does ...\n\n\n\n\ncolor\nChange color. Either a color name or a color code (e.g. 'red' or '#110044').\n\n\nsize\nRelativ size to the base text size.\n\n\nfamily\nThe font ('serif', 'sans', 'mono')\n\n\nface\nThe font face (\"plain\",\"bold\",\"italic\",\"bold.italic\")\n\n\nhjust\nHorizontal alignment (0 = left, 0.5 = centered, 1 = right)\n\n\nvjust\nVertical alignment (0 = upper, 0.5 = centered, 1 = lower)\n\n\n\n\n\n\n\n\nLine arguments can be applied to the following functions: set_dataline(), add_statline(), add_line(), add_arrow(), add_ridge(), set_xaxis(), set_yaxis(), set_separator().\n\n\n\n\nTable 5.3: Arguments for line elements\n\n\n\n\n\n\nArgument\nWhat it does ...\n\n\n\n\ncolor\nEither a color name or a color code (e.g. 'red' or '#110044').\n\n\nlinewidth\nRelativ width of the line.\n\n\nlinetype\nLinetype ('solid', 'dashed', 'dotted')\n\n\n\n\n\n\n\n\nPoint arguments can be applied to the following functions: set_dataline(), add_statline(), add_marks(), add_arrow().\n\n\n\n\nTable 5.4: Arguments for point elements\n\n\n\n\n\n\nArgument\nWhat it does ...\n\n\n\n\ncolor\nEither a color name or a color code (e.g. 'red' or '#110044').\n\n\nsize\nRelative size.\n\n\nshpae\nPoint shape.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 5.1: Some possible shapes",
    "crumbs": [
      "The single-case data frame",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Creating a single-case data plot</span>"
    ]
  },
  {
    "objectID": "ch_scplot.html#set-and-add-datalines",
    "href": "ch_scplot.html#set-and-add-datalines",
    "title": "5  Creating a single-case data plot",
    "section": "5.3 Set and add datalines",
    "text": "5.3 Set and add datalines\n\n\n\n\n\n\nThe set_dataline function call:\n\n\n\nset_dataline(object, variable = NULL, line, point, type = “continuous”, label = NULL, …)\n\n\nBy default, the single-case plot will depict the main dependent variable as defined in the scdf object. For changing this default behaviour or adding a second data line, use the set_dataline() function. The function takes the argument variable (with the main dependent variable as a default) which must correspond to a variable name within the applied scdf.\n\nscplot(exampleAB_add) |&gt;\n  set_dataline(\"depression\")\n\n\n\n\n\n\n\n\nStyling parameters like line and point colour will be set automatically based on the applied graphic theme. We will learn later about how to change and modify these themes. If you want to directly change the styling parameters, you can use the line and point arguments which take lists with styling parameters. For line, the parameters are colour, linewidth, linetype, lineend, and arrow. For point the parameters are colour, size, and shape.\n\nscplot(exampleAB_add) |&gt;\n  set_dataline(\n    line = list(colour = \"darkred\", linewidth = 2), \n    point = list(colour = \"black\", size = 3, shape = 15)\n  )",
    "crumbs": [
      "The single-case data frame",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Creating a single-case data plot</span>"
    ]
  },
  {
    "objectID": "ch_scplot.html#add-statlines",
    "href": "ch_scplot.html#add-statlines",
    "title": "5  Creating a single-case data plot",
    "section": "5.4 Add statlines",
    "text": "5.4 Add statlines\n\n\n\n\n\n\nThe add_statline function call:\n\n\n\nadd_statline(\n  object,\n  stat = c(“mean”, “median”, “min”, “max”, “quantile”, “sd”, “mad”, “trend”, “trendA”, “trendA theil-sen”, “trendA bisplit”, “trendA trisplit”, “moving mean”, “moving median”, “loreg”, “lowess”, “loess”),\n  phase = NULL,\n  color = NULL,\n  linewidth = NULL,\n  linetype = NULL,\n  variable = NULL,\n  label = NULL,\n  …\n)\n\n\n\n5.4.1 Lines indicating a constant for each phase\nPossible values for the stat argument are mean, min, max, median, sd, quantile\n\nscplot(exampleABC) |&gt;\n  add_statline(\"mean\") |&gt;\n  add_statline(\"max\") |&gt;\n  add_statline(\"min\") |&gt; \n  add_statline(\"median\")\n\n\n\n\n\n\n\n\n\n\n5.4.2 Lines indicating a constant for a specific phase\nSet the phase argument with one or multiple phase-names or phase-numbers to specifiy which phase the statistic is based on. The following example sets a line with the mean of phase A, the maximum of phases B and C and the minimum of phases 2 and 3:\n\nscplot(exampleABC) |&gt;\n  add_statline(\"mean\", phase = \"A\") |&gt;\n  add_statline(\"max\", phase = c(\"B\", \"C\")) |&gt;\n  add_statline(\"min\", phase = c(2, 3)) |&gt; \n  add_legend()\n\n\n\n\n\n\n\n\n\n\n5.4.3 Trend-lines\nThe trend statistic adds separate trend-line for each phase and trendA adds an extrapolated trend-line of the first phase:\n\nscplot(exampleABC) |&gt;\n  add_statline(\"trend\") |&gt;\n  add_statline(\"trendA\")\n\n\n\n\n\n\n\n\nYou scan specify various methods with the method argument for the trendA statistic:\n\nscplot(exampleABC) |&gt; \n  add_statline(\"trendA\") |&gt; \n  add_statline(\"trendA\", method = \"theil-sen\") |&gt; \n  add_statline(\"trendA\", method = \"bisplit\") |&gt; \n  add_statline(\"trendA\", method = \"trisplit\") |&gt; \n  add_legend()\n\n\n\n\n\n\n\n\nFor the trend statistic you can set method = \"theil-sen\" for median based Theil-Sen slope lines.\n\n\n5.4.4 Smoothed curves\nPossible values for the stat argument are moving mean, moving median, loess, lowess:\n\nscplot(exampleABC) |&gt;\n  add_statline(\"loess\") |&gt;\n  add_statline(\"moving mean\")\n\n\n\n\n\n\n\n\n\n\n5.4.5 Refine with additional arguments\nSome of the statistics allow additional arguments to specify parameters:\n\n\n\n\n\n\n\n\nStatistic\nArgument\nWhat it does …\n\n\n\n\nmean\ntrim\nTrims the mean. trim = 0.10 calculates a 10% trimmed mean.\n\n\nquantile\nprobs\nProbability. probs = 0.25 calculates the 25% quantile.\n\n\nmoving mean, moving median\nlag\nLag surrounding the estimated value. lag = 2 will calculate mean or median based on the two values before and after the to be replaced value.\n\n\nloess\nspan\nProportion of the surrounding point to estimate a value.\n\n\nlowess\nf\nProportion of the surrounding point to estimate a value.\n\n\n\n\nscplot(exampleABC) |&gt;\n  add_statline(\"moving mean\", lag = 1) |&gt;\n  add_statline(\"quantile\", probs = 0.75)\n\n\n\n\n\n\n\n\n\n\n5.4.6 Specify data-line\nIf you do not specify the variable argument, the default first data-line is addressed.\n\nscplot(exampleAB_add) |&gt;\n  set_dataline(\"cigarrets\") |&gt;\n  add_statline(\"mean\", variable = \"cigarrets\") |&gt;\n  add_statline(\"trend\")",
    "crumbs": [
      "The single-case data frame",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Creating a single-case data plot</span>"
    ]
  },
  {
    "objectID": "ch_scplot.html#annotate-and-mark",
    "href": "ch_scplot.html#annotate-and-mark",
    "title": "5  Creating a single-case data plot",
    "section": "5.5 Annotate and mark",
    "text": "5.5 Annotate and mark\n\n5.5.1 Add marks\nThe positions argument can take a numeric vector:\n\nscplot(exampleABC) |&gt;\n  add_marks(case = 1, positions = c(7, 12)) |&gt;\n  add_marks(case = 3, positions = c(3, 17), color = \"blue\", size = 7)\n\n\n\n\n\n\n\n\nThe positions argument can also be a string containing a logical expression. This will be evaluated and the respective positions will be marked.\n\nscplot(exampleABC) |&gt;\n  add_marks(case = 1, positions = \"mt &gt; 15\") |&gt;\n  add_marks(case = 2, positions = 'phase == \"B\"', color = \"green\", size = 5) |&gt;\n  add_marks(case = 3, positions = \"values &gt; quantile(values, probs = 0.80)\", color = \"blue\", size = 7) |&gt;\n  add_marks(case = \"all\", positions = \"values &lt; quantile(values, probs = 0.20)\", color = \"yellow\", size = 7) |&gt;\n  add_caption(\"red: mt &gt; 15 in case 1; \ngreen: phase 'B' in case 2; \nblue: values &gt; 80% quantile of case 3; \nyellow: values &lt; 20% quantile of all cases\")\n\n\n\n\n\n\n\n\nAnd the positions argument can take the results from a scan outlier analyses and mark the positions of the outliers of each case:\n\nscplot(exampleABC_outlier) |&gt; \n  add_marks(positions = outlier(exampleABC_outlier), size = 3)\n\n\n\n\n\n\n\n\n\n\n5.5.2 Add text\n\nscplot(exampleABC) |&gt;\n  add_text(\"Here!\", case = 2, x = 10, y = 80, color = \"red\")\n\n\n\n\n\n\n\n\n\n\n5.5.3 Add line\nDraw lines either by providing starting (x0 and y0) an end coordinates (x1 and y1) or a horizontal (hline) or vertical (vline) position:\n\nscplot(exampleABC) |&gt;\n  add_line(case = 1, x0 = 6, y0 = 90, x1 = 3, y1 = 63, color = \"red\") |&gt; \n  add_line(case = 2, hline = 80, color = \"blue\") |&gt; \n  add_line(case = 3, vline = 15, color = \"darkgreen\")\n\n\n\n\n\n\n\n\nDraw an arrow:\n\nscplot(exampleABC) |&gt;\n  add_arrow(case = 1, x0 = 6, y0 = 90, x1 = 3, y1 = 63) |&gt;\n  add_text(\"Problem\", case = 1, x = 6, y = 94, color = \"red\", size = 1, hjust = 0 )",
    "crumbs": [
      "The single-case data frame",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Creating a single-case data plot</span>"
    ]
  },
  {
    "objectID": "ch_scplot.html#change-appearance-of-basic-plot-elements",
    "href": "ch_scplot.html#change-appearance-of-basic-plot-elements",
    "title": "5  Creating a single-case data plot",
    "section": "5.6 Change appearance of basic plot elements",
    "text": "5.6 Change appearance of basic plot elements\n\n5.6.1 Data line\n\nscplot(exampleABC) |&gt;\n  set_dataline(color = \"blue\", linewidth = 1, linetype = \"dotted\", \n               point = list(colour = \"red\", size = 1, shape = 2) )\n\n\n\n\n\n\n\n# Equivalent_\n# scplot(exampleABC) |&gt;\n#   set_dataline(line = list(colour = \"blue\", size = 1, linetype = \"dotted\"), \n#                point = list(colour = \"red\", size = 1, shape = 2)) \n\n\n\n5.6.2 Background\nThe background ist the complete area of the plot including a frame.\n\nscplot(exampleABC) |&gt;\n  set_background(fill = \"grey90\", color = \"black\", size = 2)\n\n\n\n\n\n\n\n\n\n\n5.6.3 Panel\nThe panel refers to the coordinate system.\n\nscplot(exampleABC) |&gt;\n  set_panel(fill = \"tan1\", color = \"palevioletred\", size = 2)\n\n\n\n\n\n\n\n\nYou can specify a different panel color for each phase by providing a vector to the fill argument:\n\nscplot(exampleABC) |&gt;\n  set_panel(fill = c(\"grey80\", \"white\", \"blue4\"))\n\n\n\n\n\n\n\n\nNote: The colors are 50% transparent. So they might appear different.",
    "crumbs": [
      "The single-case data frame",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Creating a single-case data plot</span>"
    ]
  },
  {
    "objectID": "ch_scplot.html#themes",
    "href": "ch_scplot.html#themes",
    "title": "5  Creating a single-case data plot",
    "section": "5.7 Themes",
    "text": "5.7 Themes\nThemes are complete styles that define various elements of a plot.\nFunction set_theme(\"theme_name\")\nPossible themes:\nbasic, grid, default, small, tiny, big, minimal, dark, sienna, phase_color, phase_shade, grid2, illustration\n\n5.7.1 An overview\n\n\n\n\n\nVarious scplot themes.\n\n\n\n\n\n\n5.7.2 Combine themes\nWhen providing multiple themes the order is important as the latter overwrites styles of the former.\n\nscplot(exampleABC) |&gt;\n  set_theme(\"sienna\", \"minimal\", \"small\", \"phase_color\")\n\n\n\n\n\n\n\n\n\n\n5.7.3 Create custom themes\nFor creating a custom theme, start with the new_theme() function and add all styling parameters like described above. The resulting new object can now be applied to the set_theme() function:\n\nmy_theme &lt;- new_theme() |&gt;\n  set_panel(color = \"red\")  |&gt;\n  set_base_text(size = 12, color = \"blue\")  |&gt;\n  set_dataline(color = \"darkred\", linewidth = 2)\n\nscplot(exampleABC)  |&gt; set_theme(my_theme)",
    "crumbs": [
      "The single-case data frame",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Creating a single-case data plot</span>"
    ]
  },
  {
    "objectID": "ch_scplot.html#set-base-text",
    "href": "ch_scplot.html#set-base-text",
    "title": "5  Creating a single-case data plot",
    "section": "5.8 Set base text",
    "text": "5.8 Set base text\nThe base text size is the absolute size. All other text sizes are relative to this base text size.\n\nscplot(exampleAB_decreasing$Peter) |&gt;\n  set_base_text(colour = \"blue\", family = \"serif\", face = \"italic\", size = 14)",
    "crumbs": [
      "The single-case data frame",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Creating a single-case data plot</span>"
    ]
  },
  {
    "objectID": "ch_scplot.html#add-title-and-caption",
    "href": "ch_scplot.html#add-title-and-caption",
    "title": "5  Creating a single-case data plot",
    "section": "5.9 Add title and caption",
    "text": "5.9 Add title and caption\n\nscplot(exampleAB_decreasing) |&gt;\n  add_title(\"A new plot\", color = \"darkblue\", size = 1.3) |&gt;\n  add_caption(\"Note. What a nice plot!\", face = \"italic\", color = \"darkred\")",
    "crumbs": [
      "The single-case data frame",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Creating a single-case data plot</span>"
    ]
  },
  {
    "objectID": "ch_scplot.html#add-a-legend",
    "href": "ch_scplot.html#add-a-legend",
    "title": "5  Creating a single-case data plot",
    "section": "5.10 Add a legend",
    "text": "5.10 Add a legend\nThe add_legend() function adds a predefined legend that includes all datalines and statlines you have added. The labels for these lines are set automatically if you did not set the label argument in the corresponding set_dataline() and add_statline() functions.\nHere is a simple example:\n\nscplot(exampleABC) |&gt;\n  add_statline(\"mean\", color = \"darkred\") |&gt;\n  add_statline(\"min\", phase = \"B\", linewidth = 0.2, color = \"darkblue\") |&gt;\n  add_legend()\n\n\n\n\n\n\n\n\nAnd here is a more advanced example that uses several arguments to defines the style of the legend:\n\nscplot(exampleAB_add) |&gt;\n  set_dataline(label = \"Pychological Wellbeing\") |&gt;\n  set_dataline(variable = \"depression\", color = \"darkblue\", label = \"Depression\") |&gt;\n  add_statline(stat = \"mean\", label = \"Wellbeing mean\") |&gt;\n  add_statline(stat = \"mean\", variable = \"depression\", label = \"Depression mean\") |&gt;\n  set_phasenames(position = \"none\") |&gt;\n  set_panel(fill = c(\"lightblue\", \"grey80\")) |&gt;\n  add_legend(\n    position = \"left\",\n    section_labels = c(\"Variables\", \"Section\"),\n    title = list(color = \"brown\", size = 10, face = 2),\n    text = list(color = \"darkgreen\", size = 10, face = 2),\n    background = list(color = \"lightgrey\")\n  )",
    "crumbs": [
      "The single-case data frame",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Creating a single-case data plot</span>"
    ]
  },
  {
    "objectID": "ch_scplot.html#axis-settings",
    "href": "ch_scplot.html#axis-settings",
    "title": "5  Creating a single-case data plot",
    "section": "5.11 Axis settings",
    "text": "5.11 Axis settings\nWhen axis ticks are to close together set the increment argument to leave additional space (e.g. increment = 2 will annotate every other value). When you set increment_from = 0 an additional tick will be set at 1 although counting of the increments will start at 0.\n\nscplot(exampleA1B1A2B2) |&gt; \n  set_xaxis(increment_from = 0, increment = 5, \n            color = \"darkred\", size = 0.7, angle = -90) |&gt;\n  set_yaxis(limits = c(0, 50), size = 0.7, color = \"darkred\")",
    "crumbs": [
      "The single-case data frame",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Creating a single-case data plot</span>"
    ]
  },
  {
    "objectID": "ch_scplot.html#axis-labels",
    "href": "ch_scplot.html#axis-labels",
    "title": "5  Creating a single-case data plot",
    "section": "5.12 Axis labels",
    "text": "5.12 Axis labels\n\nscplot(exampleA1B1A2B2) |&gt; \n  set_ylabel(\"Score\", color = \"darkred\", angle = 0) |&gt;\n  set_xlabel(\"Session\", color = \"darkred\")",
    "crumbs": [
      "The single-case data frame",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Creating a single-case data plot</span>"
    ]
  },
  {
    "objectID": "ch_scplot.html#change-casenames",
    "href": "ch_scplot.html#change-casenames",
    "title": "5  Creating a single-case data plot",
    "section": "5.13 Change Casenames",
    "text": "5.13 Change Casenames\n\nscplot(exampleA1B1A2B2) |&gt;\n  set_casenames(c(\"A\", \"B\", \"C\"), color = \"darkblue\", size = 1)\n\n\n\n\n\n\n\n\nCasenames as strips:\n\nscplot(exampleA1B1A2B2) |&gt;\n  set_casenames(position = \"strip\", \n                background = list(fill = \"lightblue\"))",
    "crumbs": [
      "The single-case data frame",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Creating a single-case data plot</span>"
    ]
  },
  {
    "objectID": "ch_scplot.html#add-value-labels",
    "href": "ch_scplot.html#add-value-labels",
    "title": "5  Creating a single-case data plot",
    "section": "5.14 Add value labels",
    "text": "5.14 Add value labels\n\nscplot(exampleABC) |&gt; \n  add_labels(text = list(color = \"black\", size = 0.7), \n             background = list(fill = \"grey98\"), nudge_y = 7)\n\nWarning: Removed 1 row containing missing values or values outside the scale range\n(`geom_label()`).\n\n\n\n\n\n\n\n\n\nIf you set the nudge_y argument to 0, the label will be set on-top the datapoints:\n\nscplot(exampleABC) |&gt; \n  add_labels(text = list(color = \"black\", size = 0.7), \n             background = list(fill = \"grey98\"), nudge_y = 0)",
    "crumbs": [
      "The single-case data frame",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Creating a single-case data plot</span>"
    ]
  },
  {
    "objectID": "ch_scplot.html#add-a-ridge",
    "href": "ch_scplot.html#add-a-ridge",
    "title": "5  Creating a single-case data plot",
    "section": "5.15 Add a ridge",
    "text": "5.15 Add a ridge\n\nscplot(exampleAB_mpd) |&gt; \n  add_ridge(\"grey50\")",
    "crumbs": [
      "The single-case data frame",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Creating a single-case data plot</span>"
    ]
  },
  {
    "objectID": "ch_scplot.html#extending-scplot-with-ggplot2",
    "href": "ch_scplot.html#extending-scplot-with-ggplot2",
    "title": "5  Creating a single-case data plot",
    "section": "5.16 Extending scplot with ggplot2",
    "text": "5.16 Extending scplot with ggplot2\nscplot() generates ggplot2 objects. You can keep the ggplot2 object and assign it into a new object with the as_ggplot() function. Thereby, you can use many ggplot2 functions to rework your graphics:\n\np1 &lt;- scplot(byHeart2011$`Lisa (Turkish)`) |&gt; \n        set_theme(\"minimal\") |&gt;\n        as_ggplot()\np2 &lt;- scplot(byHeart2011$`Patrick (Spanish)`) |&gt; \n        set_theme(\"minimal\") |&gt; \n        as_ggplot()\np3 &lt;- scplot(byHeart2011$`Anna (Twi)`) |&gt; \n        set_theme(\"minimal\") |&gt; \n        as_ggplot()\np4 &lt;- scplot(byHeart2011$`Melanie (Swedish)`) |&gt; \n        set_theme(\"minimal\") |&gt; \n        as_ggplot()\n\nlibrary(patchwork)\np1 + p2 + p3 + p4 + plot_annotation(tag_levels = \"a\", tag_suffix =  \")\")",
    "crumbs": [
      "The single-case data frame",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Creating a single-case data plot</span>"
    ]
  },
  {
    "objectID": "ch_scplot.html#complexs-examples",
    "href": "ch_scplot.html#complexs-examples",
    "title": "5  Creating a single-case data plot",
    "section": "5.17 Complexs examples",
    "text": "5.17 Complexs examples\nHere are some more complex examples\n\nscplot(example_A24) |&gt; \n  add_statline(\"lowess\", linewidth = 1.5) |&gt;\n  add_statline(\"loess\", linewidth = 1.5) |&gt;\n  add_statline(\"moving mean\", lag = 3, linewidth = 1.5) |&gt;\n  set_xaxis(size = 0.8, angle = 35) |&gt;\n  set_dataline(point = \"none\") |&gt;\n  add_legend(position = c(0.8, 0.75), background = list(color = \"grey50\")) |&gt;\n  set_phasenames(c(\"no speedlimit\", \"with speedlimit\"), \n                 position = \"left\", hjust = 0, vjust = 1) |&gt;\n  set_casenames(position = \"none\") |&gt;\n  add_title(\"Effect of a speedlimit on the A24\") |&gt;\n  add_caption(\"Note: Moving mean calculated with lag three\", face = 3, size = 1) |&gt;\n  add_ridge(color = \"lightblue\")\n\n\n\n\n\n\n\n\n\nscplot(exampleAB_add) |&gt;\n  set_dataline(\"cigarrets\", point = list(size = 1)) |&gt;\n  add_statline(\"trend\", linetype = \"dashed\", label = \"Trend of wellbeing\") |&gt;\n  add_statline(\"mean\", variable = \"cigarrets\", color = \"darkred\", label = \"Mean of cigarrets\") |&gt;\n  add_marks(positions = c(14,20), size = 3, variable = \"cigarrets\")|&gt;\n  add_marks(positions = \"cigarrets &gt; quantile(cigarrets, 0.75)\", size = 3) |&gt;\n  set_xaxis(increment = 5) |&gt;\n  set_phasenames(color = NA) |&gt;\n  set_casenames(position = \"strip\") |&gt;\n  add_legend(\n    section_labels = c(\"\", \"\"),\n    text = list(face = 3)\n  ) |&gt;\n  set_panel(fill = c(\"lightblue\", \"grey80\")) |&gt;\n  add_ridge(color = \"snow\", variable = \"cigarrets\") |&gt;\n  add_labels(variable = \"cigarrets\", nudge_y = 2, \n             text = list(color = \"blue\", size = 0.5)) |&gt;\n  add_labels(nudge_y = 2, text = list(color = \"black\", size = 0.5),\n             background = list(fill = \"white\"))\n\nWarning: Removed 1 row containing missing values or values outside the scale range\n(`geom_label()`).\n\n\n\n\n\n\n\n\n\n\nscplot(exampleA1B1A2B2) |&gt; \n  set_xaxis(increment = 4, size = 0.7) |&gt;\n  set_yaxis(color = \"sienna3\") |&gt;\n  set_ylabel(\"Points\", color = \"sienna3\", angle = 0) |&gt;\n  set_xlabel(\"Weeks\", size = 1, color = \"brown\") |&gt;\n  add_title(\"Points by week\", color = \"sienna4\", face = 3) |&gt;\n  add_caption(\"Note: An extensive Example.\",\n              color = \"black\", size = 1, face = 3) |&gt;\n  set_phasenames(c(\"Baseline\", \"Intervention\", \"Fall-Back\", \"Intervention_2\"), \n                 size = 0) |&gt;\n  add_ridge(scales::alpha(\"lightblue\", 0.5)) |&gt;\n  set_casenames(labels = sample_names(3), color = \"steelblue4\", size = 0.7) |&gt;\n  set_panel(fill = c(\"grey80\", \"grey95\"), color = \"sienna4\") |&gt;\n  add_grid(color = \"grey85\", linewidth = 0.1) |&gt;\n  set_dataline(size = 0.5, linetype = \"solid\", \n               point = list(colour = \"sienna4\", size = 0.5, shape = 18)) |&gt;\n  add_labels(text = list(color = \"sienna\", size = 0.7), nudge_y = 4) |&gt;\n  set_separator(size = 0.5, linetype = \"solid\", color = \"sienna\") |&gt;\n  add_statline(stat = \"trendA\", color = \"tomato2\") |&gt;\n  add_statline(stat = \"max\", phase = c(1, 3), linetype = \"dashed\") |&gt;\n  add_marks(case = 1:2, positions = 14, color = \"red3\", size = 2, shape = 4) |&gt;\n  add_marks(case = \"all\", positions = \"values &lt; quantile(values, 0.1)\", \n            color = \"blue3\", size = 1.5) |&gt;\n  add_marks(positions = outlier(exampleABAB), color = \"brown\", size = 2) |&gt;\n  add_text(case = 1, x = 5, y = 35, label = \"Interesting\", \n           color = \"darkgreen\", angle = 20, size = 0.7) |&gt;\n  add_arrow(case = 1, 5, 30, 5, 22, color = \"steelblue\") |&gt;\n  set_background(fill = \"white\") |&gt;\n  add_legend() |&gt;\n  set_theme(\"basic\") |&gt;\n  set_theme_element(panel.spacing.y = unit(0, \"points\"))\n\nWarning: Removed 6 rows containing missing values or values outside the scale range\n(`geom_text()`).\n\n\n\n\n\n\n\n\n\nAdding bars is a bit more complicated:\n\nSet the type argument to \"bar\"\n\nExtend the limits of the x-axis by 1 (here from 0 to 41)\n\nSet the left margin of the x-axis to 0 with the expand argument.\n\n\nscplot(exampleAB_add) |&gt;\n  set_xaxis(expand = c(0, 0), limits = c(0, 41)) |&gt;\n  set_dataline(\"cigarrets\", type = \"bar\", linewidth = 0.6, point = \"none\") |&gt;\n  add_statline(\"mean\", variable = \"cigarrets\", color = \"darkred\") |&gt;\n  add_statline(\"trend\", linetype = \"dashed\") |&gt;\n  set_casenames(position = \"strip\")",
    "crumbs": [
      "The single-case data frame",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Creating a single-case data plot</span>"
    ]
  },
  {
    "objectID": "ch_helper_functions.html",
    "href": "ch_helper_functions.html",
    "title": "6  Setting variables for analysis",
    "section": "",
    "text": "In an scdf, it is specified which variables represent the measurement time, phase, and dependent variable. If multiple measurement times, phases, or dependent variables are present, they can be switched using helper functions. The function set_dvar changes the dependent variable, set_mvar modifies the measurement-time variable, and set_pvar adjusts the phase variable. The function set_vars serves as a redundant option to set multiple variables at once.\n\n\n\n\n\n\nThe set_dvar function call:\n\n\n\nset_dvar(data, dvar)\n\n\n\n\n\n\n\n\nThe set_mvar function call:\n\n\n\nset_mvar(data, mvar)\n\n\n\n\n\n\n\n\nThe set_pvar function call:\n\n\n\nset_pvar(data, pvar)\n\n\n\n\n\n\n\n\nThe set_vars function call:\n\n\n\nset_vars(data, dvar, mvar, pvar)\n\n\nAssume we have a single-case study and smooth the dependent variable:\n\nscdf &lt;- Huber2014$Berta |&gt; \n  transform(compliance_smooth = local_regression(compliance))\n\nNow we change the dependent variable to compliance_smooth for an analysis:\n\nscdf |&gt; \n  set_dvar(\"compliance_smooth\") |&gt; \n  smd()\n\nStandardized mean differences\n\n                            Berta\nmA                          36.97\nmB                          18.23\nsdA                          8.04\nsdB                         10.77\nsd cohen                     9.50\nsd hedges                    9.94\nGlass' delta                -2.33\nHedges' g                   -1.89\nHedges' g correction        -1.83\nHedges' g durlak correction -1.77\nCohen's d                   -1.97\n\n\nThe following variables were used in this analysis:\n'compliance_smooth' as dependent variable, 'phase' as phase variable, and 'mt' as measurement-time variable.\n\n\nIf you want to change the variables permanently, replace an scdf with its changed version:\n\nscdf &lt;- set_dvar(scdf, \"compliance_smooth\")",
    "crumbs": [
      "The single-case data frame",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Setting variables for analysis</span>"
    ]
  },
  {
    "objectID": "ch_missing_values_outliers.html",
    "href": "ch_missing_values_outliers.html",
    "title": "7  Missing values and outliers",
    "section": "",
    "text": "7.1 Missing values\nThere are two kinds of missing values in single-case data series. First, missings that were explicitly recorded as NA and assigned to a phase and measurement-time as in the following example:\nThe second type of missing occurs when there are gaps between measurement-times that are not explicitly coded as in the following example:\nIn both cases, missing values pose a threat to the internal validity of overlap indices. Randomization tests are more robust against the first type of missing values but are affected by the second type. Regression approaches are less impacted by both types as they take the interval between measurement-times into account.\ncase1 &lt;- scdf(c(3,6,2,4,3,5,2,6,3,2, 6,7,5,8,6,7,4,8,5,6), \n              phase_design = c(A = 10, B = 10), name = \"no NA\")\ncase2 &lt;- scdf(c(3,6,2,4,3,5,2,NA,3,2, 6,7,5,8,6,NA,4,8,5,6), \n              phase_design = c(A = 10, B = 10), name = \"NAs\")\ncase3 &lt;- fill_missing(case2)\nnames(case3) &lt;- \"interpolated NAs\"\nex &lt;- c(case1, case2, case3)\nscplot(ex)\n\nWarning: Removed 2 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\noverlap(ex)\n\nOverlap Indices\n\nComparing phase 1 against phase 2 \n\n             no NA  NAs interpolated NAs\nDesign         A-B  A-B              A-B\nPND             40   33               30\nPEM            100  100              100\nPET            100  100              100\nNAP             88   91               92\nNAP rescaled    77   83               83\nPAND            80   89               90\nIRD           0.60 0.67             0.70\nTau_U(A)      0.53 0.61             0.61\nTau_U(BA)     0.45 0.51             0.50\nBase_Tau      0.59 0.64             0.64\nDiff_mean     2.60 2.78             2.75\nDiff_trend    0.02 0.11             0.12\nSMD           1.65 1.96             2.02\nHedges_g      1.71 1.90             1.96",
    "crumbs": [
      "The single-case data frame",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Missing values and outliers</span>"
    ]
  },
  {
    "objectID": "ch_missing_values_outliers.html#sec-fill-missing",
    "href": "ch_missing_values_outliers.html#sec-fill-missing",
    "title": "7  Missing values and outliers",
    "section": "",
    "text": "scdf(c(5, 3, 4, 6, 8, 7, 9, 7, NA, 6), phase_design = c(A = 4, B = 6))\n\nscdf(c(5, 3, 4, 6, 8, 7, 9, 7, 6), phase_design = c(A = 4, B = 5), \n     mt = c(1, 2, 3, 4, 5, 6, 7, 8, 10))",
    "crumbs": [
      "The single-case data frame",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Missing values and outliers</span>"
    ]
  },
  {
    "objectID": "ch_missing_values_outliers.html#sec-outlier",
    "href": "ch_missing_values_outliers.html#sec-outlier",
    "title": "7  Missing values and outliers",
    "section": "7.2 Outlieranalysis",
    "text": "7.2 Outlieranalysis\n\n\n\n\n\n\nThe outlier function call:\n\n\n\noutlier(data, dvar, pvar, mvar, method = c(“MAD”, “Cook”, “SD”, “CI”), criteria = 3.5)\n\n\nscan provides several methods for analyzing outliers. All of them are implemented in the outliers function. Available methods are the standard deviation, mean average deviation, confidence intervals, and Cook’s distance. The criteria argument takes a vector with two information, the first defines the analyzing method (“SD”, “MAD”, CI”, “Cook”) and the second the criteria. For “SD” the criteria is the number of standard deviations (sd) from the mean of each phase for which a value is not considered to be an outlier. For example, criteria = c(\"SD\", 2) would identify every value exceeding two sd above or below the mean as an outlier whereas sd and mean refer to phase of a value. As this might be misleading particularly for small samples Iglewicz and Hoaglin Iglewicz & Hoaglin (1993) recommend the use the much more robust median average deviation (MAD) instead. The MAD is is constructed similar to the sd but uses the median instead of the mean. Multiplying the MAD by 1.4826 approximates the sd in a normal distributed sample. This corrected MAD is applied in the outlier function. A deviation of 3.5 times the corrected MAD from the median is suggested to be an outlier. To use this criterion set criteria = c(\"MAD\", 3.5). criteria = c(\"CI\", 0.95) takes exceeding the 95% confidence interval as the criteria for outliers. The Cook’s distance method for calculation outliers can be applied with a strict AB-phase design. in that case, the Cook’s distance analyses are based on a piecewise-regression model. Most commonly, Cook’s distance exceeding 4/n is used as a criteria. This could be implemented setting criteria = c(\"Cook\", \"4/n\").\n\noutlier(exampleABC_outlier, criteria = c(\"MAD\", 3.5))\n\nOutlier Analysis for Single-Case Data\n\nCase Bernadette : Dropped 3 \nCase Penny : Dropped 2 \nCase Amy : Dropped 3 \n\n# Visualizing outliers with the plot function\nres &lt;- outlier(exampleABC_outlier, criteria = c(\"MAD\", 3.5))\n\nscplot(exampleABC_outlier) |&gt;\n  add_labels(nudge_y = 15, text = list(size = 0.7)) |&gt;\n  add_marks(positions = res) |&gt;\n  set_theme(\"basic\") |&gt;\n  set_casenames(position = \"strip\") |&gt;\n  set_yaxis(limits = c(20, 120))\n\nWarning: Removed 9 rows containing missing values or values outside the scale range\n(`geom_text()`).\n\n\n\n\n\n\n\n\n\n\n\n\n\nIglewicz, B., & Hoaglin, D. C. (1993). How to detect and handle outliers. Milwaukee, Wis. : ASQC Quality Press.",
    "crumbs": [
      "The single-case data frame",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Missing values and outliers</span>"
    ]
  },
  {
    "objectID": "ch_describe.html",
    "href": "ch_describe.html",
    "title": "8  Describe single-case data frames",
    "section": "",
    "text": "8.1 Descriptive statistics\ndescribe is the basic command to get an overview on descriptive statistics. As an argument it only takes the name of an scdf object. For each case of the scdf and each phase within a case descriptive statistics are provided. The output table contains statistical indicators followed by a dot and the name of the phase (e.g., n.A for the number of measurements of phase A).\nStatistics of the describe command\n\n\nParameter\nWhat it means ...\n\n\n\n\nn\nNumber of measurements.\n\n\nmis\nNumber of missing values.\n\n\nm\nMean values.\n\n\nmd\nMedian of values.\n\n\nsd\nStandard deviation of values.\n\n\nmad\nMedian average deviation of values.\n\n\nmin/max\nMin and max of values.\n\n\ntrend\nSlope of a regression line through values by time.\ndescribe(exampleABC)\n\nDescribe Single-Case Data\n\n       Marie Rosalind  Lise\nDesign A-B-C    A-B-C A-B-C\nn.A       10       15    20\nn.B       10        8     7\nn.C       10        7     3\nmis.A      0        0     0\nmis.B      0        0     0\nmis.C      0        0     0\n\n          Marie Rosalind    Lise\nm.A      52.000   52.267  52.350\nm.B      72.100   73.250  73.571\nm.C      68.000   66.429  71.333\nmd.A       53.5     52.0    52.0\nmd.B       72.5     72.0    73.0\nmd.C         69       68      76\nsd.A      8.287    8.146  10.869\nsd.B     11.367   13.134  10.644\nsd.C     12.702   10.486  21.385\nmad.A    11.119    7.413  10.378\nmad.B    10.378   10.378  16.309\nmad.C    17.791   11.861  20.756\nmin.A        39       37      35\nmin.B        47       54      60\nmin.C        51       52      48\nmax.A        63       65      74\nmax.B        85       97      87\nmax.C        87       78      90\ntrend.A  -1.915    0.500  -0.088\ntrend.B  -0.612    0.643   1.929\ntrend.C  -0.194   -2.929 -14.000\nThe resulting table could be exported into a csv file to be used in other software (e.g., to inserted in a word processing document). Therefore, first write the results of the describe command into an R object and then use the write.csv to export the descriptives element of the object.\n# write the results into a new R object named `res`\nres &lt;- describe(exampleABC)\n# create a new file containing the descriptives on your harddrive\nwrite.csv(res$descriptives, file = \"descriptive data.csv\")\nThe file is written to the currently active working directory. If you are not sure where that is, type getwd() (you can use the setwd() command to define a different working directory. To get further details type help(setwd) into R).",
    "crumbs": [
      "Data analysis",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Describe single-case data frames</span>"
    ]
  },
  {
    "objectID": "ch_describe.html#sec-describe",
    "href": "ch_describe.html#sec-describe",
    "title": "8  Describe single-case data frames",
    "section": "",
    "text": "The describe function call:\n\n\n\ndescribe(data, dvar, pvar, mvar)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nConflicting function names\n\n\n\nSometimes R packages include the same function names. For example, the describe() function is also part of the psych package. Now, if you have loaded the psych package with library(psych) after scan the describe() function of scan will be masked (describe() would now call the corresponding function of the psych package).\nThere are two solutions to this problem:\n\nactivate the psych library before the scan library (now the psych describe() function will be masked) or\ninclude the package name into the function call with the prefix scan::: scan::describe().",
    "crumbs": [
      "Data analysis",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Describe single-case data frames</span>"
    ]
  },
  {
    "objectID": "ch_describe.html#sec-autocorr",
    "href": "ch_describe.html#sec-autocorr",
    "title": "8  Describe single-case data frames",
    "section": "8.2 Auto-regression",
    "text": "8.2 Auto-regression\n\n\n\n\n\n\nThe autocorr function call:\n\n\n\nautocorr(data, dvar, pvar, mvar, lag_max = 3, …)\n\n\nThe autocorr function calculates autocorrelations within each phase and across all phases. The lag_max argument defines the lag up to which the autocorrelation will be computed.\n\nautocorr(exampleABC, lag_max = 4)\n\nAutocorrelations\n\nMarie \n Phase Lag 1 Lag 2 Lag 3 Lag 4\n     A  0.29 -0.11  0.10  0.12\n     B -0.28 -0.10 -0.14 -0.09\n     C  0.00 -0.33 -0.14 -0.25\n   all  0.21  0.10  0.25  0.12\n\nRosalind \n Phase Lag 1 Lag 2 Lag 3 Lag 4\n     A  0.37 -0.29 -0.33 -0.34\n     B -0.34  0.24 -0.40  0.04\n     C -0.07 -0.32  0.27  0.02\n   all  0.49  0.38  0.22  0.17\n\nLise \n Phase Lag 1 Lag 2 Lag 3 Lag 4\n     A  0.04 -0.32 -0.05 -0.09\n     B -0.63  0.50 -0.40  0.31\n     C -0.38 -0.12    NA    NA\n   all  0.33  0.36  0.23  0.27",
    "crumbs": [
      "Data analysis",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Describe single-case data frames</span>"
    ]
  },
  {
    "objectID": "ch_describe.html#sec-trend",
    "href": "ch_describe.html#sec-trend",
    "title": "8  Describe single-case data frames",
    "section": "8.3 Trend analysis",
    "text": "8.3 Trend analysis\n\n\n\n\n\n\nThe trend function call:\n\n\n\ntrend(data, dvar, pvar, mvar, offset = “deprecated”, first_mt = 0, model = NULL)\n\n\nThe trend function provides an overview of linear trends in single-case data. By default, it gives you the intercept and slope of a linear and a squared regression of measurement-time on scores. Models are computed separately for each phase and across all phases. For a more advanced application, you can add regression models using the R specific formula class.\n\n# Simple example\ntrend(exampleABC$Marie)\n\nTrend for each phase\n\n              Intercept      B   Beta\nLinear.ALL       55.159  0.612  0.392\nLinear.A         60.618 -1.915 -0.700\nLinear.B         74.855 -0.612 -0.163\nLinear.C         68.873 -0.194 -0.046\nQuadratic.ALL    59.135  0.017  0.330\nQuadratic.A      57.937 -0.208 -0.712\nQuadratic.B      73.217 -0.039 -0.098\nQuadratic.C      68.490 -0.017 -0.038\n\nNote. Measurement-times start at 0 for each phase\n\n# Complex example\ntrend(\n  exampleAB$Johanna, \n  model = c(\"Cubic\" = values ~ I(mt^3), \n            \"Log Time\" = values ~ log(mt+1))\n)\n\nTrend for each phase\n\n              Intercept      B   Beta\nLinear.ALL       52.271  1.787  0.908\nLinear.A         54.400  0.100  0.066\nLinear.B         62.758  1.625  0.813\nQuadratic.ALL    58.582  0.086  0.864\nQuadratic.A      54.841 -0.040 -0.110\nQuadratic.B      66.985  0.106  0.767\nCubic.ALL        61.354  0.004  0.806\nCubic.A          55.050 -0.022 -0.251\nCubic.B          68.784  0.007  0.721\nLog Time.ALL     43.532 12.149  0.848\nLog Time.A       54.032  0.593  0.156\nLog Time.B       57.300  9.051  0.791\n\nNote. Measurement-times start at 0 for each phase",
    "crumbs": [
      "Data analysis",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Describe single-case data frames</span>"
    ]
  },
  {
    "objectID": "ch_overlapping_indices.html",
    "href": "ch_overlapping_indices.html",
    "title": "9  Overlapping indices",
    "section": "",
    "text": "9.1 Overlap overview\noverlap provides a table with some of the most important overlap indices for each case of an scdf. For calculating overlap indicators it is important to know if a decrease or an increase of values is expected between phases. By default overlap assumes an increase in values. If the argument decreasing = TRUE is set, calculation will be based on the assumption of decreasing values.\noverlap(exampleAB)\n\nOverlap Indices\n\nComparing phase 1 against phase 2 \n\n             Johanna Karolina  Anja\nDesign           A-B      A-B   A-B\nPND              100       87    93\nPEM              100      100   100\nPET              100       93   100\nNAP              100       97    98\nNAP rescaled     100       93    96\nPAND             100       90    90\nIRD             1.00     0.73  0.87\nTau_U(A)        0.59     0.55  0.62\nTau_U(BA)       0.77     0.78  0.64\nBase_Tau        0.63     0.59  0.61\nDiff_mean      19.53    21.67 20.47\nDiff_trend      1.53     0.54  2.50\nSMD             8.11     3.17  6.71\nHedges_g        2.35     2.26  2.87\nOverlap measures refer to a comparison of two phases within a single-case data-set. By default, overlap compares the first to the second phase.",
    "crumbs": [
      "Data analysis",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Overlapping indices</span>"
    ]
  },
  {
    "objectID": "ch_overlapping_indices.html#overlap-overview",
    "href": "ch_overlapping_indices.html#overlap-overview",
    "title": "9  Overlapping indices",
    "section": "",
    "text": "The overlap function call:\n\n\n\noverlap(data, dvar, pvar, mvar, decreasing = FALSE, phases = c(1, 2))\n\n\n\n\n\n\n9.1.1 Select and recombine phases\n\n\n\n\n\n\nThe select_phases function call:\n\n\n\nselect_phases(data, A, B, phase_names = “auto”)\n\n\nThe select_phases() function is needed if you like to compare specific phases or even like to combine several phases. select_phases() is designed to work within a pipe structure. So the first argument is an scdf and it returns an scdf.\n\nscdf |&gt; select_phases(A = 1, B = 3) |&gt; ...\n\nselect_phases() has the arguments A and B. Each argument takes a vector with the names or the numbers of the phases to be selected. If you want to compare the first to the third phase you can set select_phases(scdf, 1,3). If the phases of your case are named ‘A’, ‘B’, and ‘C’ you could alternatively set select_phases(scdf, \"A\",\"C\"). It is also possible to compare a combination of several cases against a combination of other phases. Each of the two list-elements could contain more than one phase which are concatenated with the c command. For example if you have an ABAB-Design and like to compare the two A-phases against the two B-phases select_phases(scdf, c(1,3), c(2,4) ) will do the trick.\n(As an alternative approach you can set the phases argument within the overlap() function. This argument takes a list with two elements where the first element defines the phases for the A-phase and the second argument the phases for the B-phase.)\n\nexampleA1B1A2B2 |&gt;\n  select_phases(c(\"A1\",\"A2\"), c(\"B1\",\"B2\")) |&gt;\n  overlap()\n\nOverlap Indices\n\nComparing phase 1 against phase 2 \n\n                 Pawel    Moritz    Jannis\nDesign       A1A2-B1B2 A1A2-B1B2 A1A2-B1B2\nPND                 55        78        71\nPEM                100       100       100\nPET                100       100       100\nNAP                 94        97        98\nNAP rescaled        89        94        97\nPAND                85        85        90\nIRD               0.75      0.80      0.89\nTau_U(A)          0.54      0.44      0.43\nTau_U(BA)         0.45      0.46      0.38\nBase_Tau          0.65      0.68      0.68\nDiff_mean        12.25     13.58     15.27\nDiff_trend       -0.05      0.00     -0.54\nSMD               2.68      3.27      3.62\nHedges_g          2.07      2.72      2.98\n\n# Alternatively:\n# overlap(exampleA1B1A2B2, phases = list( c(\"A1\",\"A2\"), c(\"B1\",\"B2\")))",
    "crumbs": [
      "Data analysis",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Overlapping indices</span>"
    ]
  },
  {
    "objectID": "ch_overlapping_indices.html#sec-pnd",
    "href": "ch_overlapping_indices.html#sec-pnd",
    "title": "9  Overlapping indices",
    "section": "9.2 Percentage non-overlapping data (PND)",
    "text": "9.2 Percentage non-overlapping data (PND)\n\n\n\n\n\n\nThe pnd function call:\n\n\n\npnd(data, dvar, pvar, decreasing = FALSE, phases = c(1, 2))\n\n\nThe percentage of non-overlapping data (PND) effect size measure was described by Scruggs, Mastropieri, & Casto (1987) . It is the percentage of all data-points of the second phase of a single-case study exceeding the maximum value of the first phase. In case you have a study where you expect a decrease of values in the second phase, PND is calculated as the percentage of data-point of the second phase below the minimum of the first phase.\n\n\n\n\n\nIllustration of PND. PND is 60% as 9 out of 15 datapoints of phase B are higher than the maximum of phase A\n\n\n\n\nThe function pnd provides the PND for each case as well as the mean of all PNDs of that scdf. When you expect decreasing values set decreasing = TRUE. When there are more than two phases or phases are not named A and B, use the phases argument as described at the beginning of this chapter.\n\npnd(exampleAB)\n\nPercent Non-Overlapping Data\n\n     Case    PND Total Exceeds\n  Johanna   100%    15      15\n Karolina 86.67%    15      13\n     Anja 93.33%    15      14\n\nMean  : 93.33 %",
    "crumbs": [
      "Data analysis",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Overlapping indices</span>"
    ]
  },
  {
    "objectID": "ch_overlapping_indices.html#sec-pem",
    "href": "ch_overlapping_indices.html#sec-pem",
    "title": "9  Overlapping indices",
    "section": "9.3 Percentage exceeding the median (PEM)",
    "text": "9.3 Percentage exceeding the median (PEM)\n\n\n\n\n\n\nThe pem function call:\n\n\n\npem(\n  data,\n  dvar,\n  pvar,\n  decreasing = FALSE,\n  binom.test = TRUE,\n  chi.test = FALSE,\n  FUN = median,\n  phases = c(1, 2),\n  …\n)\n\n\nThe pem function returns the percentage of phase B data exceeding the phase A median. Additionally, a binomial test against a 50/50 distribution is computed. Different measures of central tendency can be addressed for alternative analyses.\n\n\n\n\n\nIllustration of PEM. PEM is 75% as 13 out of 15 datapoints of phase B are higher than the median of phase A\n\n\n\n\n\npem(exampleAB)\n\nPercent Exceeding the Median\n\n     Case PEM positives total  binom.p\n  Johanna 100        15    15 3.05e-05\n Karolina 100        15    15 3.05e-05\n     Anja 100        15    15 3.05e-05\n\nAlternative hypothesis: true probability &gt; 50%",
    "crumbs": [
      "Data analysis",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Overlapping indices</span>"
    ]
  },
  {
    "objectID": "ch_overlapping_indices.html#sec-pet",
    "href": "ch_overlapping_indices.html#sec-pet",
    "title": "9  Overlapping indices",
    "section": "9.4 Percentage exceeding the regression trend (PET)",
    "text": "9.4 Percentage exceeding the regression trend (PET)\n\n\n\n\n\n\nThe pet function call:\n\n\n\npet(data, dvar, pvar, mvar, ci = 0.95, decreasing = FALSE, phases = c(1, 2))\n\n\nThe pet function provides the percentage of phase B data points exceeding the prediction based on the phase A trend. A binomial test against a 50/50 distribution is computed. Furthermore, the percentage of phase B data points exceeding the upper (or lower) 95 percent (default) confidence interval of the predicted progress is computed.\n\npet(exampleAB)\n\nPercent Exceeding the Trend\n\n     Case   PET PET CI  binom.p\n  Johanna 100.0   86.7 3.05e-05\n Karolina  93.3    0.0 4.88e-04\n     Anja 100.0  100.0 3.05e-05\n\nBinom.test: alternative hypothesis: true probability &gt; 50%\nPET CI: Percent of values greater than upper 95% confidence threshold (single sided)\n\n\n\n\n\n\n\nIllustration of PET. PET is 66.7% as 10 out of 15 datapoints of phase B are higher than the projected trend-line of phase A",
    "crumbs": [
      "Data analysis",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Overlapping indices</span>"
    ]
  },
  {
    "objectID": "ch_overlapping_indices.html#sec-pand",
    "href": "ch_overlapping_indices.html#sec-pand",
    "title": "9  Overlapping indices",
    "section": "9.5 Percentage of all non-overlapping data (PAND)",
    "text": "9.5 Percentage of all non-overlapping data (PAND)\n\n\n\n\n\n\nThe pand function call:\n\n\n\npand(data, dvar, pvar, decreasing = FALSE, phases = c(1, 2), method = c(“sort”, “minimum”))\n\n\nThe pand function calculates the percentage of all non-overlapping data (Richard I. Parker, Hagan-Burke, & Vannest, 2007), an index to quantify a level increase (or decrease) in performance after the onset of an intervention. The authors emphasize that PAND is designed for application in a multiple case design with a substantial number of measurements, technically at least 20 to 25, but preferably 60 or more. PAND is defined as 100% minus the percentage of data points that need to be removed from either phase in order to ensure nonoverlap between the phases.\nSeveral approaches have been suggested to calculate PAND, leading to potentially different outcomes. In their 2007 paper, Parker and colleagues present an algorithm for computing PAND. The algorithm involves sorting the scores of a time series, including the associated phases, and comparing the resulting phase order with the original phase order using a contingency table. To account for ties, the algorithm includes a randomization process where ties are randomly assigned to one of the two phases. Consequently, executing the algorithm multiple times could yield different results. It is important to note that this algorithm does not produce the same results as the PAND definition provided earlier in the same paper. However, it offers the advantage of allowing the calculation of an effect size measure phi, and the application of statistical tests for frequency distributions. phi equals Pearsons r for dichotomous data. Thus, phi-Square is the amount of explained variance.\nPustejovsky (2019) presented a mathematical formulation of Parker’s original definition for comparing two phases of a single case:\n\\[PAND = \\frac{1}{m+n}max\\{(i+j)I(y^A_{i}&lt;y^B_{n+1-j}\\}\\]\nThis formulation provides accurate results for PAND, but the original definition has the drawback of an unknown distribution under the null hypothesis, making a statistical test difficult.\nThe pand() function enables the calculation of PAND using both methods. The first approach (method = \"sort\") follows the algorithm described above, with the exclusion of randomization before sorting to avoid ambiguity. It calculates a phi measure and provides the results of a chi-squared test and a Fisher exact test. The second approach (method = \"minimum\") applies the aforementioned formula. For a multiple case design, overlaps are calculated for each case, summed, and then divided by the total number of measurements. No statistical test is conducted for this method.\n\npand(Parker2007)\n\nPercentage of all non-overlapping data\n\nMethod: sort \n\nPAND = 85.7%\nΦ =  0.713  ; Φ² =  0.508 \n\n28 measurements (13 Phase A, 15 Phase B) in 3 cases\nOverlapping data: n = 4 ; percentage = 14.3 \n\n2 x 2 Matrix of percentages\n         A    B total\nA     39.3  7.1  46.4\nB      7.1 46.4  53.6\ntotal 46.4 53.6 100.0\n\n2 x 2 Matrix of counts\n       A  B total\nA     11  2    13\nB      2 13    15\ntotal 13 15    28\n\n\nChi-Squared test:\nX² = 14.227, df = 1, p = 0.000 \n\nFisher exact test:\nOdds ratio = 29.007, p = 0.000 \n\n\n\npand(Parker2007, method = \"minimum\")\n\nPercentage of all non-overlapping data\n\nMethod: minimum \n\nPAND = 85.7%\n28 measurements (13 Phase A, 15 Phase B) in 3 cases\nOverlapping data: n = 4 ; percentage = 14.3 \n\n\nThe original procedure for computing PAND does not account for ambivalent datapoints (ties). The newer NAP overcomes this problem and has better precision-power (Richard I. Parker, Vannest, & Davis, 2011a).",
    "crumbs": [
      "Data analysis",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Overlapping indices</span>"
    ]
  },
  {
    "objectID": "ch_overlapping_indices.html#sec-nap",
    "href": "ch_overlapping_indices.html#sec-nap",
    "title": "9  Overlapping indices",
    "section": "9.6 Nonoverlap of all pairs (NAP)",
    "text": "9.6 Nonoverlap of all pairs (NAP)\n\n\n\n\n\n\nThe nap function call:\n\n\n\nnap(data, dvar, pvar, decreasing = FALSE, phases = c(1, 2))\n\n\nThe nap function calculates the nonoverlap of all pairs (Richard I. Parker & Vannest, 2009). NAP summarizes the overlap between all pairs of phase A and phase B data points. Therefore, every single measurement of phase A is compared to all measurements in phase B, resuting in \\(n_{Phase A} \\times n_{Phase B}\\) pairs. If an increase in phase B values is expected, a non-overlapping pair will have a higher phase B data point. The NAP is equal to the number of pairs showing no overlap / number of pairs. Since NAP has values between 0 and 100% where 50% is no effect, a rescaled NAP (ranging between -100 and 100%, where 0% is no effect) has been proposed. NAP is equivalent to the U-test and the Wilcoxon rank sum test. Therefore, a Wilcoxon signed rank sum test is conducted and reported for each case. Additionally, effect sizes d and R-squared are reported according to Parker and colleagues.\n\nnap(exampleAB)\n\nNonoverlap of All Pairs\n\n     Case NAP NAP Rescaled   w     p   d   R²\n  Johanna 100          100 0.0 &lt;.001 3.5 0.75\n Karolina  97           93 2.5  &lt;.01 2.6 0.62\n     Anja  98           96 1.5 &lt;.001 2.8 0.66",
    "crumbs": [
      "Data analysis",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Overlapping indices</span>"
    ]
  },
  {
    "objectID": "ch_overlapping_indices.html#sec-ird",
    "href": "ch_overlapping_indices.html#sec-ird",
    "title": "9  Overlapping indices",
    "section": "9.7 Improvement rate difference (IRD)",
    "text": "9.7 Improvement rate difference (IRD)\n\n\n\n\n\n\nThe ird function call:\n\n\n\nird(data, dvar, pvar, decreasing = FALSE, phases = c(1, 2))\n\n\nThe adaptation of the improvement rate difference for single-case phase comparisons was developed by Richard I. Parker, Vannest, & Brown (2009). A variation called robust improvement rate difference was proposed by Richard I. Parker et al. (2011a). The ird() function calculates the robust improvement rate difference. It follows the formula suggested by Pustejovsky (2019). For a multiple case design, IRD is based on the overall improvement rate of all cases which is the average of the IRDs for each case.\n\nird(exampleAB)\n\nImprovement rate difference = 0.867",
    "crumbs": [
      "Data analysis",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Overlapping indices</span>"
    ]
  },
  {
    "objectID": "ch_overlapping_indices.html#sec-tauu",
    "href": "ch_overlapping_indices.html#sec-tauu",
    "title": "9  Overlapping indices",
    "section": "9.8 Tau-U",
    "text": "9.8 Tau-U\n\n\n\n\n\n\nThe tau_u function call:\n\n\n\ntau_u(\n  data,\n  dvar,\n  pvar,\n  method = c(“complete”, “parker”, “tarlow”),\n  phases = c(1, 2),\n  meta_analyses = TRUE,\n  ci = 0.95,\n  ci_method = c(“z”, “tau”, “s”),\n  meta_weight_method = c(“z”, “tau”),\n  tau_method = c(“b”, “a”),\n  continuity_correction = FALSE\n)\n\n\nThe Tau-U statistic has been proposed by Richard I. Parker, Vannest, Davis, & Sauber (2011b) and is one of the more broadly used approach for reporting effect sizes of single case data. Unfortunately, various and ambiguous implementations of Tau-U exist (Brossart, Laird, & Armstrong, 2018; Pustejovsky, 2016). The tau_u function tries to cover several of these implementation. It takes an scdf and returns Tau-U calculations for each single-case within that file. Additionally, an overall Tau-U value is calculated for all cases based on a meta-analysis.\n\n9.8.1 Variations of Tau-U\nSeveral arguments can be set to define how Tau-U should be calculated. When setting the argument method = \"parker\", Tau-U is according to Richard I. Parker et al. (2011b). However, this procedure could lead to Tau-U values above 1 and below -1 which are difficult to interpret. Additionally, this method uses Kendall’s Tau A, which does not correct for ties, and no continuity correct is calculated. The default setting method = \"complete\" applies a correction that keeps the values within the -1 to 1 range. method = \"tarlow\" applies a variation that has been implemented by Tarlow (2017) . This method is similar to \"complete\" but is based on Tau A rather than Tau B and includes a continuity correction.\nNote that the arguments tau_method (\"a\" or \"b\") and continuity_correction (TRUE or FALSE) only function with method = \"complete\".\nMy recommendation is to use method = \"complete\" in combination with the defaults tau_method = \"b\" and continuity_correction = TRUE for the most appropriate results.\n\nHere is an example with setting that reconstruct the values from the original example in Richard I. Parker, Vannest, Davis, & Sauber (2011c) :\n\ntau_u(Parker2011, method = \"parker\")\n\nTau-U\nMethod: parker \nApplied Kendall's Tau-a\n95% CIs for tau are reported.\nCI method: z\n\nCase: [case #1] \n                             Tau CI lower CI upper SD_S    Z    p\nA vs. B                     0.80     0.29     0.96 8.16 2.00  .05\nA vs. B - Trend A           0.65    -0.02     0.92 8.48 1.53  .13\nA vs. B + Trend B           0.77     0.21     0.95 8.91 2.58 &lt;.05\nA vs. B + Trend B - Trend A 0.56    -0.17     0.89 9.35 2.14 &lt;.05\n\n\nA different implementation of the method (provided at http://www.singlecaseresearch.org/calculators/tau-u)) uses Kendall’s Tau B:\n\ntau_u(exampleAB$Johanna, method = \"parker\", tau_method = \"b\", continuity_correction = FALSE)\n\nTau-U\nMethod: parker \nApplied Kendall's Tau-a\n95% CIs for tau are reported.\nCI method: z\n\nCase: Johanna \n                             Tau CI lower CI upper  SD_S    Z     p\nA vs. B                     1.00      NaN      NaN 22.91 3.27 &lt;.001\nA vs. B - Trend A           1.00      NaN      NaN 23.26 3.22 &lt;.001\nA vs. B + Trend B           0.81     0.56     0.92 30.53 4.75 &lt;.001\nA vs. B + Trend B - Trend A 0.76     0.48     0.90 30.81 4.71 &lt;.001\n\n\nAnother online calculator created by Rumen Manolov is available at https://manolov.shinyapps.io/Overlap/. It uses an R code developed by Kevin Tarlow to calculate Tau-U. This setting will replicate the results of this approach:\n\ntau_u(exampleAB$Johanna, method = \"tarlow\")\n\nTau-U\nMethod: tarlow \nApplied Kendall's Tau-a\n95% CIs for tau are reported.\nCI method: z\n\nCase: Johanna \n                             Tau CI lower CI upper  SD_S    Z     p\nA vs. B                     1.00      NaN      NaN 22.90 3.23 &lt;.001\nA vs. B - Trend A           0.88     0.72     0.95 23.26 3.18 &lt;.001\nA vs. B + Trend B           0.81     0.56     0.92 30.53 4.72 &lt;.001\nA vs. B + Trend B - Trend A 0.76     0.48     0.90 30.81 4.67 &lt;.001\n\n\nThe standard return of the tau_u function does not display all calculations. If you like to have more details, apply the print function with the additional argument complete = TRUE.\n\ntau_u(exampleAB$Johanna) |&gt; print(complete = TRUE)\n\nTau-U\nMethod: complete \nApplied Kendall's Tau-b\n95% CIs for tau are reported.\nCI method: z\n\nCase: Johanna \n                            pairs pos neg ties   S      D  Tau CI lower\nA vs. B                        75  75   0    0  75  75.00 1.00      NaN\nTrend A                        10   5   5    0   0  10.00 0.00    -0.88\nTrend B                       105  87  17    1  70 104.50 0.67     0.24\nA vs. B - Trend A              85  80   5    0  75 126.75 0.59     0.20\nA vs. B + Trend B             180 162  17    1 145 184.45 0.79     0.53\nA vs. B + Trend B - Trend A   190 167  22    1 145 189.50 0.77     0.49\n                            CI upper  SD_S  VAR_S SE_Tau    Z     p  n\nA vs. B                          NaN 22.91 525.00   0.31 3.27 &lt;.001 20\nTrend A                         0.88  4.08  16.67    NaN 0.00  1.00  5\nTrend B                         0.88 20.21 408.33   0.19 3.47 &lt;.001 15\nA vs. B - Trend A               0.82 23.26 541.22   0.18 3.22 &lt;.001 20\nA vs. B + Trend B               0.91 30.53 932.39   0.17 4.75 &lt;.001 20\nA vs. B + Trend B - Trend A     0.90 30.81 949.00   0.16 4.71 &lt;.001 20\n\n\n\n\n9.8.2 Meta analyses\n\n\n\n\n\n\nNote\n\n\n\nThe procedure for calculating the meta-analyses has changed with scan version 0.55.7. Please make sure you are using the latest scan version.\n\n\nIf you pass multiple cases to the tau-u function, it will calculate a Tau-U table for each case and an overall calculation via a meta-analysis.\n\n\n\n\n\n\nCalculating a Tau-U meta analysis\n\n\n\nThe calculation of the Tau-U-meta-analyses involves the following steps:\n\nThe tau values are Fisher-Z transformed to \\(Tau_z\\).\nThe standard error for each transformed value is calculated as either:\n\\(se_z = {1 \\over \\sqrt{n-3}}\\) (Hotelling, 1953)\nor\n\\(se_z = \\sqrt{0.437 \\over n-4}\\) (Fieller, Hartley, & Pearson, 1957)\nThe average \\(tau_z\\) is the mean of \\(tau_z\\) weighted by \\(1 \\over se_z^2\\)\nThe standard error of the average \\(tau_z\\) is \\(se_{M_{tau_z}} = \\sqrt{\\frac{1}{\\sum{weights}}}\\) (Cooper, Hedges, & Valentine, 2009)\nThe p value is calculated with a Z-test (from \\(Z = \\frac{M_{tau_z}}{se_{M_{tau_z}}}\\) )\nThe overall tau value is derived from an inverse-Fisher-Z-transformation.\n\n\n\n\n\n9.8.3 Confidence intervals\n\n\n\n\n\n\nNote\n\n\n\nThe default method for calculating the confidence interval has changed with scan version 0.55.7. Confidence intervals could have been outside the [-1, 1] in earlier versions. Set ci_method = \"s\" for a replication of results from scan version 0.55.6 or earlier.\n\n\nBy default, 95% confidence intervals are calculated for each tau value. You can specify a different interval with the ci argument (ci = 0.90 will calculate a 90% interval). There are three alternative methods for calculating the confidence intervals. When ci_method = \"z\" is set (the default), a general formula for calculating the standard-error of Fisher-Z values is used (Hotelling, 1953). If ci_method = \"tau\", a specific formula for Fisher-Z transformed tau values is applied (Fieller et al., 1957). Both approaches give similar results. A third approach is derived from the standard deviation of the S statistic1. For this method, set ci_method = \"s\". The S method can give implausible values less than -1 or greater than 1. I recommend using the general “z” method or the accurate “tau” method.å\n\ntau_u(exampleAB, ci = 0.90, ci_method = \"tau\")\n\nTau-U\nMethod: complete \nApplied Kendall's Tau-b\n90% CIs for tau are reported.\nCI method: tau\n\nTau-U meta analyses:\nWeight method: z\n90% CIs are reported.\n\n                       Model Tau_U   se CI lower CI upper   z       p\n                     A vs. B  1.00 0.14     1.00     1.00 Inf 0.0e+00\n           A vs. B - Trend A  0.59 0.14     0.42     0.72 4.8 1.3e-06\n           A vs. B + Trend B  0.75 0.14     0.63     0.83 6.9 4.4e-12\n A vs. B + Trend B - Trend A  0.74 0.14     0.61     0.82 6.7 1.8e-11\n\nCase: Johanna \n                             Tau CI lower CI upper  SD_S    Z     p\nA vs. B                     1.00      NaN      NaN 22.91 3.27 &lt;.001\nA vs. B - Trend A           0.59     0.39     0.74 23.26 3.22 &lt;.001\nA vs. B + Trend B           0.79     0.66     0.87 30.53 4.75 &lt;.001\nA vs. B + Trend B - Trend A 0.77     0.63     0.86 30.81 4.71 &lt;.001\n\nCase: Karolina \n                             Tau CI lower CI upper  SD_S    Z     p\nA vs. B                     0.94     0.90     0.96 22.91 3.06 &lt;.001\nA vs. B - Trend A           0.55     0.34     0.71 23.25 3.01 &lt;.001\nA vs. B + Trend B           0.80     0.69     0.88 30.52 4.85 &lt;.001\nA vs. B + Trend B - Trend A 0.78     0.65     0.87 30.79 4.81 &lt;.001\n\nCase: Anja \n                             Tau CI lower CI upper  SD_S    Z     p\nA vs. B                     0.97     0.94     0.98 22.91 3.15 &lt;.001\nA vs. B - Trend A           0.62     0.43     0.76 23.21 3.36 &lt;.001\nA vs. B + Trend B           0.63     0.43     0.76 30.45 3.74 &lt;.001\nA vs. B + Trend B - Trend A 0.64     0.45     0.78 30.71 3.91 &lt;.001",
    "crumbs": [
      "Data analysis",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Overlapping indices</span>"
    ]
  },
  {
    "objectID": "ch_overlapping_indices.html#sec-bctau",
    "href": "ch_overlapping_indices.html#sec-bctau",
    "title": "9  Overlapping indices",
    "section": "9.9 Baseline corrected tau",
    "text": "9.9 Baseline corrected tau\n\n\n\n\n\n\nThe corrected_tau function call:\n\n\n\ncorrected_tau(\n  data,\n  dvar,\n  pvar,\n  mvar,\n  phases = c(1, 2),\n  alpha = 0.05,\n  continuity = FALSE,\n  repeated = FALSE,\n  tau_method = c(“b”, “a”)\n)\n\n\nThis method has been proposed by Tarlow (2016). The baseline data are checked for a significant autocorrelation (based on Kendalls Tau). If so, a non-parameteric Theil-Sen regression is applied for the baseline data where the dependent values are regressed on the measurement time. The resulting slope information is then used to predict data of the B-phase. The dependent variable is now corrected for this baseline trend and the residuals of the Theil-Sen regression are taken for further calculations. Finally, Kendalls tau is calculated for the dependent variable and the dichotomous phase variable. The function here provides two extensions to this procedure: The alternative Siegel repeated median regression is applied when repeated = TRUE (Siegel, 1982) and a continuity correction is applied when continuity = TRUE (both not the defaults).\nHere is a replication of an example provided by Tarlow (2016) :\n\n\n\n\n\n\n\n\n\n\ncase &lt;- scdf(\n  c(A = 33, 25, 17, 25, 14, 13,14, \n    B = 14, 15, 15, 4, 6, 9, 5 ,4 ,2 ,2 ,8, 11 ,7)\n)\n\ncorrected_tau(case)\n\nBaseline corrected tau\n\nMethod: Theil-Sen regression\nKendall's tau b applied.\nContinuity correction not applied.\n\n[case #1] :\n                           tau     z     p\nBaseline autocorrelation -0.75 -2.31  &lt;.05\nUncorrected tau          -0.58 -2.98  &lt;.01\nBaseline corrected tau    0.69  3.57 &lt;.001\n\nBaseline correction should be applied.",
    "crumbs": [
      "Data analysis",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Overlapping indices</span>"
    ]
  },
  {
    "objectID": "ch_overlapping_indices.html#sec-smd",
    "href": "ch_overlapping_indices.html#sec-smd",
    "title": "9  Overlapping indices",
    "section": "9.10 Within case standardized mean differences",
    "text": "9.10 Within case standardized mean differences\n\n\n\n\n\n\nThe smd function call:\n\n\n\nsmd(data, dvar, pvar, mvar, phases = c(1, 2))\n\n\nStandardized mean differences between two phases within a single-case can be calculated in various ways. They refer to the difference in the means of two phases divided by a (within case) standard deviation. The smd function provides an overview of the most common parameters for each single-case:\n\nsmd(exampleAB_score)\n\nStandardized mean differences\n\n                            Christiano Lionel Neymar\nmA                                2.70   3.10   2.30\nmB                               15.35  15.35  15.60\nsdA                               1.42   1.59   1.49\nsdB                               2.13   1.60   2.19\nsd cohen                          1.81   1.60   1.87\nsd hedges                         1.93   1.60   1.99\nGlass' delta                      8.92   7.68   8.90\nHedges' g                         6.54   7.67   6.68\nHedges' g correction              6.37   7.46   6.50\nHedges' g durlak correction       6.15   7.21   6.28\nCohen's d                         6.98   7.67   7.10\n\n\nThe first four rows give the mean and the standard deviation of the two respective phases. sd cohen is the (unweighted) average of the standard deviation of phase A and B: \\(\\sqrt{\\frac{{\\text{sdA}^2 + \\text{sdB}^2}}{2}}\\). sd Hedges is the weighted average of the standard deviation with a correction for degrees of freedom: \\(\\sqrt{\\frac{ (nA - 1) \\cdot \\text{sdA}^2 + (nB - 1) \\cdot \\text{sdB}^2 }{ nA + nB - 2 }}\\). Hedges' g is the mean difference divided by sd Hedges. Hedges' g correction (\\(Hedges' g * (1 - \\frac{3}{4n - 9})\\)) and Hedges' g durlak correction (\\(Hedges' g * (\\frac{{n - 3}}{{n - 2.25}} \\cdot \\sqrt{\\frac{{n - 2}}{{n}}})\\)) are two approaches of correcting Hedges’ g for small sample sizes. Glass' delta is the mean difference divided by the standard deviation of the A-phase. Cohen's d is the mean difference divided by sd cohen.",
    "crumbs": [
      "Data analysis",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Overlapping indices</span>"
    ]
  },
  {
    "objectID": "ch_overlapping_indices.html#sec-bcsmd",
    "href": "ch_overlapping_indices.html#sec-bcsmd",
    "title": "9  Overlapping indices",
    "section": "9.11 Between case standardized mean differences",
    "text": "9.11 Between case standardized mean differences\n\n\n\n\n\n\nThe between_smd function call:\n\n\n\nbetween_smd(data, method = c(“REML”, “MCMCglmm”), ci = 0.95, include_residuals = TRUE, …)\n\n\nThe previous section described the calculation of standardized mean differences within a subject (here: between two phases of one single case). An inherent problem with this calculations is, that the idea and measures of standardized mean differences (e.g. Cohen’s d) have been developed to characterize the differences between groups. That is, a difference is standardized by the variation between subjects in a group. These are two fundamentally different things. Variations within a subject might be due to measurement errors or individual fluctuations in the strength of an attribute across time. Whereas the variations between subjects indicate a social frame of reference for what is large or small. Accordingly, it is not sensible to apply the well known standards to classify standardizes mean differences (e.g., Cohen’s cut-off values for the interpretation of d) to within case standardized mean differences. Within case standardized mean differences are usually much larger compared to these cut-offs as variations within a subject across time are (mostly) much smaller that differences between subjects.\nHedges, Pustejovsky, & Shadish (2012) proposed an approach to calculate the between case standardized mean difference, that is conceptually equivalent to Cohen’s d. This approach is applicable when we have a study with multiple single-cases (here: not just two or three but, depending on the actual data structure, maybe fifteen, or more). The basic idea is, to calculate a multi-level regression model (measurements nested in subjects) with a random intercept (here: between case standard deviation of the intercept) and the phase variable as a dummy coded fixed predictor (see Chapter 11 for details on the calculation of multilevel single case regression models).\nThe estimated regression weight of the phase predictor (here: the mean differences between the phases) is divided by the variance the random intercept and the residuals to give the BC-SMD (between case standardised mean difference):\n\\[\n\\text{BC-SMD} = \\frac{\\hat{\\beta}_1}{\\sqrt{\\hat{\\sigma}^2_\\varepsilon + \\hat{\\sigma}^2_u}}\n\\] where:\n\n\\(\\hat{\\beta}_1\\) is the estimated fixed effect of the phase (i.e., mean difference between phase A and B),\n\\(\\hat{\\sigma}^2_\\epsilon\\) is the within-case residual variance (level-1),\n\\(\\hat{\\sigma}^2_u\\) is the between-case variance in intercepts (level-2).\n\nThis basic procedure and some extensions to it are provided with the between_smd() function:\n\nbetween_smd(Leidig2018)\n\nBetween-Case Standardized Mean Difference\n\nMethod: REML\nBase model\n\n Effect BC-SMD   se LL-CI95% UL-CI95%\n phaseB   0.83 0.04     0.75     0.91\n\nFull plm model\n\n Effect BC-SMD   se LL-CI95% UL-CI95%\n phaseB   0.71 0.07     0.58     0.84\n\n\nThe Base model provides the BC-SMD as originally proposed. The Full plm model adds trend and slope effects to the regression model for a more precise estimation.\nBy setting the argument include_residuals = FALSE, the residuals are not counted towards the between case standard deviation. This leads two are more precise estimation of the standard deviations based on the between case variance but at the cost that the BC-SMD is conceptually less accurately reflecting Cohen’s d:\n\nbetween_smd(Leidig2018, include_residuals = FALSE)\n\nBetween-Case Standardized Mean Difference\n\nMethod: REML\nBase model\n\n Effect BC-SMD   se LL-CI95% UL-CI95%\n phaseB   1.28 0.06     1.16     1.41\n\nFull plm model\n\n Effect BC-SMD  se LL-CI95% UL-CI95%\n phaseB   1.07 0.1     0.87     1.27\n\n\nThe method argument allows to define whether the analyses is based on a REML Estimation (method = \"REML\") or a bayesian (method = \"MCMCglmm\") regression model.\nIt is also possible to first calculate a hierarchichal piecewise regression model (hplm, see Chapter 11) and then base the BC-SMD calculation on that model:\n\nhplm(Leidig2018, slope = FALSE) |&gt; between_smd()\n\nBetween-Case Standardized Mean Difference\n\nMethod: REML\nProvided\n\n Effect BC-SMD   se LL-CI95% UL-CI95%\n phaseB   0.63 0.05     0.53     0.74",
    "crumbs": [
      "Data analysis",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Overlapping indices</span>"
    ]
  },
  {
    "objectID": "ch_overlapping_indices.html#sec-rci",
    "href": "ch_overlapping_indices.html#sec-rci",
    "title": "9  Overlapping indices",
    "section": "9.12 Reliable change index",
    "text": "9.12 Reliable change index\n\n\n\n\n\n\nThe rci function call:\n\n\n\nrci(data, dvar, pvar, rel, ci = 0.95, graph = FALSE, phases = c(1, 2))\n\n\nBasically, the reliable change index (rci) shows whether a post-test is above a pre-test value. Based on the reliability of the measurements and the standard deviation, the standard error is calculated. The mean difference between phase A and phase B is divided by the standard error. Several authors have proposed refined methods for calculating the rci.\nThe rci function calculates two indices of reliable change (Wise, 2004) and corresponding descriptive statistics.\n\nrci(exampleAB$Johanna, rel = 0.8)\n\nReliable Change Index\n\nMean Difference =  19.533 \nStandardized Difference =  1.678 \nStandard error of differences =  1.523 \nReliability of measurements =  0.8 \n\nDescriptives:\n        n   mean    SD    SE\nA-Phase 5 54.600 2.408 1.077\nB-Phase 5 74.133 8.943 4.000\n\n95 % Confidence Intervals:\n         Lower  Upper\nA-Phase 52.489 56.711\nB-Phase 66.294 81.972\n\nReliable Change Indices:\n                           RCI\nJacobson et al.         18.136\nChristensen and Mendoza 12.824\n\n\n\n\n\n\nBrossart, D. F., Laird, V. C., & Armstrong, T. W. (2018). Interpreting Kendall’s Tau and Tau-U for single-case experimental designs. Cogent Psychology, 5(1), 1–26. https://doi.org/10.1080/23311908.2018.1518687\n\n\nCooper, H., Hedges, L. V., & Valentine, J. C. (2009). Handbook of research synthesis and meta-analysis, the. Retrieved from https://www.jstor.org/stable/10.7758/9781610441384\n\n\nFieller, E. C., Hartley, H. O., & Pearson, E. S. (1957). Tests for rank correlation coefficients. Biometrika, 44, 470–481.\n\n\nHedges, L. V., Pustejovsky, J. E., & Shadish, W. R. (2012). A standardized mean difference effect size for single case designs. Research Synthesis Methods, 3(3), 224–239. https://doi.org/10.1002/jrsm.1052\n\n\nHotelling, H. (1953). New Light on the Correlation Coefficient and its Transforms. Journal of the Royal Statistical Society: Series B (Methodological), 15(2), 193–225. https://doi.org/10.1111/j.2517-6161.1953.tb00135.x\n\n\nParker, Richard I., Hagan-Burke, S., & Vannest, K. (2007). Percentage of All Non-Overlapping Data (PAND) An Alternative to PND. The Journal of Special Education, 40(4), 194–204. Retrieved from http://sed.sagepub.com/content/40/4/194.short\n\n\nParker, Richard I., & Vannest, K. (2009). An improved effect size for single-case research: Nonoverlap of all pairs. Behavior Therapy, 40(4), 357–367. Retrieved from http://www.sciencedirect.com/science/article/pii/S0005789408000816\n\n\nParker, Richard I., Vannest, K. J., & Brown, L. (2009). The improvement rate difference for single-case research. Exceptional Children, 75(2), 135150. Retrieved from http://cec.metapress.com/index/35U1148028323H3H.pdf\n\n\nParker, Richard I., Vannest, K. J., & Davis, J. L. (2011a). Effect Size in Single-Case Research: A Review of Nine Nonoverlap Techniques. Behavior Modification, 35(4), 303–322. https://doi.org/10.1177/0145445511399147\n\n\nParker, Richard I., Vannest, K. J., Davis, J. L., & Sauber, S. B. (2011b). Combining Nonoverlap and Trend for Single-Case Research: Tau-U. Behavior Therapy, 42(2), 284–299. https://doi.org/10.1016/j.beth.2010.08.006\n\n\nParker, Richard I., Vannest, K. J., Davis, J. L., & Sauber, S. B. (2011c). Combining nonoverlap and trend for single-case research: Tau-u. Behavior Therapy, 42(2), 284–299. https://doi.org/10.1016/j.beth.2010.08.006\n\n\nPustejovsky, J. E. (2016). What is tau-u? Retrieved from https://www.jepusto.com/what-is-tau-u/\n\n\nPustejovsky, J. E. (2019). Procedural sensitivities of effect sizes for single-case designs with directly observed behavioral outcome measures. Psychological Methods, 24(2), 217–235. https://doi.org/10.1037/met0000179\n\n\nScruggs, T. E., Mastropieri, M. A., & Casto, G. (1987). The Quantitative Synthesis of Single-Subject Research Methodology and Validation. Remedial and Special Education, 8(2), 24–33. https://doi.org/10.1177/074193258700800206\n\n\nSiegel, A. F. (1982). Robust Regression Using Repeated Medians. Biometrika, 69(1), 242–244. https://doi.org/10.2307/2335877\n\n\nTarlow, K. R. (2016). An Improved Rank Correlation Effect Size Statistic for Single-Case Designs: Baseline Corrected Tau. Behavior Modification, 41(4), 427–467. https://doi.org/10.1177/0145445516676750\n\n\nTarlow, K. R. (2017). An Improved Rank Correlation Effect Size Statistic for Single-Case Designs: Baseline Corrected Tau. Behavior Modification, 41(4), 427–467. https://doi.org/10.1177/0145445516676750\n\n\nWise, E. A. (2004). Methods for analyzing psychotherapy outcomes: A review of clinical significance, reliable change, and recommendations for future directions. Journal of Personality Assessment, 82(1), 50–59. Retrieved from http://www.tandfonline.com/doi/abs/10.1207/s15327752jpa8201_10",
    "crumbs": [
      "Data analysis",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Overlapping indices</span>"
    ]
  },
  {
    "objectID": "ch_overlapping_indices.html#footnotes",
    "href": "ch_overlapping_indices.html#footnotes",
    "title": "9  Overlapping indices",
    "section": "",
    "text": "S is the difference between concordant and discordant comparisons in a Kendall’s tau calculation. This is the same statistic used to calculate the p-value.↩︎",
    "crumbs": [
      "Data analysis",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Overlapping indices</span>"
    ]
  },
  {
    "objectID": "ch_piecewise_regression.html",
    "href": "ch_piecewise_regression.html",
    "title": "10  Piecewise linear regressions",
    "section": "",
    "text": "10.1 The basic plm function\nIn a piecewise-regression analysis (sometimes called segmented regression) a dataset is split at a particular break point and the regression parameters (intercept and slopes) are calculated separately for the data before and after the break point. This is done because we assume that there is a qualitative change at the break point that affects the intercept and slope. This approach is well suited to the analysis of single-case data which are from a statistical point of view time-series data segmented into phases. A general model for single-case data based on the piecewise regression approach has been proposed by Huitema and McKean Huitema & Mckean (2000). They refer to two-phase single-case designs with a pre-intervention phase containing some measurements before the start of the intervention (A-phase) and an intervention phase containing measurements starting at the beginning of the intervention and continuing throughout intervention (B-phase).\nIn this model, four parameters predict the outcome at a specific measurement point see 10.1\nscan provides an implementation based on this piecewise-regression approach. Though the original model is extended by several factors:\nThe basic function for applying a regression analyses to a single-case dataset is plm. This function analyses one single-case. In its simplest way, plm takes one argument with an scdf object and it returns a full piecewise-regression analyses.\nplm(exampleAB$Johanna)\n\nPiecewise Regression Analysis\n\nContrast model: W / level = first, slope = first\n\nFitted a gaussian distribution.\nF(3, 16) = 28.69; p = 0.000; R² = 0.843; Adjusted R² = 0.814; AIC = 126.8444\n\n                            B LL-CI95% UL-CI95%    SE      t     p delta R²\nIntercept              54.400   46.776   62.024 3.890 13.986 0.000         \nTrend (mt)              0.100   -3.012    3.212 1.588  0.063 0.951    0.000\nLevel phase B (phaseB)  7.858   -3.542   19.258 5.816  1.351 0.195    0.018\nSlope phase B (interB)  1.525   -1.642    4.692 1.616  0.944 0.359    0.009\n\nAutocorrelations of the residuals\n lag    cr\n   1 -0.32\n   2 -0.13\n   3 -0.01\nLjung-Box test: X²(3) = 2.84; p = 0.417 \n\nFormula: values ~ 1 + mt + phaseB + interB\nThe output shows the specific contrast settings for the phase and slope calculation (see below for a detailed explanation). Next, the overall model fit is provided. In this specific example the model fit is quite high explaining more than 80% of the variance of the dependent variable. The following regression table shows the unstandardised estimates (B), the lower and upper boundaries of a 95% confidence interval, the standard-error, the t-test statistic, the p-value, and the delta R squared for each predictor.\nIn this example, the intercept is the score estimation at the beginning of the study (here: at the first session, see Figure 10.2). The trend effect (variable mt) is 0.100. This means that for every one point increase in measurement-time, the score increased by 0.100. As this example has a total of 20 sessions, the overall increase due to the trend is \\(19 * 0.100 = 1.9\\) points. The level effect (variable phase with level B) is 7.858. This means that all scores of phase B are increased by 7.858 points. The slope effect (for phase B) is 1.525. This means that for every one point increase in measurement time after the first phase B session, the score increases by 1.525. As there are 15 Phase B sessions in this example, the total increase due to the slope effect is \\(14 * 1.525 = 21.35\\) points.\nFrom these values each data point can be estimated. For example, the last session (\\(i=20\\)) is estimated to be \\(54.400 + 19 * 0.100 + 7.858 + 14 * 1.525 = 85.508\\).\nFigure 10.2: Example dataset",
    "crumbs": [
      "Data analysis",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Piecewise linear regressions</span>"
    ]
  },
  {
    "objectID": "ch_piecewise_regression.html#the-basic-plm-function",
    "href": "ch_piecewise_regression.html#the-basic-plm-function",
    "title": "10  Piecewise linear regressions",
    "section": "",
    "text": "The plm function call:\n\n\n\nplm(\n  data,\n  dvar,\n  pvar,\n  mvar,\n  AR = 0,\n  model = c(“W”, “H-M”, “B&L-B”, “JW”),\n  family = “gaussian”,\n  trend = TRUE,\n  level = TRUE,\n  slope = TRUE,\n  contrast = c(“first”, “preceding”),\n  contrast_level = c(NA, “first”, “preceding”),\n  contrast_slope = c(NA, “first”, “preceding”),\n  formula = NULL,\n  update = NULL,\n  na.action = na.omit,\n  r_squared = TRUE,\n  var_trials = NULL,\n  dvar_percentage = FALSE,\n  …\n)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nAs a convenience, the predictors are renamed according to the respective effect within the single-case terminology that they represent.\nIf you prefer to use the original variable names, set the following option: options(scan.rename.predictors = \"no\").\nIf you want more concise renamed predictors, set: options(scan.rename.predictors = \"concise\").\nYou can restore the default renaming by setting: options(scan.rename.predictors = \"full\").",
    "crumbs": [
      "Data analysis",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Piecewise linear regressions</span>"
    ]
  },
  {
    "objectID": "ch_piecewise_regression.html#autocorrelation-of-the-residuals",
    "href": "ch_piecewise_regression.html#autocorrelation-of-the-residuals",
    "title": "10  Piecewise linear regressions",
    "section": "10.2 Autocorrelation of the residuals",
    "text": "10.2 Autocorrelation of the residuals\nThe output also reports the autocorrelations of the residuals, which indicate whether the models residuals are serially independent.\nThere are many reasons why this might happen. For example, in a learning study, performance may temporarily decline after a learning session (performance dip). Similarly, in a medication study, a drug might initially cause a brief worsening of symptoms before exerting its intended therapeutic effect (early symptom exacerbation effect).\nThe lag refers to the number of measurement points between observations that show autocorrelation.\nThe Ljung-Box test is an omnibus test that evaluates whether the residuals exhibit significant autocorrelation at multiple lags. It assesses whether the joint distribution of autocorrelations differs significantly from what would be expected under the assumption of independence.\nHigh and significant autocorrelations pose a threat to the validity of standard regression models, as they violate the assumption of independent residuals. To account for this, you can set the AR argument with the appropriate maximum lag (e.g., AR = 3). This implements an ARMA (autoregressive moving average) model, which takes the serial correlation in the residuals into account.",
    "crumbs": [
      "Data analysis",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Piecewise linear regressions</span>"
    ]
  },
  {
    "objectID": "ch_piecewise_regression.html#sec-rescale",
    "href": "ch_piecewise_regression.html#sec-rescale",
    "title": "10  Piecewise linear regressions",
    "section": "10.3 Standardizing predictors",
    "text": "10.3 Standardizing predictors\nIf you want standardized predictors, meaning that predictors are scaled to standard deviations, the best approach is to standardize all variables before computing the regression model. Use the rescale() function to do this.\n\nexampleAB$Johanna |&gt; \n  rescale() |&gt; \n  plm()\n\nRescaled values, mt\n\n\nPiecewise Regression Analysis\n\nContrast model: W / level = first, slope = first\n\nFitted a gaussian distribution.\nF(3, 16) = 28.69; p = 0.000; R² = 0.843; Adjusted R² = 0.814; AIC = 28.67075\n\n                            B LL-CI95% UL-CI95%    SE      t     p delta R²\nIntercept              -1.276   -1.931   -0.621 0.334 -3.818 0.002         \nTrend (mt)              0.051   -1.531    1.633 0.807  0.063 0.951    0.000\nLevel phase B (phaseB)  0.675   -0.304    1.655 0.500  1.351 0.195    0.018\nSlope phase B (interB)  0.775   -0.835    2.385 0.821  0.944 0.359    0.009\n\nAutocorrelations of the residuals\n lag    cr\n   1 -0.32\n   2 -0.13\n   3 -0.01\nLjung-Box test: X²(3) = 2.84; p = 0.417 \n\nFormula: values ~ 1 + mt + phaseB + interB \n\n\nThe regression coefficients B are now standardized Beta (ß) coefficients, indicating the expected change (in standard deviations) of the dependent variable for a one-standard-deviation increase in the predictor.",
    "crumbs": [
      "Data analysis",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Piecewise linear regressions</span>"
    ]
  },
  {
    "objectID": "ch_piecewise_regression.html#adjusting-the-model",
    "href": "ch_piecewise_regression.html#adjusting-the-model",
    "title": "10  Piecewise linear regressions",
    "section": "10.4 Adjusting the model",
    "text": "10.4 Adjusting the model\nThe plm model is a complex model specifically suited for single-case studies. It entails a series of important parameters. Nevertheless, often we have specific theoretical assumption that do no include some of these parameters. We might, for example, only expect an immediate but not a continuous change from a medical intervention. Therefore, it would not be useful to include the slope-effect into our modelling. Vice versa, we could investigate an intervention that will just develop across time without an immediate change with the intervention start. Here we should drop the level-effect from out model. Even the assumption of a trend-effect can be dropped in cases where we do not expect a serial dependency of the data and we do not assume intervention independent changes within the time-frame of the study.\nIt is important to keep in mind, that an overly complex model might have negative effects on the test power of an analyses (that is, the probability of detecting an actually existing effect is diminished) (see Wilbert, Lüke, & Börnert-Ringleb, 2022)\n\n10.4.1 The slope, level, and trend arguments\n\nThe plm function comes with three arguments (slope, level, and trend) to include or drop the respective predictors from the plm model. Buy default, all arguments are set TRUE and a full plm model is applied to the data.\nConsider the following data example:\n\nexample &lt;- scdf(\n   values = c(A = 55, 58, 53, 50, 52, \n              B = 55, 68, 68, 81, 67, 78, 73, 72, 78, 81, 78, 71, 85, 80, 76)\n)\n\nplm(example)\n\nPiecewise Regression Analysis\n\nContrast model: W / level = first, slope = first\n\nFitted a gaussian distribution.\nF(3, 16) = 21.36; p = 0.000; R² = 0.800; Adjusted R² = 0.763; AIC = 130.3907\n\n                            B LL-CI95% UL-CI95%    SE      t     p delta R²\nIntercept              56.400   48.070   64.730 4.250 13.270 0.000         \nTrend (mt)             -1.400   -4.801    2.001 1.735 -0.807 0.432    0.008\nLevel phase B (phaseB) 16.967    4.510   29.424 6.356  2.670 0.017    0.089\nSlope phase B (interB)  2.500   -0.961    5.961 1.766  1.416 0.176    0.025\n\nAutocorrelations of the residuals\n lag    cr\n   1 -0.28\n   2  0.05\n   3 -0.11\nLjung-Box test: X²(3) = 2.14; p = 0.543 \n\nFormula: values ~ 1 + mt + phaseB + interB \n\n\nThe piecewise regression reveals a significant level effect and two non significant effects for trend and slope. In a further analyses we would like to put the slope effect out of the equation. The easiest way to do this is to set the slope argument to FALSE.\n\nplm(example, slope = FALSE)\n\nPiecewise Regression Analysis\n\nContrast model: W / level = first, slope = first\n\nFitted a gaussian distribution.\nF(2, 17) = 29.30; p = 0.000; R² = 0.775; Adjusted R² = 0.749; AIC = 130.7511\n\n                            B LL-CI95% UL-CI95%    SE      t     p delta R²\nIntercept              51.572   46.455   56.690 2.611 19.752 0.000         \nTrend (mt)              1.014    0.364    1.664 0.332  3.057 0.007    0.124\nLevel phase B (phaseB) 10.329    1.674   18.983 4.416  2.339 0.032    0.072\n\nAutocorrelations of the residuals\n lag    cr\n   1 -0.07\n   2  0.06\n   3 -0.17\nLjung-Box test: X²(3) = 0.99; p = 0.804 \n\nFormula: values ~ 1 + mt + phaseB \n\n\nIn the resulting estimations the trend and level effects are now significant. The model estimated a trend effect of 1.01 points for every one point increase in measurement-time and a level effect of 10.33 points. That is, with the beginning of the intervention (the B-phase) the score increases by \\(5 x 1.01 + 10.33 = 15.38\\) points (the measurement-time is increases by five, from one to six, at the first session of phase B).",
    "crumbs": [
      "Data analysis",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Piecewise linear regressions</span>"
    ]
  },
  {
    "objectID": "ch_piecewise_regression.html#adding-additional-predictors",
    "href": "ch_piecewise_regression.html#adding-additional-predictors",
    "title": "10  Piecewise linear regressions",
    "section": "10.5 Adding additional predictors",
    "text": "10.5 Adding additional predictors\nIn more complex analyses, additional predictors can be included in the piecewise regression model.\nTo do this, we have to change the regression formula ‘manually’ by applying the update argument. The update argument allows to change the underlying regression formula. To add a new variable named for example newVar, set update = .~. + newVar. The .~. part takes the internally build model formula (based on the number of phases in your model and the setting of the slope, level, and trend arguments) and + newVar adds a variable called newVar to the equation.\nHere is an example adding the control variable cigarrets to the model:\n\nplm(exampleAB_add, update = .~. + cigarrets)\n\nPiecewise Regression Analysis\n\nContrast model: W / level = first, slope = first\n\nFitted a gaussian distribution.\nF(4, 35) = 5.87; p = 0.001; R² = 0.402; Adjusted R² = 0.333; AIC = 252.1972\n\n                      B LL-CI95% UL-CI95%    SE      t     p delta R²\nIntercept        48.971   43.387   54.555 2.849 17.189 0.000         \nTrend             0.392   -0.221    1.005 0.313  1.253 0.218    0.027\nLevel Medication  3.459   -3.382   10.301 3.490  0.991 0.328    0.017\nSlope Medication -0.294   -0.972    0.384 0.346 -0.850 0.401    0.012\ncigarrets        -0.221   -1.197    0.755 0.498 -0.443 0.660    0.003\n\nAutocorrelations of the residuals\n lag    cr\n   1  0.20\n   2 -0.19\n   3 -0.16\nLjung-Box test: X²(3) = 4.33; p = 0.228 \n\nFormula: wellbeing ~ day + phaseMedication + interMedication + cigarrets \n\n\nThe output of the plm-function shows the resulting formula for the regression model that includes the cigarettes variable:\nFormula: wellbeing ~ day + phaseMedication + interMedication + cigarrets",
    "crumbs": [
      "Data analysis",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Piecewise linear regressions</span>"
    ]
  },
  {
    "objectID": "ch_piecewise_regression.html#frequencies-and-proportions",
    "href": "ch_piecewise_regression.html#frequencies-and-proportions",
    "title": "10  Piecewise linear regressions",
    "section": "10.6 Frequencies and proportions",
    "text": "10.6 Frequencies and proportions\nWhen the dependent variable is a frequency, the assumption of a normal distribution is not appropriate. This section will demonstrate how to handle such cases using the plm function.\nWe can distinguish between a frequency without a clearly defined limit, such as the number of lightning strikes per year in a given area or the number of seizures a patient experiences in a week, and a frequency that represents a proportion, such as the number of correctly solved tasks in a test. In these cases, the dependent variable is discrete (only cardinal numbers are possible) and cannot be lower than zero. Both of these properties violate the assumptions of a Gaussian (normal) distribution.\n\n10.6.1 Bounded frequencies\nFrequencies that have a clear reference, such as “10 out of 20,” follow a binomial distribution. In these cases, set the family argument of the plm function to family = \"binomial\".\nAdditionally, you must specify the maximum value (the “out of” part). You can either use a distinct variable in your scdf that defines the maximum separately for each measurement point or specify a constant value for all measurements. In both cases, you need to set the var_trials argument either to the name of the variable containing the maximum values or to a numeric value representing a fixed maximum.\n\nplm(exampleAB_score$Christiano , family = \"binomial\", var_trials = \"trials\")\n\nPiecewise Regression Analysis\n\nContrast model: W / level = first, slope = first\n\nFitted a binomial distribution.\nX²(3) = 240.66; p = 0.000; AIC = 120.3268\n\n               B LL-CI95% UL-CI95%    SE      t     p     OR  LL-CI95%\nIntercept -1.964   -2.793   -1.239 0.394 -4.991 0.000  0.140     0.061\nTrend      0.023   -0.118    0.166 0.072  0.324 0.746  1.023     0.889\nLevel B    2.376    1.454    3.378 0.488  4.866 0.000 10.762     4.280\nSlope B    0.038   -0.111    0.186 0.075  0.504 0.614  1.039     0.895\n           UL-CI95%\nIntercept     0.290\nTrend         1.181\nLevel B      29.312\nSlope B       1.204\n\nFormula: values/trials ~ 1 + mt + phaseB + interB\nweights = trials \n\n# Or equivalent as all values in trial are 20:\n# plm(exampleAB_score$Christiano , family = \"binomial\", var_trials = 20)\n\nThe output shows the estimators B, which are on a logit scale, and odds or odds ratios (OR), which are the exponentiated logits, along with their respective confidence intervals. Logits are linear, meaning that the B coefficients can be added to obtain the combined effect of multiple predictors. For the example above, the logit of the intercept is -1.964, corresponding to a probability of approximately 12% (for details on the calculation, see note below). The level effect is \\(B = 2.376\\). This means that at the start of Phase B, the logit increases to \\(B = -1.964 + 2.376 = 0.412\\), which corresponds to a probability of approximately 60%.\nOdds ratios are not linear and must be multiplied to obtain a combined effect. In this example, the odds of the intercept is 0.14, calculated as \\(e^{-1.964}=0.14\\). For the level effect, the odds ratio (“ratio” because it indicates the change of the odds) is 10.762. The combined effect (intecept and level) gives the odds 0.14 * 10.762 = 1.50668. Converting back to the logit scale, this is \\(\\log(1.50668)\\approx 0.41\\). This result, aside from minor rounding differences, matches the value obtained by summing the respective B coefficients.\nKeep in mind that, as in Gaussian regression models, the p-value for the intercept tests the null hypothesis that the true intercept is zero. In other words, it represents the probability of obtaining an intercept estimate as extreme as the one observed if the true intercept were indeed zero (i.e., the probability of a Type I error under this null hypothesis). However, in logistic regression, an intercept (or logit) of zero does not correspond to a probability of 0%; it corresponds to a probability of 50%, because \\(e^{0}= Odds{1\\over1}=50\\%\\).\n\n\n\n\n\n\nWhat are Logits and Odds Ratios?\n\n\n\nLogits are the logarithm of the odds: \\(log(Odds)\\). Odds represent the probability of an event occurring relative to it not occurring. If the odds are 4 to 1 (\\(4\\over1\\)), this means that out of 5 total events, 4 result in one category of outcome, and 1 results in the opposite outcome.\nFor example, if a horse runs five races and we expect it to win four and lose one, the odds of winning are 4 to 1 (\\(4\\over1\\)).\nIf the odds are less than 1, the probability of the event occurring is below 50%. For instance, if the odds are 0.25 (\\(1\\over4\\)), we expect the horse to win one out of five races.\nDifference Between Probabilities and Odds\nProbabilities and odds are related but not identical. A probability of \\(1\\over4\\) (0.25) means that the specific outcome occurs in 1 out of 4 events. Odds of \\(1\\over4\\) means that the probability of two possible outcomes is 1 against 4, meaning the event occurs 1 time for every 4 non-occurrences. In this case, odds of \\(1\\over4\\) correspond to a probability of \\(1\\over5\\) = 0.20.\nFormula\nThe odds are calculated as:\n\\[Odds(y_i)=\\frac{P(y_i=1)}{1-P(y_i=0)}\\]\nFor example, if we have a probability of 25%, the odds are:\n\\[Odds=\\frac{0.25}{0.75}={1\\over3}\\]\nThe formula for calculating logits is:\n\\[logit(y_i)=log(\\frac{P(y_i=1)}{1-P(y_i=0)})=log(odds)\\]\nFor example, if we have a 25% probability that is:\n\\[logit(P=0.25)=log(\\frac{0.25}{0.75})=log({1\\over3})=-1.098612\\]\nFor calculating probabilities from logits we use:\n\\[P = \\frac{e^{\\text{logit}(B)}}{1 + e^{\\text{logit}(B)}}\\]\nFor example, a logit of B = -1.964 is approxomately a 12% probability:\n\\[P = \\frac{e^{-1.964}}{1 + e^{-1.964}}\\approx\\frac{0.14}{1.14}\\approx{0.12}\\]\n\n\n\n\n10.6.2 Proportions\nProportions are actually just a mathematical identical presentation of bounded frequencies (e.g. 10 out of 20 is identical to 0.5 from 20). When your dependent variable represents proportions, set family to a binomial distribution family = \"binomial\". Again, you have to provide a variable or a constant with the total number of trials with the var_trials argument. Finally, you have to set the dvar_percentage argument to TRUE to indicate that your dependent variable contains proportions.\nHere is the previous example where the the frequencies are transformed to proportions. The results are identical to the calculation with frequencies.\n\nscdf &lt;- exampleAB_score$Christiano |&gt;\n  transform(proportions = values/trials) |&gt;\n  set_dvar(\"proportions\")\n\nplm(scdf, family = \"binomial\", var_trials = \"trials\", dvar_percentage = TRUE)\n\nPiecewise Regression Analysis\n\nContrast model: W / level = first, slope = first\n\nFitted a binomial distribution.\nX²(3) = 240.66; p = 0.000; AIC = 120.3268\n\n               B LL-CI95% UL-CI95%    SE      t     p     OR  LL-CI95%\nIntercept -1.964   -2.793   -1.239 0.394 -4.991 0.000  0.140     0.061\nTrend      0.023   -0.118    0.166 0.072  0.324 0.746  1.023     0.889\nLevel B    2.376    1.454    3.378 0.488  4.866 0.000 10.762     4.280\nSlope B    0.038   -0.111    0.186 0.075  0.504 0.614  1.039     0.895\n           UL-CI95%\nIntercept     0.290\nTrend         1.181\nLevel B      29.312\nSlope B       1.204\n\nFormula: proportions ~ 1 + mt + phaseB + interB\nweights = trials \n\n\n\n\n\n\n\n\nPercentages\n\n\n\nWhen you have percentages instead of proportions as the dependent variable, divide these by 100 with the transform function to get proportions.\n\nscdf &lt;- transform(scdf, values = values / 100)\n\n\n\n\n10.6.3 Dichotomous outcomes\nWhen the dependent variable is dichotomous (e.g., 0 or 1; yes or no), we can estimate a logistic regression. Logistic regression is essentially a special case of binomial regression, applied under the “extreme” condition that there is only one trial per measurement—so that each observation results in either 0% (failure) or 100% (success).\nHere is an example dataset with a respective analysis:\n\n\n\n\n\n\n\n\n\n\nplm(example_dichotomous, family = \"binomial\", var_trials = 1)\n\nPiecewise Regression Analysis\n\nContrast model: W / level = first, slope = first\n\nFitted a binomial distribution.\nX²(3) = 5.91; p = 0.116; AIC = 205.503\n\n               B LL-CI95% UL-CI95%    SE      t     p    OR  LL-CI95%  UL-CI95%\nIntercept -0.174   -1.356    0.972 0.584 -0.298 0.766 0.840     0.258     2.643\nTrend     -0.029   -0.075    0.014 0.022 -1.298 0.194 0.971     0.928     1.014\nLevel B    1.390   -0.114    3.067 0.801  1.734 0.083 4.015     0.892    21.477\nSlope B    0.031   -0.014    0.079 0.023  1.333 0.183 1.031     0.986     1.082\n\nFormula: values/1 ~ 1 + mt + phaseB + interB\nweights = 1 \n\n\n\n\n10.6.4 Unbounded frequencies\nIn many cases, frequency data are collected without a predetermined upper limit. For example, in a study measuring the frequency of inappropriate classroom behavior, researchers may observe lessons and count the number of disturbances. Although there may be a practical upper limit on the number of possible disturbances, the theoretical limit is infinite.\nAnother example is a study counting the number of lorries driving on a specific street. Here too, the highest possible number is undefined.\nFor this type of data, a Poisson distribution is more appropriate. To specify a Poisson model, set the family argument in the plm() function to \"poisson\".\nThe following example shows a dataset of injured people due to an accident on an autobahn before and after implementing a speed limit:\n\n\n\n\n\n\n\n\nTable\n\n\nSingle case data frame for case A24\n\n\ninjuries\nyear\nphase\n\n\n\n\n239\n1996\nA\n\n\n263\n1997\nA\n\n\n264\n1998\nA\n\n\n277\n1999\nA\n\n\n283\n2000\nA\n\n\n296\n2001\nA\n\n\n228\n2002\nA\n\n\n136\n2003\nB\n\n\n133\n2004\nB\n\n\n106\n2005\nB\n\n\n123\n2006\nB\n\n\n96\n2007\nB\n\n\n97\n2008\nB\n\n\n103\n2009\nB\n\n\n138\n2010\nB\n\n\n129\n2011\nB\n\n\n136\n2012\nB\n\n\n142\n2013\nB\n\n\n113\n2014\nB\n\n\n177\n2015\nB\n\n\n151\n2016\nB\n\n\n106\n2017\nB\n\n\n71\n2018\nB\n\n\n\nNote. Number of injuries on a German autobahn before and after implementation of a speedlimit (130km/h). Author: Ministerium fuer Infrastruktur und Landesplanung. Land Brandenburg..\n\n\n\n\n\n\n\n\nHere, a Possion regression is applied:\n\nplm(example_A24, family = \"poisson\")\n\nPiecewise Regression Analysis\n\nContrast model: W / level = first, slope = first\n\nFitted a poisson distribution.\nX²(3) = 547.67; p = 0.000; AIC = 261.2856\n\n               B LL-CI95% UL-CI95%    SE       t     p      OR  LL-CI95%\nIntercept  5.556    5.472    5.638 0.042 131.693 0.000 258.786   237.936\nTrend      0.007   -0.016    0.030 0.012   0.604 0.546   1.007     0.984\nLevel B   -0.806   -0.938   -0.674 0.067 -11.964 0.000   0.447     0.391\nSlope B   -0.006   -0.031    0.019 0.013  -0.472 0.637   0.994     0.969\n           UL-CI95%\nIntercept   280.900\nTrend         1.030\nLevel B       0.510\nSlope B       1.019\n\nFormula: injuries ~ 1 + year + phaseB + interB",
    "crumbs": [
      "Data analysis",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Piecewise linear regressions</span>"
    ]
  },
  {
    "objectID": "ch_piecewise_regression.html#sec-anova",
    "href": "ch_piecewise_regression.html#sec-anova",
    "title": "10  Piecewise linear regressions",
    "section": "10.7 Model comparison (since version 0.64.0)",
    "text": "10.7 Model comparison (since version 0.64.0)\n\n\n\n\n\n\nExperimental\n\n\n\nThis feature is still in an experimental state. This means that the syntax structure, arguments, and defaults may change in future scan versions and may not be backward compatible.\n\n\n\n\n\n\n\n\nThe anova function call:\n\n\n\nanova(object, …)\n\n\nYou can compare the fit of piecewise linear regression (plm) models using the anova() function. This allows for stepwise regression analyses and targeted comparisons (e.g., assessing the combined effect of slope and level changes).\nLet’s consider an example where we begin with a null model that includes only an intercept but no additional predictors. We then sequentially add a level effect, followed by a trend effect, and finally a slope effect.\n\nmod0 &lt;- plm(exampleAB$Anja, trend = FALSE, level = FALSE, slope = FALSE)\nmod1 &lt;- plm(exampleAB$Anja, trend = FALSE, level = TRUE, slope = FALSE)\nmod2 &lt;- plm(exampleAB$Anja, trend = TRUE, level = TRUE, slope = FALSE)\nmod3 &lt;- plm(exampleAB$Anja, trend = TRUE, level = TRUE, slope = TRUE)\n\nWe start by comparing the null model to the first model:\n\nanova(mod0, mod1)\n\nAnalysis of Deviance Table\n\nModel 1: values ~ 1\nModel 2: values ~ 1 + phaseB\n  Resid. Df Resid. Dev Df Deviance      F    Pr(&gt;F)    \n1        19    2410.95                                 \n2        18     840.13  1   1570.8 33.655 1.697e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nThe output provides the residuals degrees of freedom (Df), the deviance of the residuals, and \\(\\Delta Df\\) and \\(\\Delta \\text{Deviance}\\), which are the difference of the degrees of freedom and the residual deviance of the respective model and the preceding model (here: \\(\\Delta \\text{Df}=19 - 18 = 1\\) and \\(\\Delta \\text{Deviance}=2410.95 - 840.13 = 1570.8\\)). Deviance is a measure of a model’s goodness of fit and is calculated as \\(-2 \\times \\text{Likelihood}\\) . The test compares the difference in degrees of freedom (\\(\\Delta Df = 1\\)) and deviance (\\(\\Delta \\text{Deviance} = 1570.8\\)) between the two models.\nSince both models assume Gaussian-distributed data, an F-test is performed. The F-value is computed as:\n\\[\nF = \\frac{\\text{Explained variance}}{\\text{Residual variance}} = \\frac{\\Delta \\text{Deviance}}{\\text{Residual deviance} / Df} = \\frac{1570.8}{\\frac{840.13}{18}} = \\frac{1570.8}{46.67389} = 33.655\n\\]\nFor \\(F(1, 18) = 33.655\\), the p-value can be obtained with:\n\npf(33.655,1,18, lower.tail = FALSE)\n\n[1] 1.697306e-05\n\n\nAlternatively, we can conduct a likelihood-ratio test by setting the argument test = \"LRT\":\n\nanova(mod0, mod1, test = \"LRT\")\n\nAnalysis of Deviance Table\n\nModel 1: values ~ 1\nModel 2: values ~ 1 + phaseB\n  Resid. Df Resid. Dev Df Deviance  Pr(&gt;Chi)    \n1        19    2410.95                          \n2        18     840.13  1   1570.8 6.581e-09 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nThis computes a \\(\\chi^2\\) test for \\(\\frac{\\Delta \\text{Deviance}}{\\text{Dispersion}} = 33.655\\) with \\(\\Delta Df = 1\\), where the p-value is given by:\n\npchisq(33.655, 1, lower.tail = FALSE)\n\n[1] 6.580551e-09\n\n\nWe can also compare all four models as incrementally nested models in a single step:\n\nanova(mod0, mod1, mod2, mod3)\n\nAnalysis of Deviance Table\n\nModel 1: values ~ 1\nModel 2: values ~ 1 + phaseB\nModel 3: values ~ 1 + mt + phaseB\nModel 4: values ~ 1 + mt + phaseB + interB\n  Resid. Df Resid. Dev Df Deviance       F    Pr(&gt;F)    \n1        19    2410.95                                  \n2        18     840.13  1  1570.82 52.1722 2.033e-06 ***\n3        17     542.08  1   298.06  9.8994  0.006243 ** \n4        16     481.73  1    60.34  2.0043  0.176029    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nEach model is compared to its predecessor. Since all models are nested and derived from the same data, the dispersion parameter for all comparisons is taken from the most complex model, here mod3 (dispersion = 30.11).\nIn this analysis, we observe that the phase predictor introduces the strongest improvement in model fit, whereas the slope predictor does not significantly enhance the model.",
    "crumbs": [
      "Data analysis",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Piecewise linear regressions</span>"
    ]
  },
  {
    "objectID": "ch_piecewise_regression.html#sec-dummy-model",
    "href": "ch_piecewise_regression.html#sec-dummy-model",
    "title": "10  Piecewise linear regressions",
    "section": "10.8 Dummy models",
    "text": "10.8 Dummy models\nThe model argument is used to code the dummy variables. These dummy variables are used to compute the slope and level effects of the phase variable.\nThe phase variable is categorical, identifying the phase of each measurement. Typically, categorical variables are implemented by means of dummy variables. In a piecewise regression model two phase effects have to be estimated: a level effect and a slope effect. The level effect is implemented quite straight forward: for each phase beginning with the second phase a new dummy variable is created with values of zero for all measurements except the measurements of the phase in focus where values of one are set.\n\n\n\n\n\nvalues\nphase\nmt\nlevel B\n\n\n\n\n3\nA\n1\n0\n\n\n6\nA\n2\n0\n\n\n4\nA\n3\n0\n\n\n7\nA\n4\n0\n\n\n5\nB\n5\n1\n\n\n3\nB\n6\n1\n\n\n4\nB\n7\n1\n\n\n6\nB\n8\n1\n\n\n3\nB\n9\n1\n\n\n\n\n\nFor estimating the slope effect of each phase, another kind of dummy variables have to be created. Like the dummy variables for level effects the values are set to zero for all measurements except the ones of the phase in focus. Here, values start to increase with every measurement until the end of the phase.\nVarious suggestions have been made regarding the way in which these values increase (see Huitema & Mckean, 2000). The B&L-B model starts with a one at the first session of the phase and increases with every session while the H-M model starts with a zero.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nslope B\n\n\n\nvalues\nphase\nmt\nlevel B\nmodel B&L-M\nmodel H-M\n\n\n\n\n3\nA\n1\n0\n0\n0\n\n\n6\nA\n2\n0\n0\n0\n\n\n4\nA\n3\n0\n0\n0\n\n\n7\nA\n4\n0\n0\n0\n\n\n5\nB\n5\n1\n1\n0\n\n\n3\nB\n6\n1\n2\n1\n\n\n4\nB\n7\n1\n3\n2\n\n\n6\nB\n8\n1\n4\n3\n\n\n3\nB\n9\n1\n5\n4\n\n\n\n\n\nApplying the H-M model will give you a “pure” level-effect while the B&L-B model will provide an estimation of the level-effect that is actually the level-effect plus on times the slope-effect (as the model assumes that the slope variable is 1 at the first measurement of the B-phase). For most studies, the H-M model is more appropriate.\nHowever, there is another aspect to be aware of. Usually, in single case designs, the measurement times are coded as starting with 1 and increasing in integers (1, 2, 3, …). At the same time, the estimation of the trend effect is based on the measurement time variable. In this case, the estimate of the model intercept (usually interpreted as the value at the beginning of the study) actually represents the estimate of the starting value plus one times the trend effect. Therefore, I have implemented the W model (since scan version 0.54.4). Here the trend effect is estimated for a time variable shifted to start at 0. As a result, the intercept represents the estimated value at the first measurement of the study. The W model treats slope estimation in the same way as the H-M model. That is, the measurement-time for calculating the slope effect is set to 0 for the first session of each phase. Since scan version 0.54.4 the W model is the default.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nmt\n\n\nslope\n\n\n\nvalues\nphase\nlevel\nB&L-M and H-M\nW\nB&L-M\nH-M and W\n\n\n\n\n3\nA\n0\n1\n0\n0\n0\n\n\n6\nA\n0\n2\n1\n0\n0\n\n\n4\nA\n0\n3\n2\n0\n0\n\n\n7\nA\n0\n4\n3\n0\n0\n\n\n5\nB\n1\n5\n4\n1\n0\n\n\n3\nB\n1\n6\n5\n2\n1\n\n\n4\nB\n1\n7\n6\n3\n2\n\n\n6\nB\n1\n8\n7\n4\n3\n\n\n3\nB\n1\n9\n8\n5\n4",
    "crumbs": [
      "Data analysis",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Piecewise linear regressions</span>"
    ]
  },
  {
    "objectID": "ch_piecewise_regression.html#designs-with-more-than-two-phases-setting-the-right-contrasts",
    "href": "ch_piecewise_regression.html#designs-with-more-than-two-phases-setting-the-right-contrasts",
    "title": "10  Piecewise linear regressions",
    "section": "10.9 Designs with more than two phases: Setting the right contrasts",
    "text": "10.9 Designs with more than two phases: Setting the right contrasts\nFor single case studies with more than two phases, things get a bit more complicated. Applying the models described above to three phases would result in a comparison between each phase and the first phase (usually phase A). That is, the regression weights and significance tests indicate the differences between each phase and the values from phase A. Another common use is to compare the effects of a phase with the previous phase.\nAs of scan version 0.54.4, plm allows to set a contrast argument. contrast = \"first\" (the default) will compare all slope and level-effects to the values in the first phase. contrast = \"preceding\" will compare the slope and level-effects to the preceding phase.\nFor the preceding contrast, the dummy variable for the level-effect is set to zero for all phases preceding the phase in focus and set to one for all remaining measurements. Similarly, the dummy variable for the slope-effect is set to zero for all phases preceding the phase in focus and starts at zero (or one, depending on the model setting, see Section 10.8) for the first measurement of the target phase and increases until the last measurement of the case.\nYou can set the contrast differently for the level and slope effects with the arguments constrast_level and contrast_slope. Both can be either \"first\" or \"preceding\".\n(Note: Prior to scan version 0.54.4, the option model = \"JW\" was identical to model = \"B&L-B\", contrast = \"preceding\").\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ncontrast\nfirst\n\n\ncontrast\npreceeding\n\n\n\n\n\nlevel\n\n\nslope\n\n\nlevel\n\n\nslope\n\n\n\nvalues\nphase\nmt\nB\nC\nB\nC\nB\nC\nB\nC\n\n\n\n\n3\nA\n1\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n6\nA\n2\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n4\nA\n3\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n7\nA\n4\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n5\nB\n5\n1\n0\n1\n0\n1\n0\n1\n0\n\n\n3\nB\n6\n1\n0\n2\n0\n1\n0\n2\n0\n\n\n4\nB\n7\n1\n0\n3\n0\n1\n0\n3\n0\n\n\n6\nB\n8\n1\n0\n4\n0\n1\n0\n4\n0\n\n\n3\nB\n9\n1\n0\n5\n0\n1\n0\n5\n0\n\n\n7\nC\n10\n0\n1\n0\n1\n1\n1\n6\n1\n\n\n5\nC\n11\n0\n1\n0\n2\n1\n1\n7\n2\n\n\n6\nC\n12\n0\n1\n0\n3\n1\n1\n8\n3\n\n\n4\nC\n13\n0\n1\n0\n4\n1\n1\n9\n4\n\n\n8\nC\n14\n0\n1\n0\n5\n1\n1\n10\n5",
    "crumbs": [
      "Data analysis",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Piecewise linear regressions</span>"
    ]
  },
  {
    "objectID": "ch_piecewise_regression.html#understanding-and-interpreting-contrasts",
    "href": "ch_piecewise_regression.html#understanding-and-interpreting-contrasts",
    "title": "10  Piecewise linear regressions",
    "section": "10.10 Understanding and interpreting contrasts",
    "text": "10.10 Understanding and interpreting contrasts\nIn this section, we will calculate four plm models with different contrast settings for the same single-case data.\nThe example scdf is the case ‘Marie’ from the exampleABC scdf (exampleABC$Marie)\n\n\n\n\n\n\n\n\nFigure 10.3: Example dataset\n\n\n\n\n\nThe dark-red lines indicate the intercept and slopes when calculated separately for each phase. They are:\n\n\n\n\nTable 10.1: Intercept, slope, and number of measurements calculated separately for each phase\n\n\n\n\n\n\n\nintercept\nslope\nn\n\n\n\n\nphase A\n60.618\n-1.915\n10\n\n\nphase B\n74.855\n-0.612\n10\n\n\nphase C\n68.873\n-0.194\n10\n\n\n\n\n\n\n\n\nNow we estimate a plm model with four contrast settings (see Table 10.2):\n\n\n\n\nTable 10.2: Estimates of a piecewise-linear regression with contrast models “first” and “preceding”.\n\n\n\n\n\n\nContrast level\nContrast slope\nintercept\ntrend\nlevel B\nlevel C\nslope B\nslope C\n\n\n\n\nfirst\nfirst\n60.618\n-1.915\n33.388\n46.558\n1.303\n1.721\n\n\npreceding\npreceding\n60.618\n-1.915\n33.388\n0.139\n1.303\n0.418\n\n\nfirst\npreceding\n60.618\n-1.915\n33.388\n33.527\n1.303\n0.418\n\n\npreceding\nfirst\n60.618\n-1.915\n33.388\n13.170\n1.303\n1.721\n\n\n\n\n\n\n\n\n\n10.10.1 Phase B estimates\nAll regression models in Table 10.2 have the same estimates for intercept and trend. These are not affected by the contrasts and are identical to those for phase A in Table 10.1. In addition, in Table 10.2, the estimates for levelB and slopeB are identical since all models contrast the same phase (the first and the preceding phase are both phase A). The values here can be calculated from Table 10.12:\n\\[\nlevelB = intercept_{phaseB} - (intercept_{phaseA} + n_{PhaseA} * slope_{phaseA})\n\\tag{10.1}\\]\n\\[\n33.388 \\approx  74.855 - (60.618 + 10*-1.915)\n\\]\n\\[\nslopeB = slope_{phaseB} - slope_{phaseA}\n\\tag{10.2}\\]\n\\[\n1.303 \\approx -1.915 - (-0.612)\n\\]\n\n\n10.10.2 Phase C estimates\nThe levelC and slopeC estimates of the regression models in Table 10.2 are different for the various contrast models. Depending on the contrast setting, the estimates “answer” a different question. Table 10.3 provides interpretation help.\n\n\n\nTable 10.3: Interpretations of the effect estimates in various contrast conditions\n\n\n\n\n\n\n\n\n\n\n\nContrast level\nContrast slope\nInterpretation of level C effect\nInterpretation slope C effect\n\n\n\n\nfirst\nfirst\nWhat would be the value if phase A had continued until to the start of phase C and what is the difference to the actual value at the first measurement of phase C?\nWhat is the difference between the slopes of phase C and A3?\n\n\npreceding\npreceding\nWhat would be the value if phase B had continued to the start of phase C and what is the difference to the actual value at the first measurement of phase C?\nWhat is the difference between the slopes of phase C and B?\n\n\nfirst\npreceding\nWhat would be the value if phase A had continued until the start of phase C (assuming a slope effect but no level effect in phase B)? And what is the difference to the actual value at the first measurement of phase C?\nWhat is the difference between the slopes of phase C and B?\n\n\npreceding\nfirst\nWhat would be the value if phase B had continued until the start of phase C (assuming a level but no slope effect in phase B)? And what is the difference to the actual value at the first measurement of phase C?\nWhat is the difference between the slopes of phase C and A?\n\n\n\n\n\n\nAll four models are mathematically equivalent, i.e. they produce the same estimates of the dependent variable. Bellow I will show how the estimates from the piecewise regression models relate to the simple regression estimates from Table 10.1. These are \\(intercept_{phaseC} = 68.873\\) and \\(slope_{phaseC} = -0.194\\).\nLevel first and slope first contrasts\nTable 10.2 estimates a levelC increase of 46.558 compared to phase A (the intercept) and a slopeC increase of 1.721.\n\\[\nlevelC = intercept_{phaseC} - (Intercept_{phaseA} + n_{phaseA+B} * slope_{phaseA})\n\\tag{10.3}\\]\n\\[46.558 \\approx 68.873 - (60.618 + 20*-1.915) \\]\n\\[\nslopeC = slope_{phaseC} - slope_{phaseA}\n\\tag{10.4}\\]\n\\[1.721 \\approx -0.194 - (-1.915)\\]\nLevel preceding and slope preceding contrasts\nTable 10.2 estimates a levelC increase of 0.139 compared to phase B and a slopeC increase of 0.418.\n\\[\nlevelC = intercept_{phaseC} - (intercept_{phaseB} + n_{phaseB} * slope_{phaseB})\n\\tag{10.5}\\]\n\\[0.139 \\approx 68.873 - (74.855 + 10*-0.612)\\]\n\\[\nslopeC = slope_{phaseC} - slope_{phaseB}\n\\tag{10.6}\\]\n\\[0.418 \\approx -0.194 - (-0.612)\\]\nLevel first and slope preceding contrasts\nTable 10.2 estimates a levelC increase of 33.388 compared to phase A and a slopeC increase of 0.418.\n\\[\nlevelC = intercept_{phaseC} - (intercept_{phaseA}  + n_{phaseA} * slope_{phaseA} + n_{phaseB} * slope_{phaseB})\n\\tag{10.7}\\]\n\\[\n33.527 \\approx 68.873 - (60.618 + 10 * -1.915 + 10 * -0.612)\n\\]\n\\[\nslopeC = slope_{phaseC} - slope_{phaseB}\n\\tag{10.8}\\]\n\\[0.418 \\approx -0.194 - (-0.612)\\]\nLevel preceding and slope first contrasts\nTable 10.2 estimates a levelC increase of 13.170 compared to phase B and a slopeC increase of 1.721.\n\\[\nlevelC = intercept_{phaseC} - (intercept_{phaseB} + n_{phaseB} * slope_{phaseA})\n\\tag{10.9}\\]\n\\[\n13.170\\approx 68.873 - (74.855 + 10*-1.915)\n\\]\n\\[\nslopeC = slope_{phaseC} - slope_{phaseA}\n\\tag{10.10}\\]\n\\[\n1.721 \\approx -0.194 - (-1.915)\n\\]\n\n\n\n\n\nHuitema, B. E., & Mckean, J. W. (2000). Design specification issues in time-series intervention models. Educational and Psychological Measurement, 60(1), 38–58. Retrieved from http://epm.sagepub.com/content/60/1/38.short\n\n\nWilbert, J., Lüke, T., & Börnert-Ringleb, M. (2022). Statistical power of piecewise regression analyses of single-case experimental studies addressing behavior problems. Frontiers in Education Educational Psychology, 7. https://doi.org/10.3389/feduc.2022.917944",
    "crumbs": [
      "Data analysis",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Piecewise linear regressions</span>"
    ]
  },
  {
    "objectID": "ch_piecewise_regression.html#footnotes",
    "href": "ch_piecewise_regression.html#footnotes",
    "title": "10  Piecewise linear regressions",
    "section": "",
    "text": "The session is the ordinal number (1st, 2nd, 3rd, …) of the measurements. While the measurement-time (the value of the measurement-time variable at a specific session/ measurement) is often identical to the session number, this is not necessarily the case. The measurement-time sometimes represents days since the start of the study or other intervals. Therefore, I refer to session or measurement for the ordinal position of the measurement and measurement-time for the value of the time variable at that session number.↩︎\nDifferences here and in the following calculations are due to rounding errors.↩︎\nThe slope of phase A is the trend effect.↩︎",
    "crumbs": [
      "Data analysis",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Piecewise linear regressions</span>"
    ]
  },
  {
    "objectID": "ch_multilevel_plm.html",
    "href": "ch_multilevel_plm.html",
    "title": "11  Multilevel plm analyses",
    "section": "",
    "text": "Note\n\n\n\nThis chapter describes the multilevel data analyses with a frequentist approach. scan also provides functions for Bayesian analyses that are described in chapter Chapter 13\nRead Chapter 10 before you start with this chapter.\n\n\n\n\n\n\n\n\nThe hplm function call:\n\n\n\nhplm(\n  data,\n  dvar,\n  pvar,\n  mvar,\n  model = c(“W”, “H-M”, “B&L-B”, “JW”),\n  contrast = c(“first”, “preceding”),\n  contrast_level = NA,\n  contrast_slope = NA,\n  method = c(“ML”, “REML”),\n  control = list(opt = “optim”),\n  random.slopes = FALSE,\n  lr.test = FALSE,\n  ICC = TRUE,\n  trend = TRUE,\n  level = TRUE,\n  slope = TRUE,\n  random_trend = FALSE,\n  random_level = FALSE,\n  random_slope = FALSE,\n  fixed = NULL,\n  random = NULL,\n  ar = 0,\n  unequal_variances = FALSE,\n  update.fixed = NULL,\n  data.l2 = NULL,\n  …\n)\n\n\nMultilevel analyses can take the piecewise-regression approach even further. It allows for\n\nanalyzing the effects between phases for multiple single-cases at once\ndescribing variability between subjects regarding these effects, and\nintroducing variables and factors for explaining the differences.\n\nThe basic function for applying a multilevel piecewise regression analysis is hplm (hierarchical piecewise linear model). The hplm function is similar to the plm function, so I recommend that you get familar with plm before applying an hplm.\nHere is a simple example:\n\nhplm(exampleAB_50)\n\nHierarchical Piecewise Linear Regression\n\nEstimation method ML \nContrast model: W / level: first, slope: first\n50 Cases\n\nAIC = 8758.802, BIC = 8790.185\nICC = 0.287; L = 339.0; p = 0.000 \n\nFixed effects (values ~ 1 + mt + phaseB + interB)\n\n                            B    SE   df      t p\nIntercept              48.398 1.484 1328 32.611 0\nTrend (mt)              0.579 0.116 1328  5.006 0\nLevel phase B (phaseB) 14.038 0.655 1328 21.436 0\nSlope phase B (interB)  0.902 0.119 1328  7.588 0\n\nRandom effects (~1 | case)\n\n             SD\nIntercept 9.970\nResidual  5.285\n\n\nThe fixed effects describe the overall effect of the respective predictors on the dependent variable, while the random effects describe the variability of the respective predictor between cases. Here, the fixed effect of the intercept is 48.398 which estimates the mean value across all cases at the first measurement. The standard deviation of this average between cases is 9.97.\nIn this analysis, we have a basic random intercept model. That is, only the intercept is assumed to vary between cases, whereas the other predictors (Trend, Level, and Slope) are assumed to be identical for all cases.\n\n11.0.1 Explained variance by the variability between cases\nThe intraclass correlation (ICC) represents the proportion of total variance in the model that is explained by differences between cases.\nIt is calculated by comparing the model fit (likelihood) of two models using a likelihood ratio test. The first model includes both a fixed and a random intercept as predictors of the measurement, while the second model includes only a fixed intercept without a random intercept.\nThe model above reports ICC = 0.287; L = 339.0; p = 0.000. This means that approximately 29% of the total variance in the dependent variable is explained by differences between cases, and this variance is statistically significantly greater than 0%.\n\n\n11.0.2 Variation of single predictors between cases\nA multilevel model that includes the variation of specific predictors between cases (e.g., the trend effect) is - in the multilevel regression “world” - called a random slope model.\nHere is an example inlcuding random slopes by setting the respective function argument for the trend, slope, and level effects to TRUE:\n\nhplm(\n  exampleAB_50, \n  random_trend = TRUE, \n  random_level = TRUE, \n  random_slope = TRUE\n)\n\nHierarchical Piecewise Linear Regression\n\nEstimation method ML \nContrast model: W / level: first, slope: first\n50 Cases\n\nAIC = 8693.218, BIC = 8771.677\nICC = 0.287; L = 339.0; p = 0.000 \n\nFixed effects (values ~ 1 + mt + phaseB + interB)\n\n                            B    SE   df      t p\nIntercept              48.211 1.398 1328 34.497 0\nTrend (mt)              0.621 0.113 1328  5.516 0\nLevel phase B (phaseB) 13.872 0.894 1328 15.513 0\nSlope phase B (interB)  0.864 0.116 1328  7.433 0\n\nRandom effects (~1 + mt + phaseB + interB | case)\n\n                          SD\nIntercept              9.352\nTrend (mt)             0.096\nLevel phase B (phaseB) 4.537\nSlope phase B (interB) 0.126\nResidual               4.974\n\nCorrelation:\n                       Intercept Trend (mt) Level phase B (phaseB)\nTrend (mt)                  0.23                                  \nLevel phase B (phaseB)      0.06      -0.58                       \nSlope phase B (interB)     -0.23      -0.64                  -0.03\n\n\nThe random part of this output now provides estimations of the between case variance of all predictors.\nAdditionally, we get a correlation matrix of the random slope variables. This depicts the interaction effect of the respective variables at the between case level on the dependent variable. For example, the model above shows a correlation of intercept and trend of r = 0.23. That is, the higher the intercept of a case (the starting value), the stronger the trend effect (here a medium effect size).\n\n\n\n\n\n\nCaution\n\n\n\nDo not be confused by the terminology: In multilevel regression models, a random slope model refers to the variability of a predictor in a regression model. These predictors contribute to the slope of a linear regression line.\nThis terminology is used in general within multilevel models, regardless of whether they are applied to single-case studies or not.\nIn single-case piecewise regressions, the slope effect refers to the difference in the slopes of the regression lines between two phases.\nThat is, unfortunately, we have a random slope effect of the slope effect!\n\n\n\n\n11.0.3 Testing the random slope variance\nIt is possible to test the statistical significance of the random slope variances by comparing the overall fit of the model (likelihood) with the fit of the same model without the respective random slope parameter using a likelihood ratio test. To do this, set the argument lr.test = TRUE.\n\nhplm(\n  exampleAB_50, \n  random_trend = TRUE, random_level = TRUE, random_slope = TRUE, \n  lr.test = TRUE\n)\n\nHierarchical Piecewise Linear Regression\n\nEstimation method ML \nContrast model: W / level: first, slope: first\n50 Cases\n\nAIC = 8693.218, BIC = 8771.677\nICC = 0.287; L = 339.0; p = 0.000 \n\nFixed effects (values ~ 1 + mt + phaseB + interB)\n\n                            B    SE   df      t p\nIntercept              48.211 1.398 1328 34.497 0\nTrend (mt)              0.621 0.113 1328  5.516 0\nLevel phase B (phaseB) 13.872 0.894 1328 15.513 0\nSlope phase B (interB)  0.864 0.116 1328  7.433 0\n\nRandom effects (~1 + mt + phaseB + interB | case)\n\n                          SD       L df     p\nIntercept              9.352 348.849  4 0.000\nTrend (mt)             0.096   0.826  4 0.935\nLevel phase B (phaseB) 4.537  42.822  4 0.000\nSlope phase B (interB) 0.126   0.761  4 0.944\nResidual               4.974                 \n\nCorrelation:\n                       Intercept Trend (mt) Level phase B (phaseB)\nTrend (mt)                  0.23                                  \nLevel phase B (phaseB)      0.06      -0.58                       \nSlope phase B (interB)     -0.23      -0.64                  -0.03\n\n\nHere, we can see that the random intercept and the random slope effect for Level phase B are significant. This means that cases have significantly different starting values at the first measurement, and the level intervention effect is also not the same for all cases.\n\n\n11.0.4 Adding additional L2-variables\n\n\n\n\n\n\nThe add_l2 function call:\n\n\n\nadd_l2(scdf, data_l2, cvar = “case”)\n\n\nIn some analyses researchers want to investigate whether attributes of the individuals contribute to the effectiveness of an intervention. For example might an intervention on mathematical abilities be less effective for student with a migration background due to too much language related material within the training. Such analyses can also be conducted with scan. Therefore, we need to define a new data frame including the relevant information of the subjects of the single-case studies we want to analyze. This data frame consists of a variable labeled case which has to correspond to the case names of the scfd and further variables with attributes of the subjects. To build a data frame we can use the R function data.frame.\n\nL2 &lt;- data.frame(\n  case = c(\"Antonia\",\"Theresa\", \"Charlotte\", \"Luis\", \"Bennett\", \"Marie\"), \n  age = c(16, 13, 13, 10, 5, 14), \n  sex = c(\"f\",\"f\",\"f\",\"m\",\"m\",\"f\")\n)\nL2\n\n       case age sex\n1   Antonia  16   f\n2   Theresa  13   f\n3 Charlotte  13   f\n4      Luis  10   m\n5   Bennett   5   m\n6     Marie  14   f\n\n\nMultilevel analyses require a high number of Level 2 units. The exact number depends on the complexity of the analyses, the size of the effects, the number of level 1 units, and the variability of the residuals. But surely we need at least about 30 level 2 units. In a single-case design that is, we need at least 30 single-cases (subjects) within the study. After setting the level 2 data frame we can merge it to the scdf with the add_l2() function (alternatively, we can use the data.l2 argument of the hplm function). Then we have to specify the regression function using the update.fixed argument. The level 2 variables can be added just like any other additional variable. For example, we have added a level 2 data-set with the two variables sex and age. update could be construed of the level 1 piecewise regression model .~. plus the additional level 2 variables of interest + sex + age. The complete argument is update.fixed = .~. + sex + age. This analyses will estimate a main effect of sex and age on the overall performance. In case we want to analyze an interaction between the intervention effects and for example the sex of the subject we have to add an additional interaction term (a cross-level interaction). An interaction is defined with a colon. So sex:phase indicates an interaction of sex and the level effect in the single case study. The complete formula now is update.fixed = .~. + sex + age + sex:phase.\nscan includes an example single-case study with 50 subjects example50 and an additional level 2 data-set example50.l2. Here are the first 10 cases of example50.l2.\n\n\n\n\n\ncase\nsex\nage\n\n\n\n\nRoman\nm\n12\n\n\nBrennen\nm\n10\n\n\nIsmael\nm\n13\n\n\nDonald\nm\n11\n\n\nRicardo\nm\n13\n\n\nIzayah\nm\n11\n\n\nIgnacio\nm\n12\n\n\nXavier\nm\n12\n\n\nArian\nm\n10\n\n\nPaul\nm\n10\n\n\n\n\n\nAnalyzing the data with hplm could look like this:\n\nexampleAB_50 %&gt;%\n  add_l2(exampleAB_50.l2) %&gt;%\n  hplm(update.fixed = .~. + sex + age)\n\nHierarchical Piecewise Linear Regression\n\nEstimation method ML \nContrast model: W / level: first, slope: first\n50 Cases\n\nAIC = 8757.458, BIC = 8799.303\nICC = 0.287; L = 339.0; p = 0.000 \n\nFixed effects (values ~ mt + phaseB + interB + sex + age)\n\n                            B     SE   df      t     p\nIntercept              44.878 11.926 1328  3.763 0.000\nTrend (mt)              0.581  0.116 1328  5.026 0.000\nLevel phase B (phaseB) 14.023  0.655 1328 21.405 0.000\nSlope phase B (interB)  0.900  0.119 1328  7.569 0.000\nsexm                   -6.440  2.727   47 -2.362 0.022\nage                     0.603  1.073   47  0.562 0.577\n\nRandom effects (~1 | case)\n\n             SD\nIntercept 9.446\nResidual  5.284\n\n# Alternatively:\n# hplm(exampleAB_50, data.l2 = exampleAB_50.l2, update.fixed = .~. + sex + age)\n\nsex is a factor with the levels f and m. So sexm is the effect of being male on the overall performance. age does not seem to have any effect. So we drop age out of the equation and add an interaction of sex and phase to see whether the sex effect is due to a weaker impact of the intervention on males.\n\nexampleAB_50 %&gt;%\n  add_l2(exampleAB_50.l2) %&gt;%\n  hplm(update.fixed = .~. + sex + sex:phaseB)\n\nHierarchical Piecewise Linear Regression\n\nEstimation method ML \nContrast model: W / level: first, slope: first\n50 Cases\n\nAIC = 8604.949, BIC = 8646.794\nICC = 0.287; L = 339.0; p = 0.000 \n\nFixed effects (values ~ mt + phaseB + interB + sex + phaseB:sex)\n\n                                 B    SE   df       t    p\nIntercept                   48.573 1.968 1327  24.676 0.00\nTrend (mt)                   0.609 0.109 1327   5.573 0.00\nLevel phase B (phaseB)      17.726 0.684 1327  25.922 0.00\nSlope phase B (interB)       0.884 0.112 1327   7.868 0.00\nsexm                        -0.593 2.741   48  -0.216 0.83\nLevel phase B (phaseB):sexm -7.732 0.609 1327 -12.699 0.00\n\nRandom effects (~1 | case)\n\n             SD\nIntercept 9.494\nResidual  4.989\n\n\nNow the interaction phase:sexm is significant and the main effect is no longer relevant. It looks like the intervention effect is \\(7.7\\) points lower for male subjects. While the level-effect is \\(17.7\\) points for female subjects it is \\(17.7\\) - \\(7.7\\) = \\(10\\) for males.\n\n\n11.0.5 Estimations for each case\nFor a multilevel model, you can estimate the values for each parameter for each case based on the random intercept and slope values.\nUse the casewise argument to access these estimations.\n\nres &lt;- hplm(exampleAB_50[1:10],random.slopes = TRUE)\n\n# retrieve the case estimations for further calculations\ncs &lt;- coef(res, casewise = TRUE)\n\n# or print them\nprint(res, casewise = TRUE)\n\nHierarchical Piecewise Linear Regression\n\nEstimation method ML \nContrast model: W / level: first, slope: first\n10 Cases\n\nAIC = 1799.887, BIC = 1854.674\nICC = 0.327; L = 84.3; p = 0.000 \n\nFixed effects (values ~ 1 + mt + phaseB + interB)\n\n                            B    SE  df      t     p\nIntercept              43.775 2.687 272 16.291 0.000\nTrend (mt)              0.994 0.299 272  3.330 0.001\nLevel phase B (phaseB)  8.675 1.745 272  4.971 0.000\nSlope phase B (interB)  0.527 0.296 272  1.779 0.076\n\nRandom effects (~1 + mt + phaseB + interB | case)\n\n                          SD\nIntercept              7.867\nTrend (mt)             0.488\nLevel phase B (phaseB) 3.309\nSlope phase B (interB) 0.440\nResidual               4.930\n\nCorrelation:\n                       Intercept Trend (mt) Level phase B (phaseB)\nTrend (mt)                  0.86                                  \nLevel phase B (phaseB)      -0.6      -0.88                       \nSlope phase B (interB)      -0.9      -0.97                   0.75\n\nCasewise estimation of effects\n\n    Case Intercept Trend (mt) Level phase B (phaseB) Slope phase B (interB)\n   Roman  43.00890  0.8466415              10.112584             0.61645794\n Brennen  47.43929  1.1041398               9.194791             0.35512803\n  Ismael  53.40011  1.7200026               3.193577            -0.01337913\n  Donald  56.00259  1.6045807               6.361723            -0.08687251\n Ricardo  43.13287  0.9900639               8.374106             0.54179252\n  Izayah  41.47913  0.9871108               8.000154             0.56548396\n Ignacio  47.69657  1.1943237               7.839439             0.32568938\n  Xavier  40.65876  0.8529937               9.300791             0.65995523\n   Arian  26.77803  0.1614967              11.741147             1.36296000\n    Paul  38.15377  0.4785177              12.631558             0.94227393\n\n\nIf you have the scplot package installed (version 0.4.1 or higher), you can create a forestplot for each parameter of the model with the scplot() function. Set the argument “effect” to choose the effect by number or a string (“intercept”, “trend”, “slope”, “level”). The ci argument sets the size of the confidence interval (default is 0.95) and the mark argument sets the value for a reference line (default is the mean effect).\n\nlibrary(scplot)\nscplot(res, effect = \"level\")\n\nPossible effects are: \n2: 'Intercept'\n3: 'Trend (mt)'\n4: 'Level phase B (phaseB)'\n5: 'Slope phase B (interB)'",
    "crumbs": [
      "Data analysis",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Multilevel plm analyses</span>"
    ]
  },
  {
    "objectID": "ch_multivariate_plm.html",
    "href": "ch_multivariate_plm.html",
    "title": "12  Multivariate piecewise regression",
    "section": "",
    "text": "Note\n\n\n\nRead Chapter 10 before you start with this chapter.\n\n\n\n\n\n\n\n\nThe mplm function call:\n\n\n\nmplm(\n  data,\n  dvar,\n  mvar,\n  pvar,\n  model = c(“W”, “H-M”, “B&L-B”, “JW”),\n  contrast = c(“first”, “preceding”),\n  contrast_level = c(NA, “first”, “preceding”),\n  contrast_slope = c(NA, “first”, “preceding”),\n  trend = TRUE,\n  level = TRUE,\n  slope = TRUE,\n  formula = NULL,\n  update = NULL,\n  na.action = na.omit,\n  …\n)\n\n\n\n\n\n\n\n\n\n\nFigure 12.1: Example dataset\n\n\n\n\n\n\nfit &lt;- mplm(exampleAB_add, dvar = c(\"wellbeing\", \"depression\"))\nfit\n\nMultivariate piecewise linear model\n\nDummy model: W level = first, slope = first\nType III MANOVA \nPillai = 0.42; F(6, 72) = 3.20; p = 0.008 \n\n                 wellbeing depression Pillai       F     p\nIntercept           48.417      4.200  0.915 188.949 0.000\nTrend                0.379      0.114  0.055   1.009 0.375\nLevel Medication     3.588     -0.945  0.033   0.588 0.561\nSlope Medication    -0.275     -0.165  0.039   0.712 0.498\n\nFormula: y ~ 1 + day + phaseMedication + interMedication\n\n\n\nprint(fit, std = TRUE)\n\nMultivariate piecewise linear model\n\nDummy model: W level = first, slope = first\nType III MANOVA \nPillai = 0.42; F(6, 72) = 3.20; p = 0.008 \n\n                 wellbeing depression Pillai       F     p\nIntercept            0.000      0.000  0.915 188.949 0.000\nTrend                0.694      0.441  0.055   1.009 0.375\nLevel Medication     0.276     -0.153  0.033   0.588 0.561\nSlope Medication    -0.356     -0.449  0.039   0.712 0.498\n\nCoefficients are standardized\nFormula: y ~ 1 + day + phaseMedication + interMedication",
    "crumbs": [
      "Data analysis",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Multivariate piecewise regression</span>"
    ]
  },
  {
    "objectID": "ch_bayesian_plm.html",
    "href": "ch_bayesian_plm.html",
    "title": "13  Bayesian regression analyses",
    "section": "",
    "text": "13.1 Setting priors\nStarting with version 0.63.0 scan included Bayesian regression analyses through the bplm() function (Bayesian Piecewise Linear Model).\nIn inferential statistics, a distinction is made between frequentist and Bayesian approaches. Frequentist statistics assess the probability of observing the data under the assumption that a null hypothesis (there is no effect or association) is true.\nBayesian statistics, on the other hand, begins with prior distributions that represent initial beliefs (priors) about the parameters of interest. These priors are then updated using observed data through Bayes’ theorem, which means that the initial beliefs about the parameters are adjusted in proportion to how well they explain the data, producing a posterior distribution that reflects both prior knowledge and new evidence. The Bayesian approach evaluates how well the data fit different parameter values by computing the likelihood of the data given these parameter estimates, rather than testing against a fixed null hypothesis.\nThe Bayesian approach is computationally intensive and often produces results that are practically similar to those of a frequentist analysis. However, it offers several advantages. In particular, when working with small samples, incorporating prior knowledge can improve parameter estimation. Additionally, Bayesian statistics does not require uniform distributional assumptions for all variables but allows each variable to have its own empirically derived distribution. Another advantage is its greater robustness against overspecified models, especially when too many predictors are included and exhibit high collinearity (intercorrelations) while the number of data points is limited.\nThese advantages make it worthwhile to use a Bayesian approach for single-case data.\nThe bplm() function comuputes a piecewise regression analysis. The syntax is quite similar to the plm() and hplm() functions. There you can find details about the general piecewise regression model, the interpretation of regression estimations, and the setting of contrast in models with more that two phases.\nThe bplm() works for datasets with one case or multiple cases.\nHer is an example of a one-case dataset:\nHere is an example of a multi-case dataset:\nThe following example show the influence of priors on paramameter estimation. Firstly, we create a random case from previously defined parameters:\nThe starting value (intercept) is 50 (the standard deviation is 10). The level effect for Phase B is one standard deviation (that is, 10 points) and there is neither a slope nor a trend effect. Random noise is introduced with 20% measurment error (reliability is 0.8).\nset.seed(123) #set random seed for replicability of the example\ndes &lt;- design(\n  start_value = 50, \n  s = 10,\n  level = list(A = 0, B = 1), \n  trend = list(0),\n  slope = list(0),\n  rtt = 0.8\n)\nscdf &lt;- random_scdf(des)\nscplot(scdf)\nHere are the estimations from a Bayesian model without informative priors:\nbplm(scdf)\n\nBayesian Piecewise Linear Regression\n\nContrast model: W (level: first, slope: first)\nDeviance Information Criterion: 128.5297 \n\nB-structure - Fixed effects (values ~ 1 + mt + phaseB + interB)\n\n                            B lower 95% CI upper 95% CI sample size     p\nIntercept              49.323       40.969       57.548     905.804 0.001\nTrend (mt)              0.826       -2.195        4.368    1146.096 0.618\nLevel phase B (phaseB)  8.238       -4.645       19.994    1684.465 0.186\nSlope phase B (interB) -0.984       -4.650        2.130    1109.566 0.532\n\nR-Structure - Residuals\n\n          SD lower 95% CI upper 95% CI \n       5.599        3.457        7.517\nNow we introduce our prior knowledge: an intercept of 50, a trend and slope effect of 0, and a level effect of 10. We also assume that our prior is quite uncertain (i.e., a weakly informative prior). mu sets the prior values for the four parameters in the order they appear in the regression model. V is a diagonal matrix of the variances of these estimates. The V matrix sets the strength of the prior. Here we set values of 100, which is one standard deviation (\\(SD^2 = 10^2 = 100\\)):\nprior &lt;- list(\n  B = list(mu = c(50, 0, 10, 0), V = diag(c(100, 100, 100, 100)))\n)\nbplm(scdf, prior = prior)\n\nBayesian Piecewise Linear Regression\n\nContrast model: W (level: first, slope: first)\nDeviance Information Criterion: 127.6719 \n\nB-structure - Fixed effects (values ~ 1 + mt + phaseB + interB)\n\n                            B lower 95% CI upper 95% CI sample size     p\nIntercept              49.375       42.277       56.028        1000 0.001\nTrend (mt)              0.680       -2.038        3.266        1000 0.596\nLevel phase B (phaseB)  9.053       -0.393       18.603        1000 0.062\nSlope phase B (interB) -0.859       -3.535        1.954        1000 0.546\n\nR-Structure - Residuals\n\n          SD lower 95% CI upper 95% CI \n       5.432        3.478        7.197\nNow we assume that we have some certainty (i.e. a prior of medium strength) by setting the variances to 10 (SD ~ 3.2):\nprior &lt;- list(\n  B = list(mu = c(50, 0, 10, 0), V = diag(c(10, 10, 10, 10)))  # Prior for regression effects\n)\nbplm(scdf, prior = prior)\n\nBayesian Piecewise Linear Regression\n\nContrast model: W (level: first, slope: first)\nDeviance Information Criterion: 125.5162 \n\nB-structure - Fixed effects (values ~ 1 + mt + phaseB + interB)\n\n                            B lower 95% CI upper 95% CI sample size     p\nIntercept              50.084       45.768       55.009    1000.000 0.001\nTrend (mt)              0.409       -1.097        1.999    1000.000 0.620\nLevel phase B (phaseB)  9.746        4.723       15.148     895.603 0.001\nSlope phase B (interB) -0.589       -2.465        1.105    1000.000 0.540\n\nR-Structure - Residuals\n\n          SD lower 95% CI upper 95% CI \n       5.293        3.480        6.953\nNow we are making somewhat incorrect and uncertain assumptions:\nprior &lt;- list(\n  B = list(mu = c(40, 2, 5, 2), V = diag(c(100, 100, 100, 100)))\n)\nbplm(scdf, prior = prior)\n\nBayesian Piecewise Linear Regression\n\nContrast model: W (level: first, slope: first)\nDeviance Information Criterion: 127.7585 \n\nB-structure - Fixed effects (values ~ 1 + mt + phaseB + interB)\n\n                            B lower 95% CI upper 95% CI sample size     p\nIntercept              47.789       39.874       54.501    1000.000 0.001\nTrend (mt)              1.396       -1.397        4.187    1000.000 0.322\nLevel phase B (phaseB)  6.931       -3.901       15.898    1136.959 0.186\nSlope phase B (interB) -1.552       -4.740        1.227    1000.000 0.302\n\nR-Structure - Residuals\n\n          SD lower 95% CI upper 95% CI \n       5.542        3.536        7.484\nFinally, we make the wrong assumptions with medium certainty:\nprior &lt;- list(\n  B = list(mu = c(40, 2, 5, 2), V = diag(c(10, 10, 10, 10)))\n)\nbplm(scdf, prior = prior)\n\nBayesian Piecewise Linear Regression\n\nContrast model: W (level: first, slope: first)\nDeviance Information Criterion: 127.267 \n\nB-structure - Fixed effects (values ~ 1 + mt + phaseB + interB)\n\n                            B lower 95% CI upper 95% CI sample size     p\nIntercept              44.321       39.423       48.356        1000 0.001\nTrend (mt)              2.287        0.832        3.828        1000 0.004\nLevel phase B (phaseB)  5.500        0.509       11.276        1000 0.060\nSlope phase B (interB) -2.385       -4.307       -0.758        1000 0.014\n\nR-Structure - Residuals\n\n          SD lower 95% CI upper 95% CI \n       5.598        3.618        7.465",
    "crumbs": [
      "Data analysis",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Bayesian regression analyses</span>"
    ]
  },
  {
    "objectID": "ch_randomization_test.html",
    "href": "ch_randomization_test.html",
    "title": "14  Randomization tests",
    "section": "",
    "text": "14.1 Arguments of the rand_test() function\nThe rand_test function computes a randomization test for single or multiple baseline single-case data. The function is based on an algorithm from the SCRT package (Bulté & Onghena, 2008, 2009), but has been rewritten and extended.\nThe basic idea of a randomization test is to think counterfactually: “Assuming that the phase had no effect on the measured data, what would be the difference between the phases of my case if I had started phase B at a different time? Given the possible phase differences under the assumption that phase had no effect, how likely are the actual phase differences of the original case?”\nTherefore, a number of new cases are generated with a random start for each phase. This means that these new cases have the same data as the original case, but different starting points for each phase. A specific statistic (e.g., the mean difference between Phase A and Phase B data) is now calculated for each new case. When enough random cases have been generated, we also generate a series of new statistics (e.g., mean differences). The statistic for the original case is now compared to these new statistic values. The percentile of the original statistic within the new generated statistic values is the probability of the original statistic assuming a random distribution of starting points for each phase. This percentile is returned as the p-value in the randomization test analyses.\nThe statistics argument defines the statistics on which the phase comparison is based. The following comparisons can be made:\nFurther argument are",
    "crumbs": [
      "Data analysis",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Randomization tests</span>"
    ]
  },
  {
    "objectID": "ch_randomization_test.html#arguments-of-the-rand_test-function",
    "href": "ch_randomization_test.html#arguments-of-the-rand_test-function",
    "title": "14  Randomization tests",
    "section": "",
    "text": "Mean A-B\nThe difference between the mean of Phase A and the mean of Phase B. This is appropriate if a decrease in scores is expected for Phase B.\n\n\nMean B-A\nThe difference between the mean of Phase B and the mean of Phase A. This is appropriate if a increase in scores is expected for Phase B.\n\n\nMean |A-B|\nThe absolute value of the difference between the means of Phases A and B.\n\n\nMedian A-B\nThe same as Mean A-B, but based on the median.\n\n\nMedian B-A\nThe same as Mean B-A, but based on the median.\n\n\nMedian |A-B|\nThe same as Mean |A-B|, but based on the median.\n\n\n\nSome further experimental statistics are also available:\n\n\nSMD hedges\nThe standardized difference between phases as Hedge’s g.\n\n\nSMD glass\nThe standardized difference between phases as Glass’s delta.\n\n\nNAP\nThe Nonoverlap of all pairs.\n\n\nNAP decreasing\nThe Nonoverlap of all pairs assuming decreasing values in phase B.\n\n\nSlope B-A\nThe difference of a linear slope line of the phases.\n\n\nSlope A-B\nThe difference of a linear slope line of the phases expecting a decreasing effect in phase B.\n\n\nW-test\nCalculates a Wilcoxon test and takes the W statistic.\n\n\nT-test\nCalculates a T-test test and takes the t statistic.\n\n\n\n\n\nArguments of the randomization test function.\n\n\n\n\n\n\nArgument\nWhat it does …\n\n\n\n\nnumber\nSample size of the randomization distribution. The exactness of the p-value can not exceed 1/number (i.e., number = 100 results in p-values with an exactness of one percent). Default is number = 500. For faster processing use number = 100. For more precise p-values set number = 1000.\n\n\ncomplete\nIf TRUE, the distribution is based on a complete permutation of all possible starting combinations. This setting overwrites the number Argument. The default setting is FALSE.\n\n\nlimit\nMinimal number of data points per phase in the sample. The first number refers to the A-phase and the second to the B-phase (e.g., limit = c(5, 3)). If only one number is given, this number is applied to both phases. Default is limit = 5.\n\n\nstartpoints\nAlternative to the limit-parameter, startpoints exactly defines the possible start points of phase B (e.g., startpoints = 4:9 restricts the phase B start points to measurements 4 to 9. startpoints overwrite the limit-parameter.\n\n\nexclude.equal\nIf set to FALSE, which is the default, random distribution values equal to the observed distribution are counted as null-hypothesis conform. That is, they decrease the probability of rejecting the null-hypothesis (increase the p-value). exclude.equal should be set to TRUE if you analyse one single-case design (not a multiple baseline data set) to reach a sufficient power. But be aware, that it increases the chance of an alpha-error.",
    "crumbs": [
      "Data analysis",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Randomization tests</span>"
    ]
  },
  {
    "objectID": "ch_randomization_test.html#example",
    "href": "ch_randomization_test.html#example",
    "title": "14  Randomization tests",
    "section": "14.2 Example",
    "text": "14.2 Example\n\nres &lt;- rand_test(exampleAB)\nres\n\nRandomization Test\n\nCombined test for three cases.\n\nComparing phase 1 against phase 2 \nStatistic:  Mean B-A \n\nMinimal length of each phase: A = 5 , B = 5 \nObserved statistic =  20.55556 \n\nDistribution based on a random sample of all 1331 possible combinations.\nn   =  500 \nM   =  18.6697 \nSD  =  1.136184 \nMin =  16.05185 \nMax =  21.31136 \n\nProbability of an equal or higher value than the observed statistic:\np   =  0.038 \n\nShapiro-Wilk Normality Test: W = 0.976; p = 0.000  (Hypothesis of normality rejected)\n\nProbabilty of observed statistic based on the assumption of normality:\nz = 1.6598, p = 0.0485 (single sided)",
    "crumbs": [
      "Data analysis",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Randomization tests</span>"
    ]
  },
  {
    "objectID": "ch_randomization_test.html#visuaization",
    "href": "ch_randomization_test.html#visuaization",
    "title": "14  Randomization tests",
    "section": "14.3 Visuaization",
    "text": "14.3 Visuaization\nThe plot_rand() function plots a distribution of the random sample against the observed statistic:\n\nplot_rand(res)\n\n\n\n\n\n\n\n\nA more sophisticated histogram is available from the scplot package since version 0.4.1.\n\nscplot(res)\n\n\n\n\n\n\n\n\nAnother visualization is available since scplot version 0.5.1 for a randomization test with one case. It gives you an overview of the respective statistic for each available startpoint of phase B.\n\nrand_test(\n  byHeart2011$`Lisa (Turkish)`, \n  statistic = \"Median B-A\", \n  limit = 1, \n  complete = TRUE) |&gt; \n  scplot(type = \"xy\")",
    "crumbs": [
      "Data analysis",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Randomization tests</span>"
    ]
  },
  {
    "objectID": "ch_randomization_test.html#providing-a-custom-function",
    "href": "ch_randomization_test.html#providing-a-custom-function",
    "title": "14  Randomization tests",
    "section": "14.4 Providing a custom function",
    "text": "14.4 Providing a custom function\nIf you want to experiment around with new statistics for the randomization test, you can add your own analyses functions.\nUse the statistic_function argument to provide your own function in a list. This list must have an element named statistic with a function that takes two arguments a and b and returns a single numeric value. For example, list(statistic = function(a, b) mean(a) - mean(b)). The second element of the list is named aggregate which takes a function with one numeric argument that returns a numeric argument. This function is used to aggregate the values of a multiple case design. If you do not provide this element, the default function(x) sum(x) / length(x). is used. The third optional argument is name which provides a name for your custom function.\nHere is an example that implements the PND statistic and uses the median to aggregate the PNDs in a multiple case study.\n\nnew_statistic &lt;- list(\n  statistic = function(a, b) sum(b &gt; max(a), na.rm = TRUE) / sum(!is.na(b)),\n  aggregate = function(x) median(x),\n  name = \"PND\"\n)\n\nrand_test(Huber2014, statistic_function = new_statistic)\n\nRandomization Test\n\nCombined test for four cases.\n\nComparing phase 1 against phase 2 \nStatistic:  user defined function PND \n\nMinimal length of each phase: A = 5 , B = 5 \nObserved statistic =  0.07575758 \n\nDistribution based on a random sample of all 2513840 possible combinations.\nn   =  500 \nM   =  0.01445816 \nSD  =  0.03814635 \nMin =  0 \nMax =  0.3412698 \n\nProbability of an equal or higher value than the observed statistic:\np   =  0.102 \n\nShapiro-Wilk Normality Test: W = 0.416; p = 0.000  (Hypothesis of normality rejected)\n\nProbabilty of observed statistic based on the assumption of normality:\nz = 1.6070, p = 0.0540 (single sided)\n\n\n\n\n\n\nBulté, I., & Onghena, P. (2008). An r package for single-case randomization tests. Behavior Research Methods, 40(2), 467–478. https://doi.org/10.3758/BRM.40.2.467\n\n\nBulté, I., & Onghena, P. (2009). Randomization tests for multiple-baseline designs: An extension of the SCRT-r package. Behavior Research Methods, 41(2), 477–485. https://doi.org/10.3758/BRM.41.2.477",
    "crumbs": [
      "Data analysis",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Randomization tests</span>"
    ]
  },
  {
    "objectID": "ch_powertest.html",
    "href": "ch_powertest.html",
    "title": "15  Power analyses",
    "section": "",
    "text": "15.1 The idea of a power-test\nThe powert_test() function provides the alpha error probability and power when analyzing a specific effect of a single-case design with a given statistical method.\nFor example, you have a one case design with phase length A = 10 and B = 20. You assume a strong level effect of d = 1 and you expect a slight trend effect of d = 0.02 (per measurement). You might be interested to answer two questions:\nIn principle, power_test() takes a single case design and repeatedly generates random cases based on that design. Each case is now analyzed with a given statistical method. The proportion of significant effects in these analyses is an estimator of the test-power. In a second step the design is stripped of the target effect and again multiple cases are generated on this changed design and analyzed with the same method. Now, the proportion of significant effects is the estimator for the alpha-error probability.",
    "crumbs": [
      "Data analysis",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Power analyses</span>"
    ]
  },
  {
    "objectID": "ch_powertest.html#the-idea-of-a-power-test",
    "href": "ch_powertest.html#the-idea-of-a-power-test",
    "title": "15  Power analyses",
    "section": "",
    "text": "How suitable is a plm model for detecting the level-effect? (also: what is the power to detect the level effect?).\nWhat if I had the same design but without a level-effect. How often would the plm falsely find a significant level-effect? (also: how large is the alpha-error probability for the level-effect?).",
    "crumbs": [
      "Data analysis",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Power analyses</span>"
    ]
  },
  {
    "objectID": "ch_powertest.html#set-up-a-single-case-design",
    "href": "ch_powertest.html#set-up-a-single-case-design",
    "title": "15  Power analyses",
    "section": "15.2 Set up a single-case design",
    "text": "15.2 Set up a single-case design\n\n\n\n\n\n\nThe design function call:\n\n\n\ndesign(\n  n = 1,\n  phase_design = list(A = 5, B = 15),\n  trend = 0,\n  level = list(0),\n  slope = list(0),\n  start_value = 50,\n  s = 10,\n  rtt = 0.8,\n  extreme_prop = list(0),\n  extreme_range = c(-4, -3),\n  missing_prop = 0,\n  distribution = c(“normal”, “gaussian”, “poisson”, “binomial”),\n  random_start_value = FALSE,\n  n_trials = NULL,\n  mt = NULL,\n  B_start = NULL,\n  m,\n  MT\n)\n\n\nThe design function sets up a single-case design. You can define various parameters of that design:\n\n\n\n\nTable 15.1: Core arguments of the design function\n\n\n\n\n\n\nArgument\nWhat it does ...\n\n\n\n\nn\nNumber of cases to be created (Default is n = 1).\n\n\nphase_design\nA list defining the length and label of each phase. E.g., phase.length = list(A1 = 10, B1 = 10, A2 = 10, B2 = 10). Use vectors if you want to define different values for each case phase.length = list(A = c(10, 15), B = c(10, 15).\n\n\ntrend\nDefines the effect size of a trend added incrementally to each measurement across the whole data-set. To assign different trends to several single-cases, use a vector of values (e.g. trend = c(.1, .3, .5)). If the number of cases exceeds the length of the vector, values are recycled. When using a 'gaussian' distribution, the trend parameters indicate effect size d changes. When using a binomial or poisson distribution, trend indicates an increase in points / counts per measurement.\n\n\nlevel\nA list that defines the level increase (effect size d) at the beginning of each phase relative to the previous phase (e.g. list(A = 0, B = 1)). The first element must be zero as the first phase of a single-case has no level effect (if you have one less list element than the number of phases, scan will add a leading element with 0 values). Use vectors to define variable level effects for each case (e.g. list(A = c(0, 0), B = c(1, 2))). When using a 'gaussian' distribution, the level parameters indicate effect size d changes. When using a binomial or poisson distribution, level indicates an increase in points / counts with the onset of each phase.\n\n\nslope\nA list that defines the increase per measurement for each phase compared to the previous phase. slope = list(A = 0, B = .1 generates an incremental increase of 0.1 per measurement starting at the B phase. The first list element must be zero as the first phase of a single-case has no slope effect (if you have one less list element than the number of phases, scan will add a leading element with 0 values). Use vectors to define variable slope effects for each case (e.g. list(A = c(0, 0), B = c(0.1, 0.2))). If the number of cases exceeds the length of the vector, values are recycled. When using a 'gaussian' distribution, the slope parameters indicate effect size d changes per measurement. When using a binomial or poisson distribution, slope indicates an increase in points / counts per measurement.\n\n\nrtt\nReliability of the underlying simulated measurements. Set rtt = .8 by default. To assign different reliabilities to several single-cases, use a vector of values (e.g. rtt = c(.6, .7, .8)). If the number of cases exceeds the length of the vector, values are repeated. rtt has no effect when you're using binomial or poisson distributed scores.\n\n\nstart_value\nStarting value at the first measurement. Default is 50. To assign different start values to several single-cases, use a vector of values (e.g. c(50, 42, 56)). If the number of cases exceeds the length of the vector, values are recycled.\n\n\ns\nStandard deviation used to calculate absolute values from level, slope, trend effects and to calculate and error distribution from the rtt values. Set to 10 by default. To assign different variances to several single-cases, use a vector of values (e.g. s = c(5, 10, 15)). If the number of cases exceeds the length of the vector, values are recycled. if the distribution is 'poisson' or 'binomial' s is not applied.\n\n\nextreme_prop\nProbability of extreme values. extreme.p = .05 gives a five percent probability of an extreme value. A vector of values assigns different probabilities to multiple cases. If the number of cases exceeds the length of the vector, values are repeated.\n\n\nextreme_range\nRange for extreme values, expressed as effect size d. extreme.d = c(-7,-6) uses extreme values within a range of -7 and -6 standard deviations. In case of a binomial or poisson distribution, extreme.d indicates points / counts. Caution: the first value must be smaller than the second, otherwise the procedure will fail.\n\n\nmissing_prop\nPortion of missing values. missing.p = 0.1 creates 10% of all values as missing). A vector of values assigns different probabilities to multiple cases. If the number of cases exceeds the length of the vector, values are repeated.\n\n\ndistribution\nDistribution of the scores. Default is distribution = 'normal'. Possible values are 'normal' (or 'gaussian'), 'binomial', and 'poisson'.\n\n\nprob\nIf distribution is set 'binomial', prob passes the probability of occurrence.",
    "crumbs": [
      "Data analysis",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Power analyses</span>"
    ]
  },
  {
    "objectID": "ch_powertest.html#conducting-a-power-test",
    "href": "ch_powertest.html#conducting-a-power-test",
    "title": "15  Power analyses",
    "section": "15.3 Conducting a power-test",
    "text": "15.3 Conducting a power-test\nWhen conduction a power test, you firstly need to define a design which you like to be tested.\n\ndesign &lt;- design(\n  n = 1,\n  phase_design = list(A = 10, B = 20),\n  level = list(A = 0, B = 1),\n  trend = 0.02,\n  distribution = \"normal\"\n)\n\nThen you have to choose the statistical method. The power_test function applies three methods by default: plm, randomization test, and Tau U. These default values are only suitable when your design is a one case single-case study.\nLet us start with the defaults and conduct a power analysis for our previously set design: (This might take some time. Even in the default setting with 100 simulations you might wait a few seconds. For more precise estimations I recommend 1000 simulations - or even higher.)\n\nres &lt;- power_test(design)\nres\n\nTest-Power in percent:\n\n    Method Power Alpha Error Alpha:Beta Correct\n plm_level    79           2     1:10.5      88\n      rand    69           3     1:10.3      83\n      tauU   100          26      1:0.0      87\n\n\nThe results show that the plm test and the randomization test have similar power and alpha-error probabilities (the differences here may be due to outliers of the random samples. A more intensive computation with 1000 simulations shows slightly better values for the plm). The tau U test has an unacceptably high alpha-error which is due to the trend we put into the design. Alpha:Beta depicts the relation of the Alpha and Beta error (power = 1 - Beta). Correct is the overall proportion of correct categorizations and p is the results of a binomial-test of Correct against 50%.",
    "crumbs": [
      "Data analysis",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Power analyses</span>"
    ]
  },
  {
    "objectID": "ch_powertest.html#statistical-methods",
    "href": "ch_powertest.html#statistical-methods",
    "title": "15  Power analyses",
    "section": "15.4 Statistical methods",
    "text": "15.4 Statistical methods\nThe method argument takes a list where each element depicts a statistical method. Currently, the following character strings are predefined:\n\n\n\n\nTable 15.2: Statistical methods\n\n\n\n\n\n\nName\nSingle/ multiple cases\nWhat it means ...\n\n\n\n\nplm_level\nsingle\nA complete plm model for normal distributed dependent variables. It checks for the level effect.\n\n\nplm_slope\nsingle\nA complete plm model for normal distributed dependent variables. It checks for the slope effect.\n\n\nplm_poisson_level\nsingle\nLike plm_level but for poisson distributed dependent variables.\n\n\nplm_poisson_slope\nsingle\nLike plm_slope but for poisson distributed dependent variables.\n\n\nhplm_level\nmultiple\nA complete hplm model for normal distributed dependent variables. It checks for the level effect.\n\n\nhplm_slope\nmultiple\nA complete hplm model for normal distributed dependent variables. It checks for the slope effect.\n\n\ntauU\nsinlge\nA tauU test with method complete and taub estimations. It checks the 'A vs. B - Trend A' variation.\n\n\ntauU_slope\nsinlge\nA tauU test with method complete and taub estimations. It checks the 'A vs. B - Trend A + Trend B' variation.\n\n\ntauU_meta\nmultiple\nLike 'TauU' but with the results from a meta analyses (fixed effects). Very slow.\n\n\ntauU_slope_meta\nmultiple\nLike 'TauU_slope' but with the results from a meta analyses (fixed effects). Very slow.\n\n\nbase_tau\nsingle\nA baseline corrected tau test.\n\n\nrand\nsingle and multiple\nA randomization test for 'Mean B-A' with 100 permutations.",
    "crumbs": [
      "Data analysis",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Power analyses</span>"
    ]
  },
  {
    "objectID": "ch_powertest.html#confidence-intervals-and-binomial-tests",
    "href": "ch_powertest.html#confidence-intervals-and-binomial-tests",
    "title": "15  Power analyses",
    "section": "15.5 Confidence intervals and binomial tests",
    "text": "15.5 Confidence intervals and binomial tests\nWith only 100 simulations you will have quite large confidence intervals for the power, alpha error probability, and correct estimations. You can calculate these intervals by setting the ci argument. For 95% CI’s set ci = 0.95 for 99% ci = 0.99.\n\npower_test(design, ci = 0.95)\n\nTest-Power in percent:\n\n    Method Power 2.5% 97.5% Alpha Error 2.5% 97.5% Alpha:Beta Correct 2.5%\n plm_level    62   52    72           5    2    11      1:7.6      78   72\n      rand    56   46    66           4    1    10     1:11.0      76   69\n      tauU   100   96   100          20   13    29      1:0.0      90   85\n 97.5%\n    84\n    82\n    94\n\n\nYou can also test the power, alpha error, and correct estimates against predefined values. In order to do that, set binom_test = TRUE. The power will be tested against being greater or equal to 80%, the alpha error against being less or equal 5%, and the correct proportion against being greater equal 87.5%.\n\npower_test(design, binom_test = TRUE)\n\nTest-Power in percent:\n\n    Method Power Alpha Error Alpha:Beta Correct p Power&gt;=80 p Alpha Error&lt;=5\n plm_level    81           4      1:4.8      88           0                0\n      rand    63           5      1:7.4      79           1                1\n      tauU   100          19      1:0.0      90           0                1\n p Correct&gt;=87.5\n             0.4\n             1.0\n             0.1\n\n\nIf you want to define individual values for the three tests, set the binom_test_power. binom_test_alpha, and/or, the binom_test_correct arguments.",
    "crumbs": [
      "Data analysis",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Power analyses</span>"
    ]
  },
  {
    "objectID": "ch_powertest.html#advanced-methods",
    "href": "ch_powertest.html#advanced-methods",
    "title": "15  Power analyses",
    "section": "15.6 Advanced methods",
    "text": "15.6 Advanced methods\nNote: You need specific knowledge on how to create functions in R and on data structures to follow all aspects of this section.\nInstead of one of the predefined character strings you can also create you own functions and implement these. You function must take an scdf as the first argument and return a single numeric p-value.\nHere is an example that implements a method for the significance of a NAP (nonoverlap of all pairs) test. This is statistically identical to a U-Test comparing phase A and B.\n\nset.seed(1) # only needed to make this example replicable\n\nmcmethod_nap &lt;- function(scdf) {\n  nap(scdf)$nap[1, \"p\"]\n}\n\npower_test(design, method = list(nap = mcmethod_nap, \"rand\", \"plm_level\"))\n\nTest-Power in percent:\n\n    Method Power Alpha Error Alpha:Beta Correct\n       nap   100          47      1:0.0      76\n      rand    65           5      1:7.0      80\n plm_level    73           3      1:9.0      85\n\n\nHere is another example for a fast plm function for poisson distributed:\n\nplm_fast &lt;- function(data) {\n  data &lt;- unlist(data, recursive = FALSE)\n  y  &lt;- data$values\n  n1 &lt;- sum(data$phase == \"A\")\n  n2 &lt;- sum(data$phase == \"B\")\n  D &lt;- c(rep(0, n1), rep(1, n2))\n  mt &lt;- data$mt\n  inter &lt;- (mt - mt[n1]) * D\n  x &lt;- matrix(\n    c(rep(1, n1 + n2), mt, D, inter),\n    nrow = n1 + n2,\n    ncol = 4\n  )\n  full &lt;- lm(y ~ 1 + mt + D + inter)\n  summary(full)$coef[3, 4]\n}\n\n\npower_test(design, method = list(\"fast plm\" = plm_fast))",
    "crumbs": [
      "Data analysis",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Power analyses</span>"
    ]
  },
  {
    "objectID": "ch_powertest.html#computation-duration",
    "href": "ch_powertest.html#computation-duration",
    "title": "15  Power analyses",
    "section": "15.7 Computation duration",
    "text": "15.7 Computation duration\nYou can print the returning object of the power_test function with added computation duration time by setting duration = TRUE\n\nprint(res, duration = TRUE)\n\nTest-Power in percent:\n\n    Method Power Alpha Error Alpha:Beta Correct\n plm_level    79           2     1:10.5      88\n      rand    69           3     1:10.3      83\n      tauU   100          26      1:0.0      87\n\nComputation duration is 0.5 seconds.\n\n\nThe duration depends heavily on the applied test methods. Regressions are faster than randomization tests and tau U tests are quiet slow:\n\nres1 &lt;- power_test(design, method = \"plm_level\")\nres2 &lt;- power_test(design, method = \"rand\")\nres3 &lt;- power_test(design, method = \"tauU\")\n\n# Elapsed time in seconds for each procedure\nattr(res1, \"computation_duration\")[3]\n## elapsed \n##   0.062\nattr(res2, \"computation_duration\")[3]\n## elapsed \n##   0.244\nattr(res3, \"computation_duration\")[3]\n## elapsed \n##    0.26\n\n… and what about our new fast-glm function?\n\nset.seed(1)\ndesign &lt;- design(\n  n = 1,\n  phase_design = list(A = 10, B = 20),\n  level = list(A = 0, B = 1),\n  trend = 0.02,\n  distribution = \"poisson\"\n)\n\nres1 &lt;- power_test(design, method = list(\"fast plm\" = plm_fast))\nres2 &lt;- power_test(design, method = \"plm_poisson_level\")\n\nattr(res1, \"computation_duration\")[3]\n\nelapsed \n   0.06 \n\nattr(res2, \"computation_duration\")[3]\n\nelapsed \n  0.097 \n\n\nIt is slower! glm in R is very well optimized!!",
    "crumbs": [
      "Data analysis",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Power analyses</span>"
    ]
  },
  {
    "objectID": "ch_export.html",
    "href": "ch_export.html",
    "title": "16  Exporting scan results",
    "section": "",
    "text": "16.1 Single case data files\nThe export function will make it easier to convert the results of your scan analyses into tables and descriptions you can add to your documents and presentations. Basically, export takes a scan object and converts it to an html-table or latex output.\nexport works best when used within an rmarkdown file and/or within RStudio. In RStudio, the html table will be displayed in the Viewer pane. There you can click the export button () to export an html or bitmap file or you can try drag and drop ➡️ copy and paste the table into another application.\nAlternatively, you can set the filename argument to export the table directly from within the export function. The file name extension you provide will define the resulting file format (e.g. filename = \"results.html\").\nIf you use the “kable engine”, possible extensions and file formats are html, png, and jpg. The newer “gt engine” also allows to export to docx, rtf, pdf, and tex formats. To apply the new eninge, set options(\"scan.export.engine\" = \"gt\").\nThe examples provided in this book are created with the gt engine.\nMost of the tables will have a default caption and footnotes. If you want to change these, use the caption and footnote arguments.\nexport(exampleA1B1A2B2_zvt)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTable\n\n\nSingle case data frame with three cases\n\n\n\nTick\n\n\nTrick\n\n\nTrack\n\n\n\nzvt\nd2\nday\npart\nzvt\nd2\nday\npart\nzvt\nd2\nday\npart\n\n\n\n\n47\n131\n1\nA1\n51\n100\n1\nA1\n54\n89\n1\nA1\n\n\n58\n134\n2\nA1\n58\n126\n2\nA1\n57\n116\n2\nA1\n\n\n76\n141\n3\nA1\n70\n130\n3\nA1\n51\n114\n3\nA1\n\n\n63\n141\n4\nB1\n65\n130\n4\nB1\n61\n131\n4\nB1\n\n\n71\n140\n5\nB1\n67\n137\n5\nB1\n57\n132\n5\nB1\n\n\n59\n140\n6\nB1\n63\n133\n6\nB1\n53\n130\n6\nB1\n\n\n64\n138\n7\nA2\n64\n136\n7\nA2\n58\n128\n7\nA2\n\n\n69\n140\n8\nA2\n70\n137\n8\nA2\n57\n131\n8\nA2\n\n\n72\n141\n9\nA2\n70\n135\n9\nA2\n60\n130\n9\nA2\n\n\n77\n140\n10\nB2\n68\n128\n10\nB2\n55\n129\n10\nB2\n\n\n76\n138\n11\nB2\n69\n137\n11\nB2\n58\n118\n11\nB2\n\n\n73\n140\n12\nB2\n70\n138\n12\nB2\n58\n131\n12\nB2",
    "crumbs": [
      "More",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Exporting *scan* results</span>"
    ]
  },
  {
    "objectID": "ch_export.html#single-case-data-files",
    "href": "ch_export.html#single-case-data-files",
    "title": "16  Exporting scan results",
    "section": "",
    "text": "The scdf export function call:\n\n\n\nexport(\n  object,\n  summary = FALSE,\n  caption = NA,\n  footnote = NA,\n  filename = NA,\n  cols,\n  round = 2,\n  …\n)",
    "crumbs": [
      "More",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Exporting *scan* results</span>"
    ]
  },
  {
    "objectID": "ch_export.html#descriptive-stats",
    "href": "ch_export.html#descriptive-stats",
    "title": "16  Exporting scan results",
    "section": "16.2 Descriptive stats",
    "text": "16.2 Descriptive stats\n\n\n\n\n\n\nThe describe export function call:\n\n\n\nexport(object, caption = NA, footnote = NA, filename = NA, flip = FALSE, round = 2, …)\n\n\nThe flip argument will rotate the table by 90 degrees.\n\nres &lt;- describe(GruenkeWilbert2014)\nexport(res)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTable\n\n\nDescriptive statistics\n\n\nCase\nDesign\n\nn\n\n\nMissing\n\n\nM\n\n\nMedian\n\n\nSD\n\n\nMAD\n\n\nMin\n\n\nMax\n\n\nTrend\n\n\n\nA\nB\nA\nB\nA\nB\nA\nB\nA\nB\nA\nB\nA\nB\nA\nB\nA\nB\n\n\n\n\nAnton\nA-B\n4\n14\n0\n0\n5.00\n9.14\n5\n9\n0.82\n0.77\n0.74\n1.48\n4\n8\n6\n10\n-0.40\n0.03\n\n\nBob\nA-B\n7\n11\n0\n0\n3.00\n8.82\n3\n9\n0.82\n0.87\n1.48\n0.00\n2\n7\n4\n10\n0.04\n0.04\n\n\nPaul\nA-B\n6\n12\n0\n0\n3.83\n8.83\n4\n9\n0.75\n0.72\n0.74\n0.74\n3\n8\n5\n10\n-0.26\n0.02\n\n\nRobert\nA-B\n8\n10\n0\n0\n4.12\n8.90\n4\n9\n0.83\n0.99\n1.48\n1.48\n3\n7\n5\n10\n-0.06\n-0.14\n\n\nSam\nA-B\n5\n13\n0\n0\n4.60\n9.08\n5\n9\n0.55\n0.86\n0.00\n1.48\n4\n8\n5\n10\n0.10\n0.03\n\n\nTim\nA-B\n4\n14\n0\n0\n3.00\n9.00\n3\n9\n0.82\n0.96\n0.74\n1.48\n2\n7\n4\n10\n-0.60\n0.00\n\n\n\nNote. n = Number of measurements; Missing = Number of missing values; M = Mean; Median = Median; SD = Standard deviation; MAD = Median average deviation; Min = Minimum; Max = Maximum; Trend = Slope of dependent variable regressed on measurement-time.",
    "crumbs": [
      "More",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Exporting *scan* results</span>"
    ]
  },
  {
    "objectID": "ch_export.html#standardized-mean-differences",
    "href": "ch_export.html#standardized-mean-differences",
    "title": "16  Exporting scan results",
    "section": "16.3 Standardized mean differences",
    "text": "16.3 Standardized mean differences\n\n\n\n\n\n\nThe smd export function call:\n\n\n\nexport(\n  object,\n  caption = NA,\n  footnote = NA,\n  filename = NA,\n  select = c(“Case”, Mean A = “mA”, Mean B = “mB”, SD A = “sdA”, SD B = “sdB”, SD Cohen = “sd cohen”, SD Hedges = “sd hedges”, “Glass’ delta”, “Hedges’ g”, “Hedges’ g correction”, “Hedges’ g durlak correction”, “Cohen’s d”),\n  round = 2,\n  decimals = 2,\n  flip = FALSE,\n  …\n)\n\n\nThe flip argument will rotate the table by 90 degrees.\n\nsmd(exampleAB) |&gt; export(flip = TRUE)\n\n\n\n\n\n\n\nTable\n\n\nStandardizes mean differences. Comparing phase 1 against phase 2\n\n\nStatistic\nJohanna\nKarolina\nAnja\n\n\n\n\nMean A\n54.60\n51.80\n53.60\n\n\nMean B\n74.13\n73.47\n74.07\n\n\nSD A\n2.41\n6.83\n3.05\n\n\nSD B\n8.94\n9.76\n7.57\n\n\nSD Cohen\n6.55\n8.43\n5.77\n\n\nSD Hedges\n7.97\n9.19\n6.83\n\n\nGlass' delta\n8.11\n3.17\n6.71\n\n\nHedges' g\n2.45\n2.36\n3.00\n\n\nHedges' g correction\n2.35\n2.26\n2.87\n\n\nHedges' g durlak correction\n2.23\n2.14\n2.72\n\n\nCohen's d\n2.98\n2.57\n3.55\n\n\n\nNote. SD Cohen = unweigted average of the variance of both phases; SD Hedges = weighted average of the variance of both phases with a degrees of freedom correction; Glass’ delta = mean difference divided by the standard deviation of the A-phase; Hedges’ g = mean difference divided by SD Hedges; Hedges’ g (durlak) correction = approaches for correcting Hedges’ g for small sample sizes; Cohens d = mean difference divided by SD Cohen.",
    "crumbs": [
      "More",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Exporting *scan* results</span>"
    ]
  },
  {
    "objectID": "ch_export.html#trend-analysis",
    "href": "ch_export.html#trend-analysis",
    "title": "16  Exporting scan results",
    "section": "16.4 Trend analysis",
    "text": "16.4 Trend analysis\n\n\n\n\n\n\nThe trend export function call:\n\n\n\nexport(object, caption = NA, footnote = NA, filename = NA, round = 3, decimals = NULL, …)\n\n\n\nexampleABC$Marie  |&gt; \n  trend()  |&gt; \n  export()\n\n\n\n\n\n\n\nTable\n\n\nTrend analysis\n\n\nPhase\nIntercept\nB\nBeta\n\n\n\n\nLinear (values ~ mt)\n\n\nALL\n55.159\n0.612\n0.392\n\n\nA\n60.618\n-1.915\n-0.700\n\n\nB\n74.855\n-0.612\n-0.163\n\n\nC\n68.873\n-0.194\n-0.046\n\n\nQuadratic (values ~ I(mt^2))\n\n\nALL\n59.135\n0.017\n0.330\n\n\nA\n57.937\n-0.208\n-0.712\n\n\nB\n73.217\n-0.039\n-0.098\n\n\nC\n68.490\n-0.017\n-0.038\n\n\n\nNote. Measurement-times start at 0 for each phase.",
    "crumbs": [
      "More",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Exporting *scan* results</span>"
    ]
  },
  {
    "objectID": "ch_export.html#overlap-indices",
    "href": "ch_export.html#overlap-indices",
    "title": "16  Exporting scan results",
    "section": "16.5 Overlap indices",
    "text": "16.5 Overlap indices\n\n\n\n\n\n\nThe overlap export function call:\n\n\n\nexport(\n  object,\n  caption = NA,\n  footnote = NULL,\n  filename = NA,\n  round = 2,\n  decimals = 2,\n  flip = FALSE,\n  …\n)\n\n\nThe flip argument will rotate the table by 90 degrees.\n\nexampleA1B1A2B2_zvt |&gt; \n  select_phases(A = c(1,3), B = c(2,4)) |&gt; \n  overlap() |&gt; \n  export(flip = TRUE)\n\n\n\n\n\n\n\nTable\n\n\nOverlap indices. Comparing phase 1 against phase 2\n\n\nStatistic\nTick\nTrick\nTrack\n\n\n\n\nPND\n16.67\n0.00\n16.67\n\n\nPEM\n66.67\n50.00\n50.00\n\n\nPET\n66.67\n33.33\n33.33\n\n\nNAP\n68.06\n51.39\n58.33\n\n\nNAP-R\n36.11\n2.78\n16.67\n\n\nPAND\n66.67\n50.00\n66.67\n\n\nIRD\n0.33\n0.33\n0.17\n\n\nTau-U (A + B - trend A)\n0.07\n−0.16\n−0.04\n\n\nTau-U (A + B - trend A + trend B)\n0.14\n0.03\n−0.03\n\n\nBase Tau\n0.27\n−0.25\n0.13\n\n\nDelta M\n5.50\n3.17\n0.83\n\n\nDelta Trend\n−0.31\n−1.10\n−0.74\n\n\nSMD\n0.52\n0.40\n0.26\n\n\nHedges g\n0.56\n0.50\n0.26\n\n\n\nNote. PND = Percentage Non-Overlapping Data; PEM = Percentage Exceeding the Median; PET = Percentage Exceeding the Trend; NAP = Nonoverlap of all pairs; NAP-R = NAP rescaled; PAND = Percentage all nonoverlapping data; IRD = Improvement rate difference; Tau U (A + B - trend A) = Parker’s Tau-U; Tau U (A + B - trend A + trend B) = Parker’s Tau-U; Base Tau = Baseline corrected Tau; Delta M = Mean difference between phases; Delta Trend = Trend difference between phases; SMD = Standardized Mean Difference; Hedges g = Corrected SMD.",
    "crumbs": [
      "More",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Exporting *scan* results</span>"
    ]
  },
  {
    "objectID": "ch_export.html#percentage-non-overlapping-data",
    "href": "ch_export.html#percentage-non-overlapping-data",
    "title": "16  Exporting scan results",
    "section": "16.6 Percentage non-overlapping data",
    "text": "16.6 Percentage non-overlapping data\n\n\n\n\n\n\nThe pnd export function call:\n\n\n\nexport(object, caption = NA, footnote = NA, filename = NA, select = c(“Case”, “PND”, “Total”, “Exceeds”), round = 2, …)\n\n\n\npnd(Huber2014) |&gt; export()\n\n\n\n\n\n\n\nTable\n\n\nPercentage Non-Overlapping Data\n\n\nCase\nPND\nTotal\nExceeds\n\n\n\n\nAdam\n0.00\n27\n0\n\n\nBerta\n0.00\n19\n0\n\n\nChristian\n15.15\n66\n10\n\n\nDavid\n65.15\n66\n43",
    "crumbs": [
      "More",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Exporting *scan* results</span>"
    ]
  },
  {
    "objectID": "ch_export.html#percentage-exceeding-the-trend",
    "href": "ch_export.html#percentage-exceeding-the-trend",
    "title": "16  Exporting scan results",
    "section": "16.7 Percentage exceeding the trend",
    "text": "16.7 Percentage exceeding the trend\n\n\n\n\n\n\nThe pet export function call:\n\n\n\nexport(object, caption = NA, footnote = NA, filename = NA, round = 1, …)\n\n\n\npet(exampleAB_decreasing) |&gt; export()\n\n\n\n\n\n\n\nTable\n\n\nPercent Exceeding the trend\n\n\nCase\nPET\nPercentage &gt; upper 95 CI\np (binomial test)\n\n\n\n\nPeter\n15.4\n0\n.99\n\n\nTony\n0.0\n0\n1.00\n\n\nBruce\n64.3\n0\n.21\n\n\n\nNote. Assumed increasing values in the B-phase; Binomial test alternative hypothesis: true probability &gt; 50%; Single-sided test.",
    "crumbs": [
      "More",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Exporting *scan* results</span>"
    ]
  },
  {
    "objectID": "ch_export.html#percentage-exceeding-the-median",
    "href": "ch_export.html#percentage-exceeding-the-median",
    "title": "16  Exporting scan results",
    "section": "16.8 Percentage exceeding the median",
    "text": "16.8 Percentage exceeding the median\n\n\n\n\n\n\nThe pem export function call:\n\n\n\nexport(object, caption = NA, footnote = NA, filename = NA, round = 2, …)\n\n\n\npem(Huber2014) |&gt; export()\n\n\n\n\n\n\n\nTable\n\n\nPercent Exceeding the Median\n\n\nCase\nPEM\npositives\ntotal\nbinom.p\n\n\n\n\nAdam\n25.93\n7\n27\n1\n\n\nBerta\n15.79\n3\n19\n1\n\n\nChristian\n98.48\n65\n66\n0\n\n\nDavid\n96.97\n64\n66\n0",
    "crumbs": [
      "More",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Exporting *scan* results</span>"
    ]
  },
  {
    "objectID": "ch_export.html#nonoverlap-of-all-pairs",
    "href": "ch_export.html#nonoverlap-of-all-pairs",
    "title": "16  Exporting scan results",
    "section": "16.9 Nonoverlap of all pairs",
    "text": "16.9 Nonoverlap of all pairs\n\n\n\n\n\n\nThe nap export function call:\n\n\n\nexport(object, caption = NA, footnote = NA, filename = NA, select = c(“Case”, “NAP”, “NAP Rescaled”, “w”, “p”, “d”, “R²”), round = 2, …)\n\n\n\nnap(Huber2014) |&gt; export()\n\n\n\n\n\n\n\nTable\n\n\nNonoverlap of all pairs\n\n\nCase\nNAP\nNAP Rescaled\nw\np\nd\nR²\n\n\n\n\nAdam\n28.15\n-43.70\n194.0\n.97\n-0.69\n0.11\n\n\nBerta\n17.11\n-65.79\n157.5\n.99\n-1.00\n0.20\n\n\nChristian\n86.97\n73.94\n86.0\n&lt;.001\n1.70\n0.42\n\n\nDavid\n91.74\n83.48\n54.5\n&lt;.001\n2.06\n0.51",
    "crumbs": [
      "More",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Exporting *scan* results</span>"
    ]
  },
  {
    "objectID": "ch_export.html#tau-u",
    "href": "ch_export.html#tau-u",
    "title": "16  Exporting scan results",
    "section": "16.10 Tau-U",
    "text": "16.10 Tau-U\n\n\n\n\n\n\nThe tauu export function call:\n\n\n\nexport(\n  object,\n  caption = NA,\n  footnote = NA,\n  filename = NA,\n  select = “auto”,\n  meta = FALSE,\n  round = 3,\n  decimals = 3,\n  …\n)\n\n\nSet the argument meta = TRUE to get the results of the meta analysis or set meta = FALSE (the default) to get a table with each case.\n\ntau_u(exampleAB_decreasing) |&gt; export()\n\n\n\n\n\n\n\nTable\n\n\nTau-U analyses\n\n\nModel\nTau\nCI lower\nCI upper\nZ\np\n\n\n\n\nPeter\n\n\nA vs. B\n−0.978\n−0.991\n−0.944\n−3.530\n&lt;.001\n\n\nTrend A\n−0.293\n−0.857\n0.590\n−0.911\n.36\n\n\nTrend B\n−0.104\n−0.619\n0.474\n−0.490\n.62\n\n\nA vs. B - Trend A\n−0.574\n−0.810\n−0.176\n−3.184\n&lt;.01\n\n\nA vs. B + Trend B\n−0.546\n−0.796\n−0.136\n−3.228\n&lt;.01\n\n\nA vs. B + Trend B - Trend A\n−0.483\n−0.762\n−0.051\n−2.957\n&lt;.01\n\n\nTony\n\n\nA vs. B\n−0.979\n−0.992\n−0.947\n−3.630\n&lt;.001\n\n\nTrend A\n−0.182\n−0.786\n0.600\n−0.623\n.53\n\n\nTrend B\n−0.092\n−0.633\n0.509\n−0.413\n.67\n\n\nA vs. B - Trend A\n−0.584\n−0.816\n−0.191\n−3.282\n&lt;.01\n\n\nA vs. B + Trend B\n−0.575\n−0.811\n−0.177\n−3.367\n&lt;.001\n\n\nA vs. B + Trend B - Trend A\n−0.504\n−0.774\n−0.079\n−3.087\n&lt;.01\n\n\nBruce\n\n\nA vs. B\n−0.976\n−0.991\n−0.940\n−3.383\n&lt;.001\n\n\nTrend A\n0.067\n−0.788\n0.833\n0.188\n.85\n\n\nTrend B\n−0.331\n−0.733\n0.242\n−1.645\n.10\n\n\nA vs. B - Trend A\n−0.607\n−0.827\n−0.225\n−3.345\n&lt;.001\n\n\nA vs. B + Trend B\n−0.616\n−0.832\n−0.238\n−3.691\n&lt;.001\n\n\nA vs. B + Trend B - Trend A\n−0.596\n−0.822\n−0.209\n−3.668\n&lt;.001\n\n\n\nNote. Method is ’ complete ’. Analyses based on Kendall’s Tau b . 95 % CIs for tau are reported.\n\n\n\n\n\n\n\n\n\ntau_u(exampleAB_decreasing) |&gt; export(meta = TRUE)\n\n\n\n\n\n\n\nTable\n\n\nOverall Tau-U\n\n\nModel\nTau U\nse\nCI lower\nCI upper\nz\np\n\n\n\n\nA vs. B\n−0.978\n0.140\n−0.987\n−0.962\n−16.036\n&lt;.001\n\n\nA vs. B - Trend A\n−0.588\n0.140\n−0.740\n−0.381\n−4.822\n&lt;.001\n\n\nA vs. B + Trend B\n−0.579\n0.140\n−0.733\n−0.369\n−4.725\n&lt;.001\n\n\nA vs. B + Trend B - Trend A\n−0.530\n0.140\n−0.698\n−0.305\n−4.210\n&lt;.001\n\n\n\nNote. Method is ’ complete ’. Analyses based on Kendall’s Tau b . 95 % CIs for tau are reported.",
    "crumbs": [
      "More",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Exporting *scan* results</span>"
    ]
  },
  {
    "objectID": "ch_export.html#between-case-standardized-mean-difference",
    "href": "ch_export.html#between-case-standardized-mean-difference",
    "title": "16  Exporting scan results",
    "section": "16.11 Between case standardized mean difference",
    "text": "16.11 Between case standardized mean difference\n\n\n\n\n\n\nThe bcsmd export function call:\n\n\n\nexport(object, caption = NA, footnote = NA, filename = NA, digits = 2, round = 2, …)\n\n\n\nbetween_smd(exampleA1B1A2B2) |&gt; export()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTable\n\n\nBetween-Case Standardized Mean Difference\n\n\nEffect\nBC-SMD\nse\n\nCI(95%)\n\n\n\nLL\nUL\n\n\n\n\nBase model\n\n\nphaseB1\n2.90\n0.25\n2.41\n3.38\n\n\nphaseA2\n0.69\n0.25\n0.19\n1.19\n\n\nphaseB2\n2.82\n0.24\n2.35\n3.29\n\n\nFull plm model\n\n\nphaseB1\n2.96\n0.43\n2.11\n3.81\n\n\nphaseA2\n0.64\n0.55\n-0.44\n1.71\n\n\nphaseB2\n2.34\n0.68\n1.02\n3.66\n\n\n\nNote. CI = confidence interval; LL = lower limit; UL = upper limit; Method: REML.",
    "crumbs": [
      "More",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Exporting *scan* results</span>"
    ]
  },
  {
    "objectID": "ch_export.html#piecewise-linear-models",
    "href": "ch_export.html#piecewise-linear-models",
    "title": "16  Exporting scan results",
    "section": "16.12 Piecewise linear models",
    "text": "16.12 Piecewise linear models\n\n\n\n\n\n\nThe plm export function call:\n\n\n\nexport(\n  object,\n  caption = NA,\n  footnote = NA,\n  filename = NA,\n  nice = TRUE,\n  ci = 0.95,\n  q = FALSE,\n  round = 2,\n  r_squared = getOption(“scan.rsquared”),\n  …\n)\n\n\n\nplm(exampleA1B1A2B2$Pawel) |&gt; export()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTable\n\n\nPiecewise-regression model predicting ‘values’\n\n\nParameter\nB\n\nCI(95%)\n\nSE\nt\np\ndelta R²\n\n\nLL\nUL\n\n\n\n\nIntercept\n12.69\n6.18\n19.20\n3.32\n3.82\n&lt;.001\n\n\n\nTrend\n0.22\n-0.99\n1.44\n0.62\n0.36\n.72\n0.00\n\n\nLevel B1\n16.28\n6.31\n26.26\n5.09\n3.20\n&lt;.001\n0.12\n\n\nLevel A2\n1.48\n-18.81\n21.76\n10.35\n0.14\n.89\n0.00\n\n\nLevel B2\n11.45\n-20.49\n43.40\n16.30\n0.70\n.49\n0.01\n\n\nSlope B1\n-1.41\n-3.13\n0.32\n0.88\n-1.60\n.12\n0.03\n\n\nSlope A2\n-1.10\n-2.83\n0.62\n0.88\n-1.25\n.22\n0.02\n\n\nSlope B2\n-1.08\n-2.81\n0.64\n0.88\n-1.23\n.23\n0.02\n\n\n\nNote. F(7, 32) = 7.86; p = 0.000; R² = 0.632; Adjusted R² = 0.552; AIC = 261; LL = lower limit; UL = upper limit; Slope estimation method = W; Contrasts for the level and slope effects are coded with the first phase as the reference.",
    "crumbs": [
      "More",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Exporting *scan* results</span>"
    ]
  },
  {
    "objectID": "ch_export.html#hierarchical-piecewise-regressions",
    "href": "ch_export.html#hierarchical-piecewise-regressions",
    "title": "16  Exporting scan results",
    "section": "16.13 Hierarchical piecewise regressions",
    "text": "16.13 Hierarchical piecewise regressions\n\n\n\n\n\n\nThe hplm export function call:\n\n\n\nexport(\n  object,\n  caption = NA,\n  footnote = NA,\n  filename = NA,\n  round = 2,\n  nice = TRUE,\n  casewise = FALSE,\n  …\n)\n\n\n\nexampleAB_50  |&gt; \n  add_l2(exampleAB_50.l2)  |&gt; \n  hplm(lr.test = TRUE, random.slopes = TRUE)  |&gt; \n  export()\n\n\n\n\n\n\n\nTable\n\n\nHierarchical Piecewise Linear Regression predicting variable ‘values’\n\n\nPredictors\nB\nSE\ndf\nt\np\n\n\n\n\nFixed effects\n\n\nIntercept\n48.21\n1.4\n1328\n34.5\n&lt;.001\n\n\nTrend\n0.62\n0.11\n1328\n5.52\n&lt;.001\n\n\nLevel B\n13.87\n0.89\n1328\n15.51\n&lt;.001\n\n\nSlope B\n0.86\n0.12\n1328\n7.43\n&lt;.001\n\n\nRandom effects\n\n\n\nSD\nL\ndf\np\n\n\n\nIntercept\n9.35\n348.85\n4\n&lt;.001\n\n\n\nTrend\n0.1\n0.83\n4\n.93\n\n\n\nLevel B\n4.54\n42.82\n4\n&lt;.001\n\n\n\nSlope B\n0.13\n0.76\n4\n.94\n\n\n\nResidual\n4.97\nNA\nNA\nNA\n\n\n\nModel\n\n\nAIC\n8693.2\n\n\n\n\n\n\nBIC\n8771.7\n\n\n\n\n\n\nICC\n0.29\nL = 339\np &lt;.001\n\n\n\n\n\nNote. Estimation method ML; Slope estimation method = W; Contrasts for the level and slope effects are coded with the first phase as the reference; N = 50 cases.",
    "crumbs": [
      "More",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Exporting *scan* results</span>"
    ]
  },
  {
    "objectID": "ch_export.html#multivariate-piecewise-regressions",
    "href": "ch_export.html#multivariate-piecewise-regressions",
    "title": "16  Exporting scan results",
    "section": "16.14 Multivariate piecewise regressions",
    "text": "16.14 Multivariate piecewise regressions\n\n\n\n\n\n\nThe mplm export function call:\n\n\n\nexport(\n  object,\n  caption = NA,\n  footnote = NA,\n  filename = NA,\n  nice = TRUE,\n  std = FALSE,\n  decimals = 2,\n  …\n)\n\n\n\nLeidig2018 |&gt; \n  select_cases(\"1a1\") |&gt; \n  mplm(dvar = c(\"academic_engagement\", \"disruptive_behavior\")) |&gt; \n  export(std = TRUE)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTable\n\n\nMultivariate Piecewise-Regression Model\n\n\nParameter\n\nDependent variables\n\nPillai\nF\np\n\n\nacademic_engagement\ndisruptive_behavior\n\n\n\n\nIntercept\n0.00\n0.00\n0.28\n15.48\n&lt;.001\n\n\nTrend\n-5.98\n5.11\n0.03\n1.28\n.28\n\n\nLevel B\n0.58\n-0.77\n0.19\n9.18\n&lt;.001\n\n\nSlope B\n5.95\n-4.90\n0.03\n1.27\n.28\n\n\n\nNote. Pillai = 0.35; F(6, 158) = 5.57; p = 0.000; Predictors are standardized.",
    "crumbs": [
      "More",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Exporting *scan* results</span>"
    ]
  },
  {
    "objectID": "ch_export.html#bayesian-regression-analysis",
    "href": "ch_export.html#bayesian-regression-analysis",
    "title": "16  Exporting scan results",
    "section": "16.15 Bayesian regression analysis",
    "text": "16.15 Bayesian regression analysis\n\n\n\n\n\n\nThe bplm export function call:\n\n\n\nexport(object, caption = NA, footnote = NA, filename = NA, round = 2, nice = TRUE, …)\n\n\n\nexampleAB_50  |&gt; \n  bplm(random_level = TRUE)  |&gt; \n  export()\n\n\n\n\n\n\n\nTable\n\n\nBayesian Multilevel Piecewise Linear Regression predicting variable ‘values’\n\n\nPredictors\nB\nlower 95% CI\nupper 95% CI\nsample size\np\n\n\n\n\nFixed effects (B-Structure)\n\n\nIntercept\n48.23\n45.37\n50.87\n1000\n&lt;.01\n\n\nTrend\n0.62\n0.4\n0.82\n1000\n&lt;.01\n\n\nLevel B\n13.91\n12.4\n15.64\n1000\n&lt;.01\n\n\nSlope B\n0.86\n0.64\n1.07\n1000\n&lt;.01\n\n\nRandom effects (G-Structure)\n\n\n\nSD\nlower 95% CI\nupper 95% CI\n\n\n\n\nIntercept\n9.85\n7.76\n11.72\n\n\n\n\nLevel B\n3.98\n2.92\n5.01\n\n\n\n\nResiduals (R-Structure)\n\n\n\nSD\nlower 95% CI\nupper 95% CI\n\n\n\n\nResiduals\n5.01\n4.82\n5.19\n\n\n\n\nModel\n\n\nDIC\n8459.8\n\n\n\n\n\n\n\nNote. Slope estimation method = W; Contrasts for the level and slope effects are coded with the first phase as the reference; N = 50 cases.",
    "crumbs": [
      "More",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Exporting *scan* results</span>"
    ]
  },
  {
    "objectID": "ch_export.html#sec-power",
    "href": "ch_export.html#sec-power",
    "title": "16  Exporting scan results",
    "section": "16.16 Power analyses",
    "text": "16.16 Power analyses\n\n\n\n\n\n\nThe power_test export function call:\n\n\n\nexport(object, caption = NA, footnote = NA, filename = NA, round = 3, …)\n\n\n\ndesign &lt;- design(\n  n = 1, phase_design = list(A = 6, B = 9),\n  rtt = 0.8, level = 1.4, trend = 0.05\n)\nset.seed(124)\npower_test(design, n_sim = 10) |&gt; export()\n\n\n\n\n\n\n\nTable\n\n\nTest power in percent\n\n\nMethod\nPower\nAlpha Error\nAlpha:Beta\nCorrect\n\n\n\n\nplm_level\n80\n10\n1:2.0\n85\n\n\nrand\n90\n30\n1:0.3\n80\n\n\ntauU\n100\n10\n1:0.0\n95",
    "crumbs": [
      "More",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Exporting *scan* results</span>"
    ]
  },
  {
    "objectID": "ch_shinyscan.html",
    "href": "ch_shinyscan.html",
    "title": "17  Graphical user interface - Shinyscan",
    "section": "",
    "text": "17.1 Start shinyscan\nThe scan package includes a graphical user interface (GUI) that enables users to perform a wide range of operations and analyses supported by the package. This GUI is implemented as a Shiny application that runs in a standard web browser.\nshinyscan was originally developed as a tool for teaching purposes. To facilitate this, the application displays the corresponding R syntax for each operation executed via the interface. Beyond its pedagogical utility, shinyscan is also well-suited for exploratory data analysis, and the visual presentation of results with immediate feedback.\nTo run the shiny app, you need to install two additional packages: shiny and scplot.\ninstall.packages(c(\"shiny\", \"scplot\"))\nYou can start the app by executing the function shinyscan() from the scan package. Based on your system settings, the app will immediately be opened within your standard browser or in R-Studios internal viewer pane:\nIn case the app starts within the internal viewer pane, I recommend that you click Open in Browser.",
    "crumbs": [
      "More",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Graphical user interface - Shinyscan</span>"
    ]
  },
  {
    "objectID": "ch_shinyscan.html#start-shinyscan",
    "href": "ch_shinyscan.html#start-shinyscan",
    "title": "17  Graphical user interface - Shinyscan",
    "section": "",
    "text": "Viewer pane",
    "crumbs": [
      "More",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Graphical user interface - Shinyscan</span>"
    ]
  },
  {
    "objectID": "ch_shinyscan.html#data-menue",
    "href": "ch_shinyscan.html#data-menue",
    "title": "17  Graphical user interface - Shinyscan",
    "section": "18.1 Data menue",
    "text": "18.1 Data menue\n\n18.1.1 New / Edit\nThe Data tab in shinyscan allows you to manually create single-case datasets (scdf, see Section 3.2). This is especially useful for small datasets, as it avoids the need to write R code. The interface contains several input fields:\nSelect case\n\nTo add a new case, choose New case.\nTo edit an existing case, select the case from the list. The corresponding data will then be loaded into the input fields below.\n\nValues\nProvide the central dependent variable and phase structure of the scdfs with the values of a named vector. For example, A = 1,2,3,4, B = 8,7,8,9,8,7, C = 8,7,6,7,6,5. You can use line breaks to structure your data entry (you can remove the comma at the end of each line if you wish):\nA = 1,2,3,4\nB = 8,7,8,9,8,7\nC = 8,7,6,7,6,5\nMeasurement times\nHere you can optionally enter measurement times. If no measurement times are specified, they are automatically set to 1, 2, 3, ..., n.\nAdditional variables\nAdd additional variables in the style: [varname] = value, value, value, value, ....\nIf you want to add several variables, use line breaks:\nengagement = 1,5,4,3,6,5\nsupport = 3,4,2,5,6,4\nCase name\nYou may provide an optional name for the case. If left empty, a random name will be generated.\nA set of buttons is located below the input fields:\n\nSave case: Adds the new case to the existing scdf when -new case- is selected under Select case. If an existing case is selected, it will be overwritten.\nRemove case: Deletes the selected case.\nClear all cases: Deletes the entire scdf.\nClear input fields: Clears all entries from the input fields above.\n\nAfter clicking Save case, a summary of the current scdf appears in the output area on the right.\n\nYou can continue to add several more cases.\nClick Save scdf to save the scdf to your local hard drive. By default, the output file name includes the prefix scdf, the number of cases, the phase names, and the current date and time (e.g., scdf-01-ABC-250322-130744.rds). Files are saved in R’s standard .rds format. Both the file format and file name can be changed in the Settings tab.\nIf you want to see the R syntax, go to the Settings tab and switch on Show scdf syntax:\n\nGo back to Data –&gt; New / Edit and the output will show R syntax:\n\n\n\n18.1.2 Load an example data file\nGo to Data -&gt; Load and select an example from Choose example:\n\nA summary of the selected scdf is displayed in the output pane\nNow, if you want to import that file, click Import scdf below. Caution: This will also replace a previously generated or loaded scdf file.\nYou can now continue with the example scdf or go to Data -&gt; New /Edit to modify the dataset.\n\n\n18.1.3 Load existing data files\nSwitch to Data -&gt; Load:\n\nClick Choose file and select a file from your hard disk.\nFour file types are accepted:\n\nA comma separated values .csv file (see Section 3.4.1 for more information on the structure of these files).\nAn Microsoft Excel .xlsx file (see Section 3.4.1 for more information on the structure of these files).\nAn .rds file containing an scdf (e.g., created with save scdf within the shiny app). See Section 3.3.\nAn .R file containing a syntax for creating an scdf. This text file must create an object called study which contains an scdf. Here is an example:\n\ncase1 &lt;- scdf(c(A = 58, 56, 60, 63, B = 51, 45, 44, 59, 45, 39, 83))\ncase2 &lt;- scdf(c(A = 47, 41, 47, 52, 54, 65, B = 55, 37, 51, 60, 60, 65, 55, 46))\n\nstudy &lt;- c(case1, case2) \nIf your data file already contains a valid scdf object (e.g. for rds or R files), the output pane will give you specific information about the data file. Click Import scdf if you want to import the data file.\nIf your data file contains a data frame, it will be displayed in the output area on the right. shinyscan will attempt to assign the columns to the corresponding elements of an scdf. Review these assignments and, if necessary, correct them using the drop-down menus.\n\nMissing values: Define which values in the data frame should be treated as missing during import (default: empty values and NA).\nSeparator: For .csv files, specify the character that separates values. By default, this is a comma. Adjust this setting if your file uses a different separator.\n\n\n\n\nAssign columns to corresponding scdf elements\n\n\nClick Import scdf to create and import an scdf from the data. The resulting scdf will be displayed in the output pane on the right.",
    "crumbs": [
      "More",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Graphical user interface - Shinyscan</span>"
    ]
  },
  {
    "objectID": "ch_shinyscan.html#transform",
    "href": "ch_shinyscan.html#transform",
    "title": "17  Graphical user interface - Shinyscan",
    "section": "18.2 Transform",
    "text": "18.2 Transform\nThe Transform tab allows you to modify, create, recalculate etc. new variables from your scdf.\nIf you make any changes here, all operations in other tabs (stats, plot etc.) will be based on the transformed scdf.\nThis tab also displays the raw-data contained in the scdf.\n\nSelect cases\nSelect cases either by case number or by case name. Use commas to select multiple cases and a colon to select ranges of cases (e.g. 1, 7:9, 10).\nCombine phases\nThis option is specifically designed for later analyses that compare exactly two phases (such as overlapping indices). If your scdf has more than two phases, you can define which phases are Phase A and Phase B. You can combine multiple phases into one phase by selecting multiple phases separated by commas. Select phases by number (1 for 1st, 2 for 2nd etc.) See Section 9.1.1 for more details.\nFilter measurements\nThis takes a logical expression defining a selection of measurements. You can use variable names within your scdf and R functions. For example, if you want to select all measurements above 6 you code values &gt; 6. If you want to select all measurements whose values are above the median of Phase B, you code values &gt; median(values[phase == \"B\"]). See Section 4.2.\nTransform variables\nThis section allows you to create and modify variables in a complex way. For example, for create a new variable with standardised values code values_std = scaled(values). See Section 4.3 for a detailed description of the syntax.\nSet dependent variable\nIf your scdf has multiple dependent variables, you can switch them for the further analysis and display in this field (see Chapter 6).\nClicking on Save transformed scdf will save the transformed scdf to your hard drive.",
    "crumbs": [
      "More",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Graphical user interface - Shinyscan</span>"
    ]
  },
  {
    "objectID": "ch_shinyscan.html#stats",
    "href": "ch_shinyscan.html#stats",
    "title": "17  Graphical user interface - Shinyscan",
    "section": "18.3 Stats",
    "text": "18.3 Stats\nThis tab is used to select and refine analysis procedures. The following section explains the functionality of the GUI. For detailed descriptions of the statistical methods, refer to the corresponding sections of this book.\nThe Statistic field provides access to a range of procedures. By default, the descriptives of the scdf are displayed.\n\nFor most procedures, specific output arguments can be defined. These arguments apply to the corresponding print() function (text output) or export() function (HTML output). The default arguments for each function are shown in the Output arguments field. You can adjust these settings, and the output below will update immediately.\nFor example set flip = TRUE here to switch columns and rows:\n\nClick Save to export the output content. The save format (text, html, or docx) is based on the settings of the HTML switch and the Settings tab.\n\n18.3.1 Function arguments\nIf you choose a statistical procedure which takes additional arguments, a list of the available arguments will be provided on the left pane. Here is an example for the hierarchical piecewise regression function. All arguments are provided with their respective default settings (please refer to the respective section in this book or the help files for detailed explanations of the arguments):",
    "crumbs": [
      "More",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Graphical user interface - Shinyscan</span>"
    ]
  },
  {
    "objectID": "ch_shinyscan.html#plots",
    "href": "ch_shinyscan.html#plots",
    "title": "17  Graphical user interface - Shinyscan",
    "section": "18.4 Plots",
    "text": "18.4 Plots\nThe Plot tab lets you create graphs using the scplot function (see Chapter 5 for details).\nIf a valid scdf is loaded or created, a standard scplot appears by default.\n\nOn the left of the plot are two cards that show a few options that can be applied to the plot.\nOn the left side of the plot, two option cards allow you to adjust the graph:\n\nSelect case: Plot all cases or a specific case.\n\nThemes: Use three drop-down menus to apply full visual themes or selected theme elements.\n\nAdd legend: Adds a legend to the right of the plot.\n\nText size: Sets the base size of all text elements.\n\nY Axis: Define the minimum and maximum of the y-axis.\n\nX Axis: Set the increment of x-axis values. If labels overlap, choose a larger increment (e.g., 5 prints x-axis values 1, 6, 11, 16, 21…).\n\nExport: Specify width, height, and DPI. Click Save plot to export the plot as a PNG file. Filename and size can also be set in the Settings tab.\n\nBaseline: Adds baseline statistics to the graph and extrapolates them to all other phases.\n\nPhases: Adds statistics for each phase separately to the plot.\n\nCurves: Adds a smoothed curve to the data.\n\nAdd variable: Adds a second variable from the scdf to the plot.",
    "crumbs": [
      "More",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Graphical user interface - Shinyscan</span>"
    ]
  },
  {
    "objectID": "ch_shinyscan.html#settings",
    "href": "ch_shinyscan.html#settings",
    "title": "17  Graphical user interface - Shinyscan",
    "section": "18.5 Settings",
    "text": "18.5 Settings\nThe Settings tab allows users to customize various aspects of shinyscan’s behavior. The available options are grouped by functional area:\n\n\n18.5.1 Data\nShow scdf syntax defines whether the output in the Data tab is shown as R syntax (convert(scdf)) or as a summary (summary(scdf)).\nPhase structure inline specifies the coding of the phase structure when Show scdf syntax is switched on.\nPrefix save filename defines a prefix used for automatically generated filename when exporting the scdf.\nSave format sets the export format of the scdf file.\n\n\n18.5.2 Transform\nPrefix save filename defines a prefix used for automatically generated filename when exporting the transformed scdf.\nSave format sets the export format of the scdf file.\n\n\n18.5.3 Stats\nShow short description gives a short description of the selected statistical function when switched on.\nShow defaults If switched on, the generated R syntax includes all function arguments, including those set to their default values. This can be useful for teaching and documentation.\nRename predictors allows choosing among three naming conventions for predictors in the regression output.\nPrefix save filename defines a prefix filenames generated when exporting statistical results.\nSave format sets the export format.\n\n\n18.5.4 Plot\nPrefix save filename defines a prefix for filenames when exporting plots.\n\n\n18.5.5 General\nHtml_engine specifies the internal engine used to generate HTML tables, either via the gt or kableExtra package (for scdf and statistical results). Only gt currently supports exporting tables in .docx format. The kableExtra engine is retained for compatibility purposes but may be deprecated in future versions of the package.",
    "crumbs": [
      "More",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Graphical user interface - Shinyscan</span>"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Brossart, D. F., Laird, V. C., & Armstrong, T. W. (2018).\nInterpreting Kendall’s Tau and Tau-U for single-case\nexperimental designs. Cogent Psychology, 5(1), 1–26.\nhttps://doi.org/10.1080/23311908.2018.1518687\n\n\nBulté, I., & Onghena, P. (2008). An r package for single-case\nrandomization tests. Behavior Research Methods, 40(2),\n467–478. https://doi.org/10.3758/BRM.40.2.467\n\n\nBulté, I., & Onghena, P. (2009). Randomization tests for\nmultiple-baseline designs: An extension of the SCRT-r package.\nBehavior Research Methods, 41(2), 477–485. https://doi.org/10.3758/BRM.41.2.477\n\n\nCooper, H., Hedges, L. V., & Valentine, J. C. (2009). Handbook\nof research synthesis and meta-analysis, the. Retrieved from https://www.jstor.org/stable/10.7758/9781610441384\n\n\nFieller, E. C., Hartley, H. O., & Pearson, E. S. (1957). Tests for\nrank correlation coefficients. Biometrika, 44,\n470–481.\n\n\nHedges, L. V., Pustejovsky, J. E., & Shadish, W. R. (2012). A\nstandardized mean difference effect size for single case designs.\nResearch Synthesis Methods, 3(3), 224–239. https://doi.org/10.1002/jrsm.1052\n\n\nHotelling, H. (1953). New Light on the Correlation Coefficient and its\nTransforms. Journal of the Royal Statistical Society: Series B\n(Methodological), 15(2), 193–225. https://doi.org/10.1111/j.2517-6161.1953.tb00135.x\n\n\nHuitema, B. E., & Mckean, J. W. (2000). Design specification issues\nin time-series intervention models. Educational and Psychological\nMeasurement, 60(1), 38–58. Retrieved from http://epm.sagepub.com/content/60/1/38.short\n\n\nIglewicz, B., & Hoaglin, D. C. (1993). How to detect and handle\noutliers. Milwaukee, Wis. : ASQC Quality Press.\n\n\nParker, Richard I., Hagan-Burke, S., & Vannest, K. (2007).\nPercentage of All Non-Overlapping\nData (PAND) An\nAlternative to PND. The Journal of Special\nEducation, 40(4), 194–204. Retrieved from http://sed.sagepub.com/content/40/4/194.short\n\n\nParker, Richard I., & Vannest, K. (2009). An improved effect size\nfor single-case research: Nonoverlap of all pairs.\nBehavior Therapy, 40(4), 357–367. Retrieved from http://www.sciencedirect.com/science/article/pii/S0005789408000816\n\n\nParker, Richard I., Vannest, K. J., & Brown, L. (2009). The\nimprovement rate difference for single-case research. Exceptional\nChildren, 75(2), 135150. Retrieved from http://cec.metapress.com/index/35U1148028323H3H.pdf\n\n\nParker, Richard I., Vannest, K. J., & Davis, J. L. (2011). Effect\nSize in Single-Case\nResearch: A Review of\nNine Nonoverlap Techniques.\nBehavior Modification, 35(4), 303–322. https://doi.org/10.1177/0145445511399147\n\n\nParker, Richard I., Vannest, K. J., Davis, J. L., & Sauber, S. B.\n(2011b). Combining Nonoverlap and Trend for\nSingle-Case Research:\nTau-U. Behavior Therapy,\n42(2), 284–299. https://doi.org/10.1016/j.beth.2010.08.006\n\n\nParker, Richard I., Vannest, K. J., Davis, J. L., & Sauber, S. B.\n(2011a). Combining nonoverlap and trend for single-case research: Tau-u.\nBehavior Therapy, 42(2), 284–299. https://doi.org/10.1016/j.beth.2010.08.006\n\n\nPustejovsky, J. E. (2016). What is tau-u? Retrieved from https://www.jepusto.com/what-is-tau-u/\n\n\nPustejovsky, J. E. (2019). Procedural sensitivities of effect sizes for\nsingle-case designs with directly observed behavioral outcome measures.\nPsychological Methods, 24(2), 217–235. https://doi.org/10.1037/met0000179\n\n\nR Core Team. (2025). R: A language and environment for statistical\ncomputing. Retrieved from https://www.R-project.org/\n\n\nRStudio Team. (2018). RStudio: Integrated development environment\nfor r. Retrieved from http://www.rstudio.com/\n\n\nScruggs, T. E., Mastropieri, M. A., & Casto, G. (1987). The\nQuantitative Synthesis of\nSingle-Subject Research\nMethodology and Validation. Remedial and\nSpecial Education, 8(2), 24–33. https://doi.org/10.1177/074193258700800206\n\n\nSiegel, A. F. (1982). Robust Regression Using Repeated\nMedians. Biometrika, 69(1), 242–244. https://doi.org/10.2307/2335877\n\n\nTarlow, K. R. (2016). An Improved Rank Correlation Effect Size\nStatistic for Single-Case Designs:\nBaseline Corrected Tau. Behavior Modification,\n41(4), 427–467. https://doi.org/10.1177/0145445516676750\n\n\nTarlow, K. R. (2017). An Improved Rank Correlation Effect Size Statistic\nfor Single-Case Designs: Baseline Corrected Tau. Behavior\nModification, 41(4), 427–467. https://doi.org/10.1177/0145445516676750\n\n\nWilbert, Juergen. (2025). Scplot: Plot function for single-case data\nframes. https://doi.org/10.32614/CRAN.package.scplot\n\n\nWilbert, Juergen, & Lueke, T. (2025). Scan: Single-case data\nanalyses for single and multiple baseline designs. Retrieved from\nhttps://github.com/jazznbass/scan/\n\n\nWilbert, Jürgen, Lüke, T., & Börnert-Ringleb, M. (2022). Statistical\npower of piecewise regression analyses of single-case experimental\nstudies addressing behavior problems. Frontiers in Education\nEducational Psychology, 7. https://doi.org/10.3389/feduc.2022.917944\n\n\nWise, E. A. (2004). Methods for analyzing psychotherapy outcomes:\nA review of clinical significance, reliable change, and\nrecommendations for future directions. Journal of Personality\nAssessment, 82(1), 50–59. Retrieved from http://www.tandfonline.com/doi/abs/10.1207/s15327752jpa8201_10",
    "crumbs": [
      "References"
    ]
  },
  {
    "objectID": "app_some_things.html",
    "href": "app_some_things.html",
    "title": "Appendix A — Some things about R",
    "section": "",
    "text": "A.1 Basic R\nR is a programming language optimized for statistical purposes. It was created in 1992 by Ross Ihaka and Robert Gentleman at the University of Auckland. Since then it has been developed continuously and became one of the leading statistical software programs. R is unmatched in its versatility. It is used for teaching introductory courses into statistics up to doing the most sophisticated mathematical analysis. It has become the defacto standard in many scientific disciplines from the natural to the social sciences.\nR is completely community driven . That is, it is developed and extended by anybody who likes to participate . It comes at no costs and can be downloaded for free for all major and many minor platforms at www.r-project.org. Yet, it is as reliable as other proprietary software like Mplus, STATA, SPSS etc . You can tell from my writing that is hard not to become an R-fan when you are into statistics :-)\nR can be used in at least two ways:\nIt is the second function that is the origin of R’s huge success and versatility. New statistical procedures and functions can be published to be used for everyone in so called packages. A package usually contains several functions, help files and example data-sets. Hundreds of such packages are available to help in all kinds of specialized analyses. The basic installation of R comes with a large variety of packages per installed. New packages can most of the times be easily installed from within R. Admittedly, if you must have the latest developmental version of a new package installation sometimes can get a bit more complex. But with a bit of help and persistence it is not to difficult to accomplish.\nR is a script language. That is, you type in text and let R execute the commands you wrote down. Either you work in a console or a textfile. In a console the command will be executed every time you press the RETURN-key. In a textfile you type down your code, mark the part you like to be executed, and run that code (with a click or a certain key). The latter text files can be saved and reused for later R sessions. Therefore, usually you will work in a text file.\nA value is assigned to a variable with the &lt;- operator. Which should be read as an arrow rather than a less sign and a minus sign. A # is followed by a comment to make your code more understandable. So, what follows a # is not interpreted by R. A vector is a chain of several values. With a vector you could describe the values of a measurement series. The c function is used to build a vector (e.g., c(1, 2, 3, 4)). If you like to see the content of a variable you could use the print function. print(x) will display the content of the variable x. A shortcut for this is just to type variable name (and press return) x.\n# x is assigned the value 10:\nx &lt;- 10\n\n# See what's inside of x:\nx\n\n[1] 10\n\n# x is assigned a vector with three values:\nx &lt;- c(10, 11, 15)\n\n# ... and display the content of x:\nx\n\n[1] 10 11 15\nTwo important concepts in R are functions and arguments. A function is the name for a procedure that does something with the arguments that are provided by you. For example, the function mean calculated the mean. mean has an argument x which “expects” that you provide a vector (a series of values) from which it will calculate the mean. mean( x = c(1, 3, 5) ) will compute the mean of the values 1, 3, and 5 and return the result 3. Some functions can take several arguments. mean for example also takes the argument trim. For calculating a trimmed mean. mean( x = c(1, 1, 3, 3, 5, 6, 7, 8, 9, 9), trim = 0.1) will calculate the 10% trimmed mean of the provided values. The name of the first argument could be dropped. That is, mean( c(1, 3, 5) ) will be interpreted by R as mean( x = c(1, 3, 5) ). You could also provide a variable to an argument.\nvalues &lt;- c(1, 4, 5, 6, 3, 7, 7, 5)\nmean(x = values)\n\n[1] 4.75\n\n# or shorter:\nmean(values)\n\n[1] 4.75\nThe return value of a function can be assigned to a new variable instead:\ny &lt;- c(1, 4, 5, 6, 3, 7, 7, 5)\nres &lt;- mean(y)\n#now res contains the mean of y:\nres\n\n[1] 4.75\nEvery function in R has a help page written by the developers of a package. You can retrieve these pages with the help function or the short cut ?. help(\"mean\") will display the help page for the mean function. The quotation marks are necessary here because you do not provide a variable with the name mean but a word ‘mean’. The shortcut works ?mean. A bit confusingly, you do not need the quotation marks here.\nThis basic introduction should help you get started with the material in this book. If you need more details about R, you can search the web for books and online courses. Here are some recommendations:",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Some things about R</span>"
    ]
  },
  {
    "objectID": "app_some_things.html#basic-r",
    "href": "app_some_things.html#basic-r",
    "title": "Appendix A — Some things about R",
    "section": "",
    "text": "A somewhat technical but detailed introduction to R as a computer language:\nhttps://cran.r-project.org/doc/manuals/r-release/R-intro.pdf\nA didactically well developed page on R with a textbook, videos, excercises etc.\nhttps://alexd106.github.io/intro2R/index.html\nLearning R from the perspective of Data Science\nhttps://r4ds.hadley.nz/\nYou can also find R tutorials from within the R Studio application (on the Tutorial tab in the upper right pane).",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Some things about R</span>"
    ]
  },
  {
    "objectID": "app_changes.html",
    "href": "app_changes.html",
    "title": "Appendix B — Changes",
    "section": "",
    "text": "B.1 Important changes with version 0.53",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>Changes</span>"
    ]
  },
  {
    "objectID": "app_changes.html#important-changes-with-version-0.53",
    "href": "app_changes.html#important-changes-with-version-0.53",
    "title": "Appendix B — Changes",
    "section": "",
    "text": "B.1.1 Single-case studies with cases of varying phase design\nSometimes it is necessary to combine single-cases with different phase-designs into one single-case study (for instance when some cases include an extension phase and others do not). Various functions in scan now can handle such a data structure.\n\n\nB.1.2 Piping\nThe concept of piping is great for writing clean and intelligible code that is easier to debug. We imported the pipe function %&gt;% from the magrittr package. Since version 4.1, R has its own pipe operator implementation |&gt;. This is great and works fine with the scan package. But since the |&gt; Operator is not backwards compatible for R prior versions 4.1, we will stick with the %&gt;% for a while.\nTo allow for smooth “piping” we began adding some functions select_phases, subset, select_cases, set_var, set_dvar, set_mvar, set_pvar, and add_l2.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>Changes</span>"
    ]
  },
  {
    "objectID": "app_changes.html#important-changes-with-version-0.50",
    "href": "app_changes.html#important-changes-with-version-0.50",
    "title": "Appendix B — Changes",
    "section": "B.2 Important changes with version 0.50",
    "text": "B.2 Important changes with version 0.50\n\nB.2.1 New function names\nWith version 0.50 scan introduced new names for its functions. The old function names are still usable but they will return a “deprecated” warning telling you to use the new function names.\n(rtbl-aliases?) shows the changes.\n\n\n\n\nTable B.1: Scan previous and current function names\n\n\n\n\n\n\nCurrent function name\nPrevious function name\n\n\n\n\nautocorr\nautocorrSC\n\n\ncorrected_tau\ncorrected_tauSC\n\n\ndescribe [since v0.52]\ndescribeSC\n\n\nfill_missing\nfillmissingSC\n\n\noutlier\noutlierSC\n\n\noverlap\noverlapSC\n\n\npower_test\npower_testSC\n\n\nrand_test\nrandSC; rand.test\n\n\nranks\nrankSC\n\n\nrci\nrCi; rciSC\n\n\nshift\nshiftSC\n\n\nsmooth_cases\nsmoothSC\n\n\nstyle_plot\nstyle.plotSC; style_plotSC\n\n\ntau_u\ntauUSC\n\n\ntrend\ntrendSC\n\n\ntruncate_phase\ntruncateSC\n\n\n\n\n\n\n\n\n\n\nB.2.2 Change target variables in functions\nAll functions in R that analyze data now allow for temporarily changing dependent, phase, and measurement-time variables by adding three argument:\ndvar sets the dependent variable.\npvar sets the phase variable.\nmvar sets the measurement-time variable.\nFor example, overlap(exampleAB_add, dvar = \"depression\") will report overlap parameters for the variable depression while overlap(exampleAB_add) while take wellbeing as the dependent variable (as defined in the scdf).\nAfter finishing the analysis, the variables are set back to their original values as defined in the scdf.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>Changes</span>"
    ]
  },
  {
    "objectID": "app_default_settings.html",
    "href": "app_default_settings.html",
    "title": "Appendix C — Default settings",
    "section": "",
    "text": "Some of the default settings of scan can be changed with the options() argument. Table C.1 shows a complete list of options and their default values.\n\n# get the current value of an option\ngetOption(\"scan.print.rows\")\n\n[1] 15\n\n# set option to a different value\noptions(scan.print.rows = 5, scan.print.scdf.name = FALSE)\nprint(exampleAB)\n\n#A single-case data frame with three cases\n\n values mt phase ｜ values mt phase ｜ values mt phase ｜\n     54  1     A ｜     41  1     A ｜     55  1     A ｜\n     53  2     A ｜     59  2     A ｜     58  2     A ｜\n     56  3     A ｜     56  3     A ｜     53  3     A ｜\n     58  4     A ｜     51  4     A ｜     50  4     A ｜\n     52  5     A ｜     52  5     A ｜     52  5     A ｜\n# ... up to 15 more rows\n\noptions(scan.print.rows = 15, scan.print.scdf.name = TRUE)\nprint(exampleAB)\n\n#A single-case data frame with three cases\n\n Johanna: values mt phase ｜ Karolina: values mt phase ｜ Anja: values mt phase\n              54  1     A ｜               41  1     A ｜           55  1     A\n              53  2     A ｜               59  2     A ｜           58  2     A\n              56  3     A ｜               56  3     A ｜           53  3     A\n              58  4     A ｜               51  4     A ｜           50  4     A\n              52  5     A ｜               52  5     A ｜           52  5     A\n              61  6     B ｜               57  6     B ｜           55  6     B\n              62  7     B ｜               56  7     B ｜           68  7     B\n              71  8     B ｜               67  8     B ｜           68  8     B\n              66  9     B ｜               75  9     B ｜           81  9     B\n              64 10     B ｜               66 10     B ｜           67 10     B\n              78 11     B ｜               69 11     B ｜           78 11     B\n              70 12     B ｜               68 12     B ｜           73 12     B\n              74 13     B ｜               73 13     B ｜           72 13     B\n              82 14     B ｜               77 14     B ｜           78 14     B\n              77 15     B ｜               79 15     B ｜           81 15     B\n ｜\n ｜\n ｜\n ｜\n ｜\n ｜\n ｜\n ｜\n ｜\n ｜\n ｜\n ｜\n ｜\n ｜\n ｜\n ｜\n# ... up to five more rows\n\n\n\n\n\n\nTable C.1: Scan Options\n\n\n\n\n\n\nOption\nDefault\nWhat it does ...\n\n\n\n\nscan.print.cases\n\"fit\"\nMax number of cases printed for scdf objects\n\n\nscan.print.rows\n15\nMax number of rows printed for scdf objects\n\n\nscan.print.cols\n\"all\"\nMax number of columns printed for scdf objects\n\n\nscan.print.digits\n2\nMax number of digits printed for scdf objects\n\n\nscan.print.long\nFALSE\nIf TRUE, prints scdf objects in long format\n\n\nscan.print.scdf.name\nTRUE\nIf TRUE, prints case names of scdf\n\n\nscan.deprecated.warning\nFALSE\nWhen TRUE returns information on deprecated functions\n\n\nscan.export.kable\nlist(digits = 2, linesep = \"\", booktab = TRUE)\nList with default arguments for the kable argument of the export function\n\n\nscan.export.kable_styling\nlist(bootstrap_options = c(\"bordered\", \"condensed\"), full_width = FALSE, position = \"left\", latex_options = \"hold_position\", htmltable_class = \"lightable-classic\")\nList with default arguments for the kable_styling argument of the export function\n\n\nscan.export.engine\n\"gt\"\nNA\n\n\nscan.export.footnote.collapse\n\"; \"\nNA\n\n\nscan.export.title.prefix\nstructure(\"**Table**\", class = \"from_markdown\")\nNA\n\n\nscan.plot.style\n\"grid\"\nNA\n\n\nscan.print.bar\n\"｜\" |\nA |\n\n\nscan.rename.predictors\n\"full\"\nNA\n\n\nscan.rsquared\n\"delta\"\nNA\n\n\nscan.shiny.theme\n\"cerulean\"\nNA\n\n\nscan.string.dummy.phase\n\"phase\"\nNA\n\n\nscan.string.dummy.slope\n\"inter\"\nNA\n\n\n\n\n\n\n\n\n\nD Example datasets\n\n\n\n\nTable D.1: Example scdfs\n\n\n\n\n\n\nName\nInfo\nAuthor\n\n\n\n\nBeretvas2008\n\nBeretvas, S., & Chung, H. (2008). An evaluation of modified R2-change effect size indices for single-subject experimental designs. Evidence-Based Communication Assessment and Intervention, 2, 120-128.\n\n\nBorckardt2014\n\nBorckardt, J. J., & Nash, M. R. (2014). Simulation modelling analysis for small sets of single-subject data collected over time. Neuropsychological Rehabilitation, 24(3-4), 492-506.\n\n\nGrosche2011\nDirect instruction intervention on reading accuracy.\nGrosche, M. (2011). Effekte einer direkt-instruktiven Förderung der Lesegenauigkeit. Empirische Sonderpädagogik, 3(2), 147-161.\n\n\nGrosche2014\nData from a multiple material multi person intervention study on reading.\nMichael Grosche, Timo Lueke, and Juergen Wilbert\n\n\nGruenkeWilbert2014\nData from an intervention study on text comprehension.\nGruenke, M., Wilbert, J., & Stegemann-Calder, K. (2013). Analyzing the effects of story mapping on the reading comprehension of children with low intellectual abilities. Learning Disabilities: A Contemporary Journal, 11(2), 51-64.\n\n\nHuber2014\nBehavioral data (compliance in percent).\nChristian Huber\n\n\nHuitema2000\nExample from Huitema, B. E., & Mckean, J. W. (2000). Design specification issues in time-series intervention models. Educational and Psychological Measurement, 60(1), 38-58.\n\n\n\nLeidig2018\n\nLeidig, T., Casale, G., Wilbert, J., Hennemann, T., Volpe, R. J., Briesch, A., & Grosche, M. (2022). Individual, generalized, and moderated effects of the good behavior game on at-risk primary school students: A multilevel multiple baseline study using behavioral progress monitoring. Frontiers in Education, 7. https://www.frontiersin.org/articles/10.3389/feduc.2022.917138\n\n\nLeidig2018_l2\n\n\n\n\nLenz2013\n\nLenz, A. S. (2013). Calculating Effect Size in Single-Case Research: A Comparison of Nonoverlap Methods. Measurement and Evaluation in Counseling and Development, 46(1), 64-73.\n\n\nParker2007\n\nParker, R. I., Hagan-Burke, S., & Vannest, K. (2007). Percentage of All Non-Overlapping Data (PAND) An Alternative to PND. The Journal of Special Education, 40(4), 194-204.\n\n\nParker2009\n\nParker, R. I., Vannest, K. J., & Brown, L. (2009). The improvement rate difference for single-case research. Exceptional Children, 75(2), 135-150.\n\n\nParker2009b\n\nParker, R. I., & Vannest, K. (2009). An improved effect size for single-case research: Nonoverlap of all pairs. Behavior Therapy, 40(4), 357-367.\n\n\nParker2011\n\nParker, R. I., Vannest, K. J., Davis, J. L., & Sauber, S. B. (2011). Combining Nonoverlap and Trend for Single-Case Research: Tau-U. Behavior Therapy, 42(2), 284-299.\n\n\nParker2011b\n\nParker, R. I., Vannest, K. J., & Davis, J. L. (2011). Effect Size in Single-Case Research: A Review of Nine Nonoverlap Techniques. Behavior Modification, 35(4), 303-322. https://doi.org/10.1177/0145445511399147\n\n\nSSDforR2017\nExample from the SSDforR package.\nCharles Auerbach, PhD & Wendy Zeitlin, PhD; Yeshiva University, Wurzweiler school of social work.\n\n\nTarlow2017\n\nTarlow, K. R. (2017). An Improved Rank Correlation Effect Size Statistic for Single-Case Designs: Baseline Corrected Tau. Behavior Modification, 41(4), 427-467. https://doi.org/10.1177/0145445516676750\n\n\nWaddell2011\nExample from Waddell, D. E., Nassar, S. L., & Gustafson, S. A. (2011). Single-Case Design in Psychophysiological Research: Part II: Statistical Analytic Approaches. Journal of Neurotherapy, 15(2), 160 - 169.\n\n\n\nbyHeart2011\nData from university students learning vocabulary by heart and checking their progress with 20 flashcards each session.\nJuergen Wilbert, 2011\n\n\nexampleA1B1A2B2\n\n\n\n\nexampleA1B1A2B2_zvt\n\n\n\n\nexampleAB\nRandomly created data with normal distributed dependent variable.\n\n\n\nexampleABAB\nRandomly created data with uniform distribution.\n\n\n\nexampleABC\n\n\n\n\nexampleABC_150\nRandom data-set for testing out hplm. Level and slope effects vary.\n\n\n\nexampleABC_outlier\nRandom data-set based on exampleABC but with outliers.\n\n\n\nexampleAB_50\n\n\n\n\nexampleAB_50.l2\n\n\n\n\nexampleAB_add\nRandom data-set for testing out plm with additional variables.\n\n\n\nexampleAB_decreasing\nRandom data-set from a poisson distribution. Level effect is negative.\n\n\n\nexampleAB_mpd\nA multiple phase design study.\nJuergen Wilbert\n\n\nexampleAB_score\nRandom data-set for binomial data.\n\n\n\nexampleAB_simple\nA simple multiple baseline AB Design.\nJuergen Wilbert\n\n\nexample_A24\nNumber of injuries on a German autobahn before and after implementation of a speedlimit (130km/h).\nMinisterium fuer Infrastruktur und Landesplanung. Land Brandenburg.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>C</span>  <span class='chapter-title'>Default settings</span>"
    ]
  },
  {
    "objectID": "app_supseded_functions.html",
    "href": "app_supseded_functions.html",
    "title": "Appendix D — Supseded functions",
    "section": "",
    "text": "D.1 The plotSC plotting function\nAfter you build an scdf the plot command helps to visualize the data. When the scdf includes more than one case a multiple baseline figure is provided. Various arguments can be set to customize the appearance of the plot. Table D.1 gives an overview of all available arguments.\nplot(exampleA1B1A2B2_zvt)\n\n\n\n\nA simple plot does not need much.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>D</span>  <span class='chapter-title'>Supseded functions</span>"
    ]
  },
  {
    "objectID": "app_supseded_functions.html#sec-plotsc",
    "href": "app_supseded_functions.html#sec-plotsc",
    "title": "Appendix D — Supseded functions",
    "section": "",
    "text": "The plotSC function call:\n\n\n\nplotSC(\n  data,\n  dvar,\n  pvar,\n  mvar,\n  ylim = NULL,\n  xlim = NULL,\n  xinc = 1,\n  lines = NULL,\n  marks = NULL,\n  xlab = NULL,\n  ylab = NULL,\n  main = ““,\n  case.names = NULL,\n  style = getOption(”scan.plot.style”),\n  …\n)",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>D</span>  <span class='chapter-title'>Supseded functions</span>"
    ]
  },
  {
    "objectID": "app_supseded_functions.html#plot-axis",
    "href": "app_supseded_functions.html#plot-axis",
    "title": "Appendix D — Supseded functions",
    "section": "D.2 Plot axis",
    "text": "D.2 Plot axis\nLabels of the axes and for the phases can be changed with the xlab, ylab, and the phase.names arguments. The x- and y-scaling of the graphs are by default calculated as the minimum and the maximum of all included single cases. The xlim and the ylim argument are used to set specific values. The argument takes a vector of two numbers. The first for the lower and the second for the upper limit of the scale. In case of multiple single cases an NA sets the individual minimum or maximum for each case. Assume for example the study contains three single cases ylim = c(0, NA) will set the lower limit for all three single cases to 0 and the upper limit individually at the maximum of each case. The argument xinc sets the incremental steps for the x-axis ticks with corresponding values. For example xinc = 1 will set a tick for every measurement time increase of 1 while xinc = 5 will only set every ffith tick.\n\nplot(\n  exampleABC,\n  phase.names = c(\"Baseline\", \"Intervention\", \"Follow-Up\"),\n  case.names = c(\"First\", \"Second\", \"Third\"),\n  ylab = \"Frequency\",\n  xlab = \"Days\",\n  main = \"An example\",\n  ylim = c(0, 120),\n  xinc = 2\n)\n\n\n\n\nA plot with various axis specidications.\n\n\n\n\n\n\n\n\nTable D.1: Arguments of the plot function\n\n\n\n\n\n\nArgument\nWhat it does ...\n\n\n\n\ndata\nA single-case data frame.\n\n\nylim\nLower and upper limits of the y-axis\n\n\nxlim\nLower and upper limits of the x-axis.\n\n\nstyle\nA specific design for displaying the plot.\n\n\nlines\nA character or list defining one or more lines or curves to be plotted.\n\n\nmarks\nA list of parameters defining markings of certain data points.\n\n\nmain\nA figure title\n\n\nphase.names\nBy default phases are labeled as given in the phase variable. Use this argument to specify different labels: `phase.names = c('Baseline', 'Intervention')`.\n\n\ncase.names\nCase names. If not provided, names are taken from the scdf or left blank if the scdf does not contain case names.\n\n\nxlab\nThe label of the x-axis. The default is taken from the name of the measurement variable as provided by the scdf.\n\n\nylab\nThe labels of the y-axis. The default is taken from the name of the dependent variable as provided by the scdf.\n\n\nxinc\nAn integer. Increment of the x-axis. 1 : each mt value will be printed, 2 : every other value, 3 : every third values etc.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>D</span>  <span class='chapter-title'>Supseded functions</span>"
    ]
  },
  {
    "objectID": "app_supseded_functions.html#adding-lines",
    "href": "app_supseded_functions.html#adding-lines",
    "title": "Appendix D — Supseded functions",
    "section": "D.3 Adding lines",
    "text": "D.3 Adding lines\nExtra lines can be added to the plot using the lines argument. The lines argument takes several separate sub-arguments which have to be provided in a list. In its most simple form this list contains one element. lines = list(type = 'median') adds a line with the median of each phase to the plot. Additional arguments like col or lwd help to format these lines. For adding red thick median lines use the command lines = list(type = 'median', col = 'red', lwd = '2').\n\n\n\n\nTable D.2: Values of the lines argument\n\n\n\n\n\n\nArgument\nWhat it does ...\n\n\n\n\nmedian\nseparate lines for the medians of each phase\n\n\nmean\nseparate lines for the means of each phase. By default it is 10%-trimmed. Other trims can be set using a second parameter (e.g., `lines = list(type = 'mean', trim = 0.2)` draws a 20%-trimmed mean line).\n\n\ntrend\nSeparate lines for the trend of each phase.\n\n\ntrendA\nTrend line for phase A, extrapolated throughout the other phases\n\n\nmaxA\nLine at the level of the highest phase A score.\n\n\nminA\nLine at the level of the lowest phase A score.\n\n\nmedianA\nLine at the phase A median score.\n\n\nmeanA\nLine at the phase A 10%-trimmed mean score. Apply a different trim, by using the additional argument (e.g., `lines = list(type = 'meanA', trim = 0.2)`).\n\n\nmovingMean\nDraws a moving mean curve, with a specified lag: `lines = list(type = 'movingMean', lag = 2)`. Default is a lag 1 curve.\n\n\nmovingMedian\nDraws a moving median curve, with a specified lag: `lines = list(type = 'movingMedian', lag = 3).` Default is a lag 1 curve.\n\n\nloreg\nDraws a non-parametric local regression line. The proportion of data influencing each data point can be specified using `lines = list(type = 'loreg', f = 0.66)`. The default is 0.5.\n\n\nlty\nLine type. Examples are: 'solid','dashed', 'dotted'.\n\n\nlwd\nLine thickness, e.g., `lwd = 4`.\n\n\ncol\nLine colour, e.g., `col = 'red'`.\n\n\n\n\n\n\n\n\n\nplot(\n  exampleAB, \n  lines = list(\n    list(type = \"median\", col = \"red\", lwd = 0.5),\n    list(type = \"trend\", col = \"blue\", lty = \"dashed\", lwd = 2),\n    list(type = \"loreg\", f = 0.2, col = \"green\", lty = \"solid\", lwd = 1)\n  )\n)\n\n\n\n\nA plot with various visual aids",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>D</span>  <span class='chapter-title'>Supseded functions</span>"
    ]
  },
  {
    "objectID": "app_supseded_functions.html#mark-data-points",
    "href": "app_supseded_functions.html#mark-data-points",
    "title": "Appendix D — Supseded functions",
    "section": "D.4 Mark data points",
    "text": "D.4 Mark data points\nSpecific data points can be highlighted using the marks argument. A list defines the measurement times to be marked, the marking color and the size of the marking. marks = list(position = c(1,5,6)) marks the first, fifth, and sixth measurement time. If the scdf contains more than one data-set marking would be the same for all data sets in this example. In case you define a list Containing vectors, marking can be individually defined for each data set. Assume, for example, we have an scdf comprising three data sets, then marks = list(position = list(c(1,2), c(3,4), c(5,6))) will highlight measurement times one and two for the first data set, three and four for the second and five and six for the third. pch, col and cex define symbol, colour and size of the markings.\n\n# plot with marks in a red circles 2.5 times larger than the standard symbol \n# size. exampleAB is an example scdf included in the scan package\nmarks &lt;- list(\n  positions = list( c(8, 9), c(17, 19), c(7, 18) ), \n  col = 'red', cex = 2.5, pch = 1\n)\nplot(exampleAB, marks = marks, style = \"sienna\")\n\n\n\n\nA plot with highlighted data-points",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>D</span>  <span class='chapter-title'>Supseded functions</span>"
    ]
  },
  {
    "objectID": "app_supseded_functions.html#graphical-styles-of-a-plot",
    "href": "app_supseded_functions.html#graphical-styles-of-a-plot",
    "title": "Appendix D — Supseded functions",
    "section": "D.5 Graphical styles of a plot",
    "text": "D.5 Graphical styles of a plot\n\n\n\n\n\n\nThe style_plot function call:\n\n\n\nstyle_plot(style = “default”, …)\n\n\nThe style argument of the plot function allows to specify a specific design of a plot. By default, the grid style is applied. scan includes some further predefined styles. default, yaxis, tiny, small, big, chart, ridge, annotate, grid, grid2, dark, nodot, and sienna. The name of a style is provided as a character string (e.g., style = \"grid\").\nSome styles only address specific elements (e.g., “small” or “tiny” just influence text and line sizes). These styles lend themselves to be combined with other styles. This could be achieved by providing several style names to the plot argument: style = c(\"grid\", \"annotate\", \"small\"). A style overwrites the settings of all previously included style.\nBeyond predefined styles, styles can be individually modified and created. New styles are provided as a list of several design parameters that are passed to the style argument of the plot function. Table D.3 shows all design parameter that could be defined.\nTo define a new style, first create a list containing a plain design. The style_plot function returns such a list with the default values for a plain design (e.g., mystyle &lt;- style_plot()). Single design parameters can now be set by assigning a specific value within the list. For example, newstyle$fill &lt;- \"grey90\" will set the fill parameter to \"grey90\". Alternatively, changes to the plain design can already by defined within the style_plot function. To set a light-blue background color and also an orange grid, create the style style_plot(fill.bg = \"lightblue\", grid = \"orange\"). If you do not want to start with the plain design but a different of the predefined styles, set the style argument. If, for example, you like to have the grid combined with the big style but want to change the color of the grid to orange type style_plot(style = c(\"grid\", \"big\"), col.grid = \"orange\"). plot(mydata, style = mystyle) will apply the new style in a plot. Please note that the new style is not passed in quotation marks.\n\n\n\n\nTable D.3: Arguments of the style plot function\n\n\n\n\n\n\nArgument\nWhat it does ...\n\n\n\n\nfill\nIf TRUE area under the line is filled.\n\n\ncol.fill\nSets the color of the area under the line.\n\n\ngrid\nIf TRUE a grid is included.\n\n\ncol.grid\nSets the color of the grid.\n\n\nlty.grid\nSets the line type of the grid.\n\n\nlwd.grid\nSets the line thikness of the grid.\n\n\nfill.bg\nIf not NA the backgorund of the plot is filled with the given color. If multiple colours are provided, the colours change with phases (e.g., `fill.bg = c('aliceblue', 'mistyrose1', 'honeydew')`\n\n\nannotations\nA list of parameters defining annotations to each data point. This adds the score of each MT to your plot. `'pos'` Position of the annotations: 1 = below, 2 = left, 3 = above, 4 = right. `'col'` Color of the annotations. `'cex'` Size of the annotations. `'round'` rounds the values to the specified decimal. `annotations = list(pos = 3, col = 'brown', round = 1)` adds scores rounded to one decimal above the data point in brown color to the plot.\n\n\ntext.ABlag\nBy default a vertical line separates phases A and B in the plot. Alternatively, you could print a character string between the two phases using this argument: `text.ABlag = 'Start'`.\n\n\nlwd\nWidth of the plot line. Default is `lwd = 2`.\n\n\npch\nPoint type. Default is `pch = 17` (triangles). Other options are for example: 16 (filled circles) or 'A' (uses the letter A).\n\n\ncol.lines\nThe color of the lines. If set to an empty string no lines are drawn.\n\n\ncol.dots\nThe color of the dots. If set to an empty string no dots are drawn.\n\n\nmai\nSets the margins of the plot.\n\n\n...\nFurther arguments passed to the plot command.\n\n\n\n\n\n\n\n\nThe width of the lines are set with the lwd argument, col is used to set the line colour and pch sets the symbol for a data point. The pch argument can take several values for defining the symbol in which data points are plotted.\n\n\n\n\n\nSome of the possible symbols and their pch values.\n\n\n\n\nHere is an example customizing a plot with several additional graphic parameters\n\nnewstyle &lt;- style_plot(\n  fill = \"grey95\",\n  fill.bg = c('aliceblue', 'mistyrose1', 'honeydew'),\n  names = list(col = \"brown\", cex = 2, font = 3, side = 3),\n  annotations = list(col = \"brown\"),\n  col.dots = \"blue\",\n  grid = \"lightblue\", \n  pch = 16)\n\nplot(exampleABAB, style = newstyle)\n\n\n\n\nA plot with a customized style.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>D</span>  <span class='chapter-title'>Supseded functions</span>"
    ]
  },
  {
    "objectID": "app_supseded_functions.html#sec-smoothing",
    "href": "app_supseded_functions.html#sec-smoothing",
    "title": "Appendix D — Supseded functions",
    "section": "D.6 Smoothing data",
    "text": "D.6 Smoothing data\n\n\n\n\n\n\nThe smooth_cases function call:\n\n\n\nsmooth_cases(data, dvar, mvar, method = “mean”, intensity = NULL, FUN = NULL)\n\n\nThe smooth_cases function provides procedures to smooth single-case data and eliminate noise. A moving average function (mean- or median-based) replaces each data point by the average of the surrounding data points step-by-step. A lag defines the number of measurements before and after the calculation is based on. So a lag-1 will take the average of the proceeding and following value and lag-2 the average of the two proceeding and two following measurements. With a local regression function, each data point is regressed by its surrounding data points. Here, the proportion of measurements surrounding a value is usually defined. So an intensity of 0.2 will take the surrounding 20% of data as the basis for a regression.\nThe function returns am scdf with smoothed data points.\n\n## Use the three different smoothing functions and compare the results\nberta_mmd &lt;- smooth_cases(Huber2014$Berta)\nberta_mmn &lt;- smooth_cases(Huber2014$Berta, FUN = \"movingMean\")\nberta_lre &lt;- smooth_cases(Huber2014$Berta, FUN = \"localRegression\")\nnew_study &lt;- c(Huber2014$Berta, berta_mmd, berta_mmn, berta_lre)\nnames(new_study) &lt;- c(\"Original\", \"Moving Median\", \"Moving Mean\", \"Local Regression\")\nplot(new_study, style = \"grid2\")\n\n\n\n\n\n\n\n\nHere is the syntax for the upcoming scplot() function (see Chapter 5):\n\nscplot(new_study) %&gt;% add_ridge()",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>D</span>  <span class='chapter-title'>Supseded functions</span>"
    ]
  },
  {
    "objectID": "app_author.html",
    "href": "app_author.html",
    "title": "Appendix E — About the author",
    "section": "",
    "text": "Currently, I am a Professor of Special Education at the University of Münster, Germany. Before that, I was a professor for research methods and diagnostics at the University of Potsdam in Germany. I studied education sciences at the University of Cologne where I also did my PhD in psychology. Thereafter, I got a tenured position as a senior researcher at the department of special education (also University of Cologne). Later I did my habilitation on “Pedagogic and psychology in learning disabilities” at the Carl von Ossietzky University Oldenburg.\nMy current work focuses on:\n\nSingle-case research designs, analyzing single case data, and reporting single-case based results.\nSocial inclusion and social participation in classrooms.\nImplementation of Open Science and Data Science concepts into special education research.\n\nYou can find more information about me on my homepage:\nhttps://jazznbass.github.io/homepage/",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>E</span>  <span class='chapter-title'>About the author</span>"
    ]
  }
]