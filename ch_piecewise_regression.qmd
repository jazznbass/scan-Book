# Piecewise linear regressions {#sec-plm}

```{r}
#| label: setup
#| include: false
library(knitr)
library(kableExtra)
```

In a piecewise-regression analysis (sometimes called segmented regression) a dataset is split at a particular break point and the regression parameters (intercept and slopes) are calculated separately for the data before and after the break point. This is done because we assume that there is a qualitative change at the break point that affects the intercept and slope. This approach is well suited to the analysis of single-case data which are from a statistical point of view time-series data segmented into phases. A general model for single-case data based on the piecewise regression approach has been proposed by Huitema and McKean @huitema_design_2000. They refer to two-phase single-case designs with a pre-intervention phase containing some measurements before the start of the intervention (A-phase) and an intervention phase containing measurements starting at the beginning of the intervention and continuing throughout intervention (B-phase).

In this model, four parameters predict the outcome at a specific measurement point [see @fig-plm]

1.  The performance at the beginning of the study (**intercept**),

2.  a developmental effect leading to a continuous increase throughout all measurements (**trend effect**),

3.  an intervention effect leading to an immediate and constant increase in performance (**level effect**), and

4.  a second intervention effect that evolves continuously with the beginning of the intervention (**slope effect**).

```{r echo = FALSE, fig.height=4, fig.width=6}
#| label: fig-plm
#| fig-cap: Illustration of the piecewise-linear model

ex <- scdf(c(A = 1,3,3,4,2,3,4,5,4, 4, B = 8,7,8,9,9,7,11,10,10,13))
scplot(ex) %>%
  set_theme("minimal") %>%
  add_statline("trend", color = "red") %>%
  add_statline("trendA", color = "steelblue3") %>%
  add_arrow(x0 = 10.5, y0 = 7, x1 = 10.5, y1 = 5, ends = "both", color = "blue3") %>%
  add_arrow(x0 = 1, y0 = 0, x1 = 1, y1 = 1.8, color = "blue3") %>%
  add_arrow(x0 = 14, y0 = 10, x1 = 15, y1 = 9.2, color = "blue3") %>%
  add_arrow(x0 = 7, y0 = 2.2, x1 = 8, y1 = 3.5, color = "blue3") %>%
  add_text("Intercept", x = 1.2, y = 0.5, size = 1.2, hjust = 0, color = "deeppink4") %>%
  add_text("Level effect", x = 10.7, y = 6, color = "deeppink4", size = 1.2, hjust = 0) %>%
  add_text("Slope effect\n(difference between red and blue line)", x = 16, y = 11.2, color = "deeppink4", size = 1.2, hjust = 1) %>%
  add_text("Trend effect", x = 7, y = 2, color = "deeppink4", size = 1.2) %>%
  set_casenames(" ") %>%
  set_yaxis(limits = c(0,14))

```

::: callout-note
## The regression model

The default model applied in the plm function has the formula

$$
y_i = \beta_0 + \beta_1 (time_i-time_1) + \beta_2 phase_i + \beta_3 (time_i-time_{n_A+1})*phase_i 
$$

Where:

$i$ := the session[^ch_piecewise_regression-1] number.

$\beta_0$ := the intercept.

$\beta_1 time$ := the continuous effect of the measurement time (the trend effect).

$\beta_2 phase$ := the discrete effect of the phase switch (the level effect).

$\beta_3 (time-n_{A+1})*phase$ := the continuous effect of the phase switch (the slope effect).

$n_{A+1}$ := the first session of phase B.

For $\beta_1$ and $\beta_3$, the time variables are shifted to start with a 0 at the first session of the respective phase.
:::

[^ch_piecewise_regression-1]: The session is the ordinal number (1st, 2nd, 3rd, ...) of the measurements. While the measurement-time (the value of the measurement-time variable at a specific session/ measurement) is often identical to the session number, this is not necessarily the case. The measurement-time sometimes represents days since the start of the study or other intervals. Therefore, I refer to *session* or *measurement* for the ordinal position of the measurement and *measurement-time* for the value of the time variable at that session number.

*scan* provides an implementation based on this piecewise-regression approach. Though the original model is extended by several factors:

-   multiple phase designs
-   additional (control) variables
-   autoregression modelling
-   logistic, binomial, and poisson distributed dependent variables and error terms
-   multivariate analyses for analysing the effect of an intervention on more than one outcome variable (see @sec-mplm).
-   multilevel analyses for multiple cases (see @sec-hplm).

## The basic plm function

```{r}
#| label: call-plm
#| results: asis
#| echo: false

function_structure("plm")
```

The basic function for applying a regression analyses to a single-case dataset is `plm`. This function analyses one single-case. In its simplest way, `plm` takes one argument with an *scdf* object and it returns a full piecewise-regression analyses.

```{r}
#| label: plm-ex-1
plm(exampleAB$Johanna)
```

The output shows the specific contrast settings for the phase and slope calculation (see below for a detailed explanation). Next, the overall model fit is provided. In this specific example the model fit is quite high explaining more than 80% of the variance of the dependent variable. The following regression table shows the unstandardised estimates (B), the lower and upper boundaries of a 95% confidence interval, the standard-error, the t-test statistic, the p-value, and the delta R squared for each predictor.

In this example, the intercept is the score estimation at the beginning of the study (here: at the first session, see @fig-plm-ex-1). The trend effect (variable *mt*) is 0.100. This means that for every one point increase in measurement-time, the score increased by 0.100. As this example has a total of 20 sessions, the overall increase due to the trend is $19 * 0.100 = 1.9$ points. The level effect (variable *phase* with level B) is 7.858. This means that all scores of phase B are increased by 7.858 points. The slope effect (for phase B) is 1.525. This means that for every one point increase in measurement time after the first phase B session, the score increases by 1.525. As there are 15 Phase B sessions in this example, the total increase due to the slope effect is $14 * 1.525 = 21.35$ points.

From these values each data point can be estimated. For example, the last session ($i=20$) is estimated to be $54.400 + 19 * 0.100 + 7.858 + 14 * 1.525 = 85.508$.

```{r}
#| label: fig-plm-ex-1
#| fig-cap: Example dataset
scplot(exampleAB$Johanna) |> add_statline("trend")
```

::: callout-note
As a convenience, the predictors are renamed according to the respective effect within the single-case terminology that they represent.

If you prefer to use the original variable names, set the following option: `options(scan.rename.predictors = "no")`.

If you want more concise renamed predictors, set: `options(scan.rename.predictors = "concise")`.

You can restore the default renaming by setting: `options(scan.rename.predictors = "full")`.
:::

## Autocorrelation of the residuals

The output also reports the **autocorrelations of the residuals**, which indicate whether the models residuals are serially independent.

There are many reasons why this might happen. For example, in a learning study, performance may temporarily decline after a learning session (performance dip). Similarly, in a medication study, a drug might initially cause a brief worsening of symptoms before exerting its intended therapeutic effect (early symptom exacerbation effect).

The *lag* refers to the number of measurement points between observations that show autocorrelation.

The **Ljung-Box** test is an omnibus test that evaluates whether the residuals exhibit significant autocorrelation at multiple lags. It assesses whether the joint distribution of autocorrelations differs significantly from what would be expected under the assumption of independence.

High and significant autocorrelations pose a threat to the validity of standard regression models, as they violate the assumption of independent residuals. To account for this, you can set the `AR` argument with the appropriate maximum lag (e.g., `AR = 3`). This implements an *ARMA* (autoregressive moving average) model, which takes the serial correlation in the residuals into account.

## Standardizing predictors {#sec-rescale}

If you want standardized predictors, meaning that predictors are scaled to standard deviations, the best approach is to standardize all variables before computing the regression model. Use the `rescale()` function to do this.

```{r}
#| label: plm-ex-standardize

exampleAB$Johanna |> 
  rescale() |> 
  plm()
```

The regression coefficients *B* are now standardized Beta (ß) coefficients, indicating the expected change (in standard deviations) of the dependent variable for a one-standard-deviation increase in the predictor.

## Adjusting the model

The plm model is a complex model specifically suited for single-case studies. It entails a series of important parameters. Nevertheless, often we have specific theoretical assumption that do no include some of these parameters. We might, for example, only expect an immediate but not a continuous change from a medical intervention. Therefore, it would not be useful to include the slope-effect into our modelling. Vice versa, we could investigate an intervention that will just develop across time without an immediate change with the intervention start. Here we should drop the level-effect from out model. Even the assumption of a trend-effect can be dropped in cases where we do not expect a serial dependency of the data and we do not assume intervention independent changes within the time-frame of the study.

It is important to keep in mind, that an overly complex model might have negative effects on the test power of an analyses (that is, the probability of detecting an actually existing effect is diminished) [see @wilbert2022]

### The `slope`, `level`, and `trend` arguments

<!--# treat each argument separately and add more examples -->

The plm function comes with three arguments (`slope`, `level`, and `trend`) to include or drop the respective predictors from the plm model. Buy default, all arguments are set `TRUE` and a full plm model is applied to the data.

Consider the following data example:

```{r}
#| label: plm-ex-slope-level-1
example <- scdf(
   values = c(A = 55, 58, 53, 50, 52, 
              B = 55, 68, 68, 81, 67, 78, 73, 72, 78, 81, 78, 71, 85, 80, 76)
)

plm(example)
```

The piecewise regression reveals a significant level effect and two non significant effects for trend and slope. In a further analyses we would like to put the slope effect out of the equation. The easiest way to do this is to set the `slope` argument to `FALSE`.

```{r}
#| label: plm-ex-slope-level-2
plm(example, slope = FALSE)
```

In the resulting estimations the trend and level effects are now significant. The model estimated a trend effect of 1.01 points for every one point increase in measurement-time and a level effect of 10.33 points. That is, with the beginning of the intervention (the B-phase) the score increases by $5 x 1.01 + 10.33 = 15.38$ points (the measurement-time is increases by five, from one to six, at the first session of phase B).

## Adding additional predictors

In more complex analyses, additional predictors can be included in the piecewise regression model.

To do this, we have to change the regression formula 'manually' by applying the `update` argument. The `update` argument allows to change the underlying regression formula. To add a new variable named for example `newVar`, set `update = .~. + newVar`. The `.~.` part takes the internally build model formula (based on the number of phases in your model and the setting of the `slope`, `level`, and `trend` arguments) and `+ newVar` adds a variable called `newVar` to the equation.

Here is an example adding the control variable `cigarrets` to the model:

```{r}
#| include: false
options(scan.rename.predictors = "concise")
```

```{r}
#| label: plm-ex-update

plm(exampleAB_add, update = .~. + cigarrets)
```

```{r}
#| include: false
options(scan.rename.predictors = "full")
```

The output of the plm-function shows the resulting formula for the regression model that includes the cigarettes variable:\
`Formula: wellbeing ~ day + phaseMedication + interMedication + cigarrets`

```{=html}
<!--#  

completely changing the regression formula:

The formula has two parts divided by a tilde. Left of the tilde is the variable to be predicted and right of it the predictors. A 1 indicates the intercept, the variable mt estimates the trend effect, phaseB the level effect of the B-phase and the variable interB the slope effect of the B-phase (the interaction of measurement-time and phase). If formula is not explicitly defined, it is set to formula = values ~ 1 + mt + phaseB + interB (assuming an AB-design) to estimate the full piecewise regression model.

-->
```

## Frequencies and proportions

```{r}
#| include: false
options(scan.rename.predictors = "concise")
```

When the dependent variable is a frequency, the assumption of a normal distribution is not appropriate. This section will demonstrate how to handle such cases using the `plm` function.

We can distinguish between a frequency without a clearly defined limit, such as the number of lightning strikes per year in a given area or the number of seizures a patient experiences in a week, and a frequency that represents a proportion, such as the number of correctly solved tasks in a test. In these cases, the dependent variable is discrete (only cardinal numbers are possible) and cannot be lower than zero. Both of these properties violate the assumptions of a Gaussian (normal) distribution.

### Bounded frequencies

Frequencies that have a clear reference, such as “10 out of 20,” follow a binomial distribution. In these cases, set the `family` argument of the `plm` function to `family = "binomial"`.

Additionally, you must specify the maximum value (the “out of” part). You can either use a distinct variable in your *scdf* that defines the maximum separately for each measurement point or specify a constant value for all measurements. In both cases, you need to set the `var_trials` argument either to the name of the variable containing the maximum values or to a numeric value representing a fixed maximum.

```{r}
#| include: false

model <- plm(exampleAB_score$Christiano, family = "binomial", var_trials = "trials")
```

```{r}
#| label: plm-ex-plm-binomal-1

plm(exampleAB_score$Christiano , family = "binomial", var_trials = "trials")

# Or equivalent as all values in trial are 20:
# plm(exampleAB_score$Christiano , family = "binomial", var_trials = 20)
```

The output shows the estimators *B*, which are on a logit scale, and odds or odds ratios (*OR*), which are the exponentiated logits, along with their respective confidence intervals. Logits are linear, meaning that the *B* coefficients can be added to obtain the combined effect of multiple predictors. For the example above, the logit of the intercept is -1.964, corresponding to a probability of approximately 12% (for details on the calculation, see note below). The level effect is $B = 2.376$. This means that at the start of Phase B, the logit increases to $B = -1.964 + 2.376 = 0.412$, which corresponds to a probability of approximately 60%.

Odds ratios are not linear and must be multiplied to obtain a combined effect. In this example, the odds of the intercept is 0.14, calculated as $e^{-1.964}=0.14$. For the level effect, the *odds ratio* ("*ratio*" because it indicates the change of the odds) is 10.762. The combined effect (intecept and level) gives the odds 0.14 \* 10.762 = 1.50668. Converting back to the logit scale, this is $\log(1.50668)\approx 0.41$. This result, aside from minor rounding differences, matches the value obtained by summing the respective *B* coefficients.

Keep in mind that, as in Gaussian regression models, the p-value for the intercept tests the null hypothesis that the true intercept is zero. In other words, it represents the probability of obtaining an intercept estimate as extreme as the one observed if the true intercept were indeed zero (i.e., the probability of a Type I error under this null hypothesis). However, in logistic regression, an intercept (or logit) of zero does not correspond to a probability of 0%; it corresponds to a probability of 50%, because $e^{0}= Odds{1\over1}=50\%$.

::: callout-note
#### What are Logits and Odds Ratios? {#sec-note-logits-odds}

Logits are the logarithm of the odds: $log(Odds)$. Odds represent the probability of an event occurring relative to it not occurring. If the odds are 4 to 1 ($4\over1$), this means that out of 5 total events, 4 result in one category of outcome, and 1 results in the opposite outcome.

For example, if a horse runs five races and we expect it to win four and lose one, the odds of winning are 4 to 1 ($4\over1$).

If the odds are less than 1, the probability of the event occurring is below 50%. For instance, if the odds are 0.25 ($1\over4$), we expect the horse to win one out of five races.

*Difference Between Probabilities and Odds*

Probabilities and odds are related but not identical. A probability of $1\over4$ (0.25) means that the specific outcome occurs in 1 out of 4 events. Odds of $1\over4$ means that the probability of two possible outcomes is 1 against 4, meaning the event occurs 1 time for every 4 non-occurrences. In this case, odds of $1\over4$ correspond to a probability of $1\over5$ = 0.20.

*Formula*

The odds are calculated as:

$$Odds(y_i)=\frac{P(y_i=1)}{1-P(y_i=0)}$$

For example, if we have a probability of 25%, the odds are:

$$Odds=\frac{0.25}{0.75}={1\over3}$$

The formula for calculating logits is:

$$logit(y_i)=log(\frac{P(y_i=1)}{1-P(y_i=0)})=log(odds)$$

For example, if we have a 25% probability that is:

$$logit(P=0.25)=log(\frac{0.25}{0.75})=log({1\over3})=-1.098612$$

For calculating probabilities from logits we use:

$$P = \frac{e^{\text{logit}(B)}}{1 + e^{\text{logit}(B)}}$$

For example, a logit of *B* = -1.964 is approxomately a 12% probability:

$$P = \frac{e^{-1.964}}{1 + e^{-1.964}}\approx\frac{0.14}{1.14}\approx{0.12}$$
:::

### Proportions

Proportions are actually just a mathematical identical presentation of bounded frequencies (e.g. 10 out of 20 is identical to 0.5 from 20). When your dependent variable represents proportions, set `family` to a binomial distribution `family = "binomial"`. Again, you have to provide a variable or a constant with the total number of trials with the `var_trials` argument. Finally, you have to set the `dvar_percentage` argument to `TRUE` to indicate that your dependent variable contains proportions.

Here is the previous example where the the frequencies are transformed to proportions. The results are identical to the calculation with frequencies.

```{r}
#| label: plm-ex-plm-binomal-2
scdf <- exampleAB_score$Christiano |>
  transform(proportions = values/trials) |>
  set_dvar("proportions")

plm(scdf, family = "binomial", var_trials = "trials", dvar_percentage = TRUE)

```

::: callout-tip
#### Percentages

When you have percentages instead of proportions as the dependent variable, divide these by 100 with the `transform` function to get proportions.\
\
`scdf <- transform(scdf, values = values / 100)`
:::

### Dichotomous outcomes

When the dependent variable is dichotomous (e.g., 0 or 1; yes or no), we can estimate a logistic regression. Logistic regression is essentially a special case of binomial regression, applied under the "extreme" condition that there is only one trial per measurement—so that each observation results in either 0% (failure) or 100% (success).

Here is an example dataset with a respective analysis:

```{r}
#| echo = FALSE
set.seed(123)
des <- design(
  phase_design = list(A = 50, B = 100), 
  start_value = 0.30, 
  level = list(0, 0.2), 
  distribution = "binomial",
  n_trials = 1)

example_dichotomous <- random_scdf(des)

scplot(example_dichotomous) |> set_theme("minimal") |> set_casenames("")

```

```{r}
#| label: ex-plm-dichotomous
plm(example_dichotomous, family = "binomial", var_trials = 1)
```

### Bounded frequencies

In many cases, frequency data are collected without a predetermined upper limit. For example, in a study measuring the frequency of inappropriate classroom behavior, researchers may observe lessons and count the number of disturbances. Although there may be a practical upper limit on the number of possible disturbances, the theoretical limit is infinite.

Another example is a study counting the number of lorries driving on a specific street. Here too, the highest possible number is undefined.

For this type of data, a Poisson distribution is more appropriate. To specify a Poisson model, set the `family` argument in the `plm()` function to `"poisson"`.

The following example shows a dataset of injured people due to an accident on an autobahn before and after implementing a speed limit:

```{r}
#| echo: false
#| label: plm-ex-poisson-data
example_A24 |> export()
```

Here, a Possion regression is applied:

```{r}
#| label: plm-ex-poisson
plm(example_A24, family = "poisson")
```

```{r}
#| include: false
options(scan.rename.predictors = "full")
```

## Model comparison (since version 0.64.0) {#sec-anova}

::: callout-caution
## Experimental

This feature is still in an experimental state. This means that the syntax structure, arguments, and defaults may change in future scan versions and may not be backward compatible.
:::

```{r}
#| label: call-anova-plm
#| results: asis
#| echo: false
function_structure(scan:::anova.sc_plm, label="anova", fname="anova")
```

You can compare the fit of piecewise linear regression (plm) models using the `anova()` function. This allows for stepwise regression analyses and targeted comparisons (e.g., assessing the combined effect of slope and level changes).

Let’s consider an example where we begin with a null model that includes only an intercept but no additional predictors. We then sequentially add a level effect, followed by a trend effect, and finally a slope effect.

```{r}
#| label: plm-ex-anova

mod0 <- plm(exampleAB$Anja, trend = FALSE, level = FALSE, slope = FALSE)
mod1 <- plm(exampleAB$Anja, trend = FALSE, level = TRUE, slope = FALSE)
mod2 <- plm(exampleAB$Anja, trend = TRUE, level = TRUE, slope = FALSE)
mod3 <- plm(exampleAB$Anja, trend = TRUE, level = TRUE, slope = TRUE)
```

We start by comparing the null model to the first model:

```{r}
anova(mod0, mod1)
```

The output provides the *residuals degrees of freedom* (Df), the *deviance of the residuals*, and $\Delta Df$ and $\Delta \text{Deviance}$, which are the difference of the degrees of freedom and the residual deviance of the respective model and the preceding model (here: $\Delta \text{Df}=19 - 18 = 1$ and $\Delta \text{Deviance}=2410.95 - 840.13 = 1570.8$). Deviance is a measure of a model’s goodness of fit and is calculated as $-2 \times \text{Likelihood}$ . The test compares the difference in degrees of freedom ($\Delta Df = 1$) and deviance ($\Delta \text{Deviance} = 1570.8$) between the two models.

Since both models assume Gaussian-distributed data, an F-test is performed. The F-value is computed as:

$$
F = \frac{\text{Explained variance}}{\text{Residual variance}} = \frac{\Delta \text{Deviance}}{\text{Residual deviance} / Df} = \frac{1570.8}{\frac{840.13}{18}} = \frac{1570.8}{46.67389} = 33.655
$$

For $F(1, 18) = 33.655$, the p-value can be obtained with:

```{r}
pf(33.655,1,18, lower.tail = FALSE)
```

Alternatively, we can conduct a *likelihood-ratio test* by setting the argument `test = "LRT"`:

```{r}
anova(mod0, mod1, test = "LRT")
```

This computes a $\chi^2$ test for $\frac{\Delta \text{Deviance}}{\text{Dispersion}} = 33.655$ with $\Delta Df = 1$, where the p-value is given by:

```{r}
pchisq(33.655, 1, lower.tail = FALSE)
```

We can also compare all four models as incrementally nested models in a single step:

```{r}
anova(mod0, mod1, mod2, mod3)
```

Each model is compared to its predecessor. Since all models are nested and derived from the same data, the dispersion parameter for all comparisons is taken from the most complex model, here *mod3* (dispersion = 30.11).

In this analysis, we observe that the phase predictor introduces the strongest improvement in model fit, whereas the slope predictor does not significantly enhance the model.

## Dummy models {#sec-dummy-model}

The `model` argument is used to code the *dummy variables*. These *dummy variables* are used to compute the slope and level effects of the *phase* variable.\
The *phase* variable is categorical, identifying the phase of each measurement. Typically, categorical variables are implemented by means of dummy variables. In a piecewise regression model two phase effects have to be estimated: a level effect and a slope effect. The level effect is implemented quite straight forward: for each phase beginning with the second phase a new dummy variable is created with values of zero for all measurements except the measurements of the phase in focus where values of one are set.

```{r}
#| label: plm-ex-2
#| echo: false
res <- data.frame(
  values = c(3,6,4,7, 5,3,4,6,3), 
  phase = c(rep("A",4),rep("B",5)), 
  mt = 1:9,
  "level B" = c(0,0,0,0,1,1,1,1,1),
  check.names = FALSE
)
kable(res, align = "c") %>%
  kable_classic(full_width = FALSE)
```

For estimating the *slope effect* of each phase, another kind of dummy variables have to be created. Like the dummy variables for level effects the values are set to zero for all measurements except the ones of the phase in focus. Here, values start to increase with every measurement until the end of the phase.\
Various suggestions have been made regarding the way in which these values increase [see @huitema_design_2000]. The *B&L-B* model starts with a one at the first session of the phase and increases with every session while the *H-M* model starts with a zero.

```{r}
#| echo: false
#| label: dummy-slope

res <- data.frame(
  values = c(3,6,4,7, 5,3,4,6,3), 
  phase = c(rep("A",4),rep("B",5)), 
  mt = 1:9,
  "level B" = c(0,0,0,0,1,1,1,1,1), 
  "model B&L-M" = c(0,0,0,0,1,2,3,4,5), 
  "model H-M" = c(0,0,0,0,0,1,2,3,4), 
  check.names = FALSE )
kable(res, align = "c") %>%
  kable_classic(full_width = FALSE) %>%
  add_header_above(c(" " = 4, "slope B" = 2))
```

Applying the *H-M* model will give you a "pure" level-effect while the *B&L-B* model will provide an estimation of the level-effect that is actually the level-effect plus on times the slope-effect (as the model assumes that the slope variable is *1* at the first measurement of the B-phase). For most studies, the *H-M* model is more appropriate.

However, there is another aspect to be aware of. Usually, in single case designs, the measurement times are coded as starting with *1* and increasing in integers (1, 2, 3, ...). At the same time, the estimation of the trend effect is based on the measurement time variable. In this case, the estimate of the model intercept (usually interpreted as the value at the beginning of the study) actually represents the estimate of the starting value plus one times the trend effect. Therefore, I have implemented the *W* model (since scan version `0.54.4`). Here the trend effect is estimated for a time variable shifted to start at *0*. As a result, the intercept represents the estimated value at the first measurement of the study. The *W* model treats slope estimation in the same way as the *H-M* model. That is, the measurement-time for calculating the slope effect is set to *0* for the first session of each phase. Since scan version `0.54.4` the *W* model is the default.

```{r}
#| echo: false
#| label: dummy-slope-mt

res <- data.frame(
  values = c(3,6,4,7, 5,3,4,6,3), 
  phase = c(rep("A",4),rep("B",5)), 
  level = c(0,0,0,0,1,1,1,1,1),
  "B&L-M and H-M" = 1:9,
  "W" = 0:8,
  "B&L-M" = c(0,0,0,0,1,2,3,4,5), 
  "H-M and W" = c(0,0,0,0,0,1,2,3,4), 
  check.names = FALSE )
kable(res, align = "c") %>%
  kable_classic(full_width = FALSE) %>%
  add_header_above(c(" " = 3, "mt" = 2, "slope" = 2))
```

## Designs with more than two phases: Setting the right contrasts

For single case studies with more than two phases, things get a bit more complicated. Applying the models described above to three phases would result in a comparison between each phase and the first phase (usually phase A). That is, the regression weights and significance tests indicate the differences between each phase and the values from phase A. Another common use is to compare the effects of a phase with the previous phase.

As of scan version `0.54.4`, plm allows to set a contrast argument. `contrast = "first"` (the default) will compare all slope and level-effects to the values in the first phase. `contrast = "preceding"` will compare the slope and level-effects to the preceding phase.

For the *preceding contrast*, the dummy variable for the level-effect is set to zero for all phases preceding the phase in focus and set to one for all remaining measurements. Similarly, the dummy variable for the slope-effect is set to zero for all phases preceding the phase in focus and starts at zero (or one, depending on the model setting, see @sec-dummy-model) for the first measurement of the target phase and increases until the last measurement of the case.

You can set the contrast differently for the level and slope effects with the arguments `constrast_level` and `contrast_slope`. Both can be either `"first"` or `"preceding"`.

(Note: Prior to scan version `0.54.4`, the option `model = "JW"` was identical to `model = "B&L-B", contrast = "preceding"`).

```{r}
#| echo: false
#| label: dummy-contrast

res <- data.frame(
  values =  c(3,6,4,7, 5,3,4,6,3, 7,5,6,4,8), 
  phase = c(rep("A", 4), rep("B", 5), rep("C", 5)), 
  mt = 1:14,
  B = c(0,0,0,0, 1,1,1,1,1, 0,0,0,0,0), 
  C = c(0,0,0,0, 0,0,0,0,0, 1,1,1,1,1), 
  B = c(0,0,0,0, 1,2,3,4,5, 0,0,0,0,0), 
  C = c(0,0,0,0, 0,0,0,0,0, 1,2,3,4,5),
  
  B = c(0,0,0,0, 1,1,1,1,1, 1,1,1,1,1), 
  C = c(0,0,0,0, 0,0,0,0,0, 1,1,1,1,1), 
  B = c(0,0,0,0, 1,2,3,4,5, 6,7,8,9,10), 
  C = c(0,0,0,0, 0,0,0,0,0, 1,2,3,4,5),
  check.names = FALSE)
kable(res, align = "c") |> 
  kable_classic(full_width = FALSE) |> 
  add_header_above(c(" "=3, "level"=2, "slope"=2, "level"=2, "slope"=2))  |> 
  add_header_above(c(" "=3, "contrast\nfirst"=4, "contrast\npreceeding"=4))
```

## Understanding and interpreting contrasts

In this section, we will calculate four plm models with different contrast settings for the same single-case data.

The example scdf is the case 'Marie' from the *exampleABC* scdf (`exampleABC$Marie`)

```{r}
#| echo: false
#| label: fig-marie
#| fig-cap: Example dataset

scplot(exampleABC$Marie) %>% 
  add_statline("trend", color = "darkred")
  
```

The dark-red lines indicate the intercept and slopes when calculated separately for each phase. They are:

```{r}
#| echo: false
#| label: tbl-contrasts-ex-2
#| tbl-cap: Intercept, slope, and number of measurements calculated separately for each phase
res <- trend(exampleABC$Marie)$trend[2:4, 1:2] %>% round(3)
res$n <- 10
row.names(res) <- paste0("phase ", c("A","B", "C"))
colnames(res) <- c("intercept", "slope", "n")
kable(res, align = "c") %>%
  kable_classic(full_width = TRUE)
```

Now we estimate a plm model with four contrast settings (see @tbl-contrast-first-preceding):

```{r}
#| echo: false
#| label: tbl-contrast-first-preceding
#| tbl-cap: Estimates of a piecewise-linear regression with contrast models "first" and "preceding".
df <- rbind(
  coef(plm(exampleABC$Marie, contrast_level = "first", contrast_slope = "first"))[,"Estimate"],
  coef(plm(exampleABC$Marie, contrast_level = "preceding", contrast_slope = "preceding"))[,"Estimate"],
  coef(plm(exampleABC$Marie, contrast_level = "first", contrast_slope = "preceding"))[,"Estimate"],
  coef(plm(exampleABC$Marie, contrast_level = "preceding", contrast_slope = "first"))[,"Estimate"]
) %>% 
  round(3) %>% as.data.frame()
names(df) <- c("intercept", "trend", "level B", "level C", "slope B", "slope C")
data.frame(
  "Contrast level" = c("first", "preceding", "first", "preceding"), 
  "Contrast slope" = c("first", "preceding", "preceding", "first"),
  df, 
  check.names = FALSE) %>%
  kable(align = "c") %>%
  kable_classic(full_width = FALSE)
```

### Phase B estimates

All regression models in @tbl-contrast-first-preceding have the same estimates for `intercept` and `trend`. These are not affected by the contrasts and are identical to those for phase A in @tbl-contrasts-ex-2. In addition, in @tbl-contrast-first-preceding, the estimates for `levelB` and `slopeC` are identical since all models contrast the same phase (the first and the preceding phase are both phase A). The values here can be calculated from @tbl-contrasts-ex-2[^ch_piecewise_regression-2]:

[^ch_piecewise_regression-2]: Differences here and in the following calculations are due to rounding errors.

$$
levelB = intercept_{phaseB} - (intercept_{phaseA} + n_{PhaseA} * slope_{phaseA})
$$ {#eq-contrast-level-2}

$$
33.388 \approx  74.855 - (60.618 + 10*-1.915)
$$

$$
slopeB = slope_{phaseB} - slope_{phaseA}
$$ {#eq-contrast-slope-2}

$$
1.303 \approx -1.915 - (-0.612) 
$$

### Phase C estimates

The `levelC` and `slopeC` estimates of the regression models in @tbl-contrast-first-preceding are different for the various contrast models. Depending on the contrast setting, the estimates "answer" a different question. @tbl-interpretation-contrasts provides interpretation help.

| Contrast level | Contrast slope | Interpretation of level C effect | Interpretation slope C effect |
|------------------|------------------|------------------|------------------|
| first | first | What would be the value if phase A had continued until to the start of phase C and what is the difference to the actual value at the first measurement of phase C? | What is the difference between the slopes of phase C and A[^ch_piecewise_regression-3]? |
| preceding | preceding | What would be the value if phase B had continued to the start of phase C and what is the difference to the actual value at the first measurement of phase C? | What is the difference between the slopes of phase C and B? |
| first | preceding | What would be the value if phase A had continued until the start of phase C (assuming a slope effect but no level effect in phase B)? And what is the difference to the actual value at the first measurement of phase C? | What is the difference between the slopes of phase C and B? |
| preceding | first | What would be the value if phase B had continued until the start of phase C (assuming a level but no slope effect in phase B)? And what is the difference to the actual value at the first measurement of phase C? | What is the difference between the slopes of phase C and A? |

: Interpretations of the effect estimates in various contrast conditions {#tbl-interpretation-contrasts}

[^ch_piecewise_regression-3]: The slope of phase A is the trend effect.

All four models are mathematically equivalent, i.e. they produce the same estimates of the dependent variable. Bellow I will show how the estimates from the piecewise regression models relate to the simple regression estimates from @tbl-contrasts-ex-2. These are $intercept_{phaseC} = 68.873$ and $slope_{phaseC} = -0.194$.

***Level first and slope first contrasts***

@tbl-contrast-first-preceding estimates a `levelC` increase of 46.558 compared to phase A (the intercept) and a `slopeC` increase of 1.721.

$$
levelC = intercept_{phaseC} - (Intercept_{phaseA} + n_{phaseA+B} * slope_{phaseA})
$$ {#eq-contrast-ff-1}

$$46.558 \approx 68.873 - (60.618 + 20*-1.915) $$

$$
slopeC = slope_{phaseC} - slope_{phaseA} 
$$ {#eq-contrast-ff-2}

$$1.721 \approx -0.194 - (-1.915)$$

***Level preceding and slope preceding contrasts***

@tbl-contrast-first-preceding estimates a `levelC` increase of 0.139 compared to phase B and a `slopeC` increase of 0.418.

$$
levelC = intercept_{phaseC} - (intercept_{phaseB} + n_{phaseB} * slope_{phaseB})
$$ {#eq-contrast-pp-1}

$$0.139 \approx 68.873 - (74.855 + 10*-0.612)$$

$$
slopeC = slope_{phaseC} - slope_{phaseB}
$$ {#eq-contrast-pp-2}

$$0.418 \approx -0.194 - (-0.612)$$

***Level first and slope preceding contrasts***

@tbl-contrast-first-preceding estimates a `levelC` increase of 33.388 compared to phase A and a `slopeC` increase of 0.418.

$$
levelC = intercept_{phaseC} - (intercept_{phaseA}  + n_{phaseA} * slope_{phaseA} + n_{phaseB} * slope_{phaseB})
$$ {#eq-contrast-fp-1}

$$
33.527 \approx 68.873 - (60.618 + 10 * -1.915 + 10 * -0.612)
$$

$$
slopeC = slope_{phaseC} - slope_{phaseB}
$$ {#eq-contrast-fp-2}

$$0.418 \approx -0.194 - (-0.612)$$

***Level preceding and slope first contrasts***

@tbl-contrast-first-preceding estimates a `levelC` increase of 13.170 compared to phase B and a `slopeC` increase of 1.721.

$$
levelC = intercept_{phaseC} - (intercept_{phaseB} + n_{phaseB} * slope_{phaseA})
$$ {#eq-contrast-pf-1}

$$
13.170\approx 68.873 - (74.855 + 10*-1.915)
$$

$$
slopeC = slope_{phaseC} - slope_{phaseA}
$$ {#eq-contrast-pf-2}

$$
1.721 \approx -0.194 - (-1.915)
$$

<!--# same analyses for models without level estimators and models without slope estimators -->
