# Piecewise linear regressions {#sec-plm}

```{r}
#| label: setup
#| include: false
library(knitr)
library(kableExtra)
```

In a piecewise-regression analysis (sometimes called segmented regression) a dataset is split at a specific break point and regression parameters (intercept and slopes) are calculated separately for data before and after the break point. This is done because we assume that at the break point a qualitative change happens affecting intercept and slope. This approach lends itself perfectly to analyze single-case data which are from a statistical point of view time-series data segmented into phases. A general model for single-case data based on the piecewise regression approach has been suggested by Huitema and McKean @huitema_design_2000. They refer to two-phase single-case designs with a pre-intervention phase containing some measurements before the start of the intervention (A-phase) and an intervention phase containing measurements beginning at the intervention's start and lasting throughout the intervention (B-phase).

In this model, four parameters predict the outcome at a specific measurement point:

1.  The performance at the beginning of the study (**intercept**),

2.  a developmental effect leading to a continuous increase throughout all measurements (**trend effect**),

3.  an intervention effect leading to an immediate and constant increase in performance (**level effect**), and

4.  a second intervention effect that evolves continuously with the beginning of the intervention (**slope effect**).

```{r figure-plm, echo = FALSE, fig.height=4, fig.width=6}
ex <- scdf(c(A = 1,3,3,4,2,3,4,5,4, 4, B = 8,7,8,9,9,7,11,10,10,13))
scplot(ex) %>%
  add_theme("minimal") %>%
  add_statline("trend", color = "red") %>%
  add_statline("trendA", color = "red") %>%
  add_arrow(x0 = 10.5, y0 = 7, x1 = 10.5, y1 = 5, ends = "both", color = "blue3") %>%
  add_arrow(x0 = 1, y0 = 0, x1 = 1, y1 = 1.8, color = "blue3") %>%
  add_arrow(x0 = 14, y0 = 10, x1 = 15, y1 = 9.2, color = "blue3") %>%
  add_arrow(x0 = 7, y0 = 2.2, x1 = 8, y1 = 3.5, color = "blue3") %>%
  add_text("Intercept", x = 1.2, y = 0.5, size = 1.2, hjust = 0, color = "deeppink4") %>%
  add_text("Level effect", x = 10.7, y = 6, color = "deeppink4", size = 1.2, hjust = 0) %>%
  add_text("Slope effect", x = 14, y = 10.2, color = "deeppink4", size = 1.2) %>%
  add_text("Trend effect", x = 7, y = 2, color = "deeppink4", size = 1.2) %>%
  set_casenames(" ") %>%
  set_yaxis(limits = c(0,14))

```

*scan* provides an implementation based on this piecewise-regression approach. Though the original model is extended by several factors:

-   multiple phase designs
-   additional (control) variables
-   autoregression modeling
-   logistic, binomial, and poisson distributed dependent variables and error terms
-   multivariate analyzes for analyzing the effect of an intervention on more than one outcome variable (see @sec-mplm).
-   multilevel analyzes for multiple cases (see @sec-hplm).

## The basic plm function

```{r results='asis', echo=FALSE}
function_structure("plm")
```

The basic function for applying a regression analyzes to a single-case dataset is `plm`. This function analyzes one single-case. In its simplest way, `plm` takes one argument with an *scdf* object and it returns a full piecewise-regression analyzes.

```{r}
#| label: plm-ex-1
plm(exampleAB$Johanna)
```

<!--# describe the output -->

## Adjusting the model

The plm model is a complex model specifically suited for single-case studies. It entails a series of important parameters. Nevertheless, oftentimes we have specific theoretical assumption that do no include some of these parameters. We might, for example, only expect an immediate but not a continuous change from a medical intervention. Therefore, it would not be useful to include the slope-effect into our modelling. Vice versa, we could investigate an intervention that will just develop across time without an immediate change with the intervention start. Here we should drop the level-effect from out model. Even the assumption of a trend-effect can be dropped in cases where we do not expect a serial dependency of the data and we do not assume intervention independent changes within the time-frame of the study.

It is important to keep in mind, that an overly complex model might have negative effects on the test power of an analyzes (that is, the probability of detecting an actually existing effect is diminished) [see @wilbert2022]

### The `slope`, `level`, and `trend` arguments

The plm function comes with three arguments (`slope`, `level`, and `trend`) to include or drop the respective predictors from the plm model. Buy default, all arguments are set `TRUE` and a full plm model is applied to the data.

Consider the following data example:

```{r}
example <- scdf(
   values = c(55, 58, 53, 50, 52, 55, 68, 68, 81, 67, 78, 73, 72, 78, 81, 78, 71, 85, 80, 76),
   phase_design = c(A = 5, B = 15)
)

plm(example)
```

The piecewise regression reveals a significant level effect and two non significant effects for trend and slope. In a further analyses we would like to put the slope effect out of the equation. he easiest way to do this is by setting the `slope` argument to `FALSE`.

```{r}
plm(example, slope = FALSE)
```

In the resulting estimations the trend and level effects are now significant. The model estimated a trend effect of 1.01 points per measurement time and a level effect of 10.33 points. That is, with the beginning of the intervention (the B-phase) the score increases by 15.38 points (5 x 1.01 + 10.33).

## Adding additional predictors

In more complex analyses additional predictors can be included in the piecewise regression model.

To do this, we have to change the regression formula 'manually' by applying the `update` argument. The `update` argument allows to change the underlying regression formula. To add a new variable named for example `newVar`, set `update = .~. + newVar`. The `.~.` part takes the internally build formula and `+ newVar` adds a variable named `newVar` to the equation.

Here is an example adding the control variable `cigarrets` to the model:

```{r}
plm(exampleAB_add, update = .~. + cigarrets)
```

The formula has two parts divided by a tilde. Left of the tilde is the variable to be predicted and right of it the predictors. A `1` indicates the intercept, the variable `mt` estimates the trend effect, `phaseB` the level effect of the B-phase and the variable `interB` the slope effect of the B-phase (the *inter*action of measurement-time and phase). If `formula` is not explicitly defined, it is set to `formula = values ~ 1 + mt + phaseB + interB` (assuming an AB-design) to estimate the full piecewise regression model.

## Dummy models

The `model` argument is used to code the *dummy variables*. These *dummy variables* are used to compute the slope and level effects of the *phase* variable.\
The *phase* variable is categorical, identifying the phase of each measurement. Typically, categorical variables are implemented by means of dummy variables. In a piecewise regression model two phase effects have to be estimated: a level effect and a slope effect. The level effect is implemented quite straight forward: for each phase beginning with the second phase a new dummy variable is created with values of zero for all measurements except the measurements of the phase in focus where values of one are set.

```{r}
#| label: plm-ex-2
#| echo: false
res <- data.frame(
  values = c(3,6,4,7, 5,3,4,6,3), 
  phase = c(rep("A",4),rep("B",5)), 
  mt = 1:9,
  "level B" = c(0,0,0,0,1,1,1,1,1),
  check.names = FALSE
)
kable(res, align = "c") %>%
  kable_classic(full_width = FALSE)
```

For estimating the *slope effect* of each phase, another kind of dummy variables have to be created. Like the dummy variables for level effects the values are set to zero for all measurements except the ones of the phase in focus. Here, values start to increase with every measurement until the end of the phase.\
Various suggestions have been made regarding the way in which these values increase [see @huitema_design_2000]. The *B&L-B* model starts with a one at the first measurement of the phase and increases with every measurement while the *H-M* model starts with a zero.

```{r}
#| echo: false
#| label: dummy-slope

res <- data.frame(
  values = c(3,6,4,7, 5,3,4,6,3), 
  phase = c(rep("A",4),rep("B",5)), 
  mt = 1:9,
  "level B" = c(0,0,0,0,1,1,1,1,1), 
  "model B&L-M" = c(0,0,0,0,1,2,3,4,5), 
  "model H-M" = c(0,0,0,0,0,1,2,3,4), 
  check.names = FALSE )
kable(res, align = "c") %>%
  kable_classic(full_width = FALSE) %>%
  add_header_above(c(" " = 4, "slope B" = 2))
```

Applying the *H-M* model will give you a "pure" level-effect while the *B&L-B* model will provide an estimation of the level-effect that is actually the level-effect plus on times the slope-effect (as the the model assumes that the slope variable is *1* at the first measurement of the B-phase). For most studies, the *H-M* model is more appropriate.

Still, we have to be aware of another aspect. Usually, measurement-times in single-case designs are coded as starting with *1* and increasing in integers (e.g., 1, 2, 3, ...). At the same time, the estimation of the trend-effect is based on the measurement-time variable. In that case, the estimation of the model intercept (usually interpreted as the value at the start of the study) actually depicts the estimation of the start value plus one times the trend-effect. Therefore, I implemented the *W* model (since scan version `0.54.4`). Here, the trend-effect is estimated for a measurement-time variable that starts with *0*. As a result the intercept will then represent the estimated value at the first measurement fo the study. The *W* model handles the slope estimation the same way as the *H-M* model. Since scan version `0.54.4` the *W* model is the default.

```{r}
#| echo: false
#| label: dummy-slope-mt

res <- data.frame(
  values = c(3,6,4,7, 5,3,4,6,3), 
  phase = c(rep("A",4),rep("B",5)), 
  level = c(0,0,0,0,1,1,1,1,1),
  "B&L-M and H-M" = 1:9,
  "W" = 0:8,
  "B&L-M" = c(0,0,0,0,1,2,3,4,5), 
  "H-M and W" = c(0,0,0,0,0,1,2,3,4), 
  check.names = FALSE )
kable(res, align = "c") %>%
  kable_classic(full_width = FALSE) %>%
  add_header_above(c(" " = 3, "mt" = 2, "slope" = 2))
```

## Designs with more than two phases: Setting the right contrasts

With single-case studies with more than two phases it gets a bit more complicated. Applying the models described above to three phases would result in a comparison between each phase and the first phase (usually phase A). That is, the regression weights and significance tests indicate the differences between each phase and the phase A values. Another common use is to compare the effects of one phase with the preceding phase.\
Since scan version `0.54.4` plm allows to set a contrast argument. `contrast = "first"`\` (the default) will compare all slope and level-effects to the values in the first phase. `contrast = "preceding"`\` will compare the slope and level-effects to the preceding phase. (Note: Prior to scan version `0.54.4` you had to set `model = "JW"` which is identical to `model = "B&L-B", contrast = "preceding"`\`).

For the *preceding contrast*, the dummy variable for the level-effect is set to zero for all phases preceding the phase in focus and set to one for all remaining measurements. Similar, the dummy variable for the slope-effect is set to zero for all phases preceding the one in focus and starts with one for the first measurement of the target phase and increases until the last measurement of the case.

```{r}
#| echo: false
#| label: dummy-contrast

res <- data.frame(
  values =  c(3,6,4,7, 5,3,4,6,3, 7,5,6,4,8), 
  phase = c(rep("A", 4), rep("B", 5), rep("C", 5)), 
  mt = 1:14,
  B = c(0,0,0,0, 1,1,1,1,1, 0,0,0,0,0), 
  C = c(0,0,0,0, 0,0,0,0,0, 1,1,1,1,1), 
  B = c(0,0,0,0, 1,2,3,4,5, 0,0,0,0,0), 
  C = c(0,0,0,0, 0,0,0,0,0, 1,2,3,4,5),
  
  B = c(0,0,0,0, 1,1,1,1,1, 1,1,1,1,1), 
  C = c(0,0,0,0, 0,0,0,0,0, 1,1,1,1,1), 
  B = c(0,0,0,0, 1,2,3,4,5, 6,7,8,9,10), 
  C = c(0,0,0,0, 0,0,0,0,0, 1,2,3,4,5),
  check.names = FALSE)
kable(res, align = "c") %>%
  kable_classic(full_width = FALSE) %>%
  add_header_above(c(" " = 3, "level" = 2, "slope" = 2,"level" = 2, "slope" = 2)) %>%
  add_header_above(c(" " = 3, "contrast\nfirst" = 4, "contrast\npreceeding" = 4))
```

### Understanding and interpreting contrasts

In this section, we will calculate four plm models with different contrast settings for the same single-case data.

The example scdf is the case 'Marie' from the *exampleABC* scdf (`exampleABC$Marie`)

```{r}
#| echo: false
#| label: fig-marie
#| fig-cap: Example dataset

scplot(exampleABC$Marie) %>% 
  add_statline("trend", color = "darkred")
  
```

The darkred lines indicate the intercept and slopes when calculated separately for each phase. They are:

```{r}
#| echo: false
#| label: tbl-contrasts-ex-2
#| tbl-cap: Intercept, slope, and number of measurements calculated separately for each phase
res <- trend(exampleABC$Marie)$trend[2:4, 1:2] %>% round(3)
res$n <- 10
row.names(res) <- paste0("phase ", c("A","B", "C"))
colnames(res) <- c("intercept", "slope", "n")
kable(res, align = "c") %>%
  kable_classic(full_width = TRUE)
```

Now we estimate a plm model with four contrast settings:

```{r}
#| echo: false
#| label: tbl-contrast-first-preceding
#| tbl-cap: Estimates of a piecewise-linear regression with contrast models "first" and "preceding".
df <- rbind(
  coef(plm(exampleABC$Marie, contrast = list("first", "first")))[,"Estimate"],
  coef(plm(exampleABC$Marie, contrast = list("preceding", "preceding")))[,"Estimate"],
  coef(plm(exampleABC$Marie, contrast = list("first", "preceding")))[,"Estimate"],
  coef(plm(exampleABC$Marie, contrast = list("preceding", "first")))[,"Estimate"]
) %>% 
  round(3) %>% as.data.frame()
names(df) <- c("intercept", "trend", "level B", "level C", "slope B", "slope C")
data.frame(
  "Contrast level" = c("first", "preceding", "first", "preceding"), 
  "Contrast slope" = c("first", "preceding", "preceding", "first"),
  df, 
  check.names = FALSE) %>%
  kable(align = "c") %>%
  kable_classic(full_width = FALSE)
```

#### Phase B estimates

All regression models in @tbl-contrast-first-preceding have the same estimates for `intercept` and `trend`. These are not affected by the contrasts and are identical to those for phase A in @tbl-contrasts-ex-2. In addition, in @tbl-contrast-first-preceding, the estimates for `levelB` and `slopeC` are identical since all models contrast the same phase (the first and the preceding phase are both phase A). The values here can be calculated from @tbl-contrasts-ex-2[^ch_piecewise_regression-1]:

[^ch_piecewise_regression-1]: Differences here and in the following calculations are due to rounding errors.

<!--
$$
intercept_{phaseB} = intercept + n_{PhaseA} * trend + levelB
$$ {#eq-contrast-level}

$$
74.855 \approx 60.618 + 10*-1.915 + 33.388
$$
-->

$$
levelB = intercept_{phaseB} - intercept_{phaseA} - (n_{PhaseA} * slope_{phaseA})
$$ {#eq-contrast-level-2}

$$
33.388 \approx  74.855 - 60.618 - 10*-1.915
$$

The `levelB`-effect is the difference between the estimated first measurement of phase B and the last measurement of phase A.

<!--
$$
slope_{phaseB} = slope_{phaseA} + slopeB
$$ {#eq-contrast-slope}

$$
-0.612 \approx -1.915 + 1.303
$$
-->

$$
slopeB = slope_{phaseB} - slope_{phaseA}
$$ {#eq-contrast-slope-2}

$$
1.303 \approx -1.915 - (-0.612) 
$$

The `slopeB`-effect is the difference between the slopes of phase B and phase A.

#### Level first and slope first contrasts

`levelC` and `slopeC` estimates are different for "first" and "preceding" models. The estimates from @tbl-contrasts-ex-2 are $intercept_{phaseC} = 68.873$ and $slope_{phaseC} = -0.194$.

For *contrast level first/ slope first*, `levelC` indicates an increase of 46.558 compared to phase A (the intercept) ans `slopeC` an increase of 1.721.

<!--
$$
intercept_{phaseC} = intercept + n_{phaseA+B} * trend + levelC
$$ {#eq-contrast-ff-1}

$$68.873 \approx 60.618 + 20*-1.915 + 46.558$$
-->

$$
levelC = intercept_{phaseC} - intercept - (n_{phaseA+B} * trend)
$$ {#eq-contrast-ff-1}

$$46.558 \approx 68.873 - 60.618 - (20*-1.915) $$



<!--
$$
slope_{phaseC} \approx trend + slopeC
$$ {#eq-contrast-ff-2}

$$-0.194 \approx -1.915 + 1.721$$
-->

$$
slopeC = slope_{phaseC} - slope_{phaseA} 
$$ {#eq-contrast-ff-2}

$$1.721 \approx -0.194 - (-1.915)$$



#### Level preceding and slope preceding contrasts

For *contrast level preceding/ slope preceding*, `levelC` indicates an increase of 0.139 compared to phase B and `slopeC` an increase of 0.418.

<!--
$$
intercept_{phaseC} = intercept_{phaseB} + n_{phaseB} * slope_{phaseB} + levelC 
$$ {#eq-contrast-pp-1}

$$68.873 \approx 74.855 + 10*-0.612 + 0.139$$

$$
slope_{phaseC} = slope_{phaseB} + slopeC
$$ {#eq-contrast-pp-2}

$$-0.194 \approx -0.612 + 0.418$$
-->

$$
levelC = intercept_{phaseC} - intercept_{phaseB} - (n_{phaseB} * slope_{phaseB})
$$ {#eq-contrast-pp-1}

$$0.139 \approx 68.873 - 74.855 - (10*-0.612)$$

$$
slopeC = slope_{phaseC} - slope_{phaseB}
$$ {#eq-contrast-pp-2}

$$0.418 \approx -0.194 - (-0.612)$$

#### Level first and slope preceding contrasts

For *contrast level first/ slope preceding*, `levelC` indicates an increase of 33.388 compared to phase B and `slopeC` an increase of 0.418.

$$
intercept_{phaseC} = intercept +n_{phaseA+B} * trend + 10 * slopeB + levelC
$$ {#eq-contrast-fp-1}

$$
68.873 \approx 60.618 + 20 * -1.915 + 10 * 1.303 + 33.527
$$

$$
slope_{phaseC} = slope_{phaseB} + slopeC
$$ {#eq-contrast-fp-2}

$$-0.194 \approx -0.612 + 0.418$$

#### Level preceding and slope first contrasts

For *contrast level preceding/ slope first*, `level C` indicates an increase of 13.170 compared to phase B and `slope C` an increase of 1.721.

$$
intercept_{phaseC} = intercept_{phaseB} + n_{phaseB} * trend + levelC
$$ {#eq-contrast-pf-1}

$$
68.873 \approx 74.855 + 10*-1.915 + 13.170
$$

$$
slope_{phaseC} = trend + slopeC
$$ {#eq-contrast-pf-2}

$$
-0.194 \approx -1.915 + 1.721
$$

# Modelling autoregression

<!--# to be written -->

```{r}
autocorr(Grosche2011)
```

# Multivariate piecewise regression {#sec-mplm}

<!--# to be written -->

::: callout-note
Read @sec-plm before you start with this chapter.
:::

```{r}
#| echo: false
#| results: asis

function_structure("mplm")
```

```{r}
mplm(exampleAB_add, dvar = c("wellbeing", "depression"))
```

# Multilevel plm analyses {#sec-hplm}

::: callout-note
Read @sec-plm before you start with this chapter.
:::

```{r}
#| echo: false
#| results: asis
function_structure("hplm")
```

Multilevel analyses can take the piecewise-regression approach even further. It allows for

-   analyzing the effects between phases for multiple single-cases at once
-   describing variability between subjects regarding these effects, and
-   introducing variables and factors for explaining the differences.

The basic function for applying a multilevel piecewise regression analysis is `hplm`. The `hplm` function is similar to the `plm` function, so I recommend that you get familar with `plm` before applying an `hplm`.

Here is a simple example:

```{r}
#| label: hplm
hplm(exampleAB_50)
```

Here is an example inlcuding random slopes:

```{r}
#| label: hplm-random
hplm(exampleAB_50, random.slopes = TRUE)
```

### Adding additional L2-variables {#sec-add-l2}

```{r}
#| echo: false
#| results: asis
function_structure("add_l2")
```

In some analyses researchers want to investigate whether attributes of the individuals contribute to the effectiveness of an intervention. For example might an intervention on mathematical abilities be less effective for student with a migration background due to too much language related material within the training. Such analyses can also be conducted with *scan*. Therefore, we need to define a new *data frame* including the relevant information of the subjects of the single-case studies we want to analyze. This *data frame* consists of a variable labeled `case` which has to correspond to the case names of the *scfd* and further variables with attributes of the subjects. To build a *data frame* we can use the R function `data.frame`.

```{r}
#| label: hplm-df-l2
L2 <- data.frame(
  case = c("Antonia","Theresa", "Charlotte", "Luis", "Bennett", "Marie"), 
  age = c(16, 13, 13, 10, 5, 14), 
  sex = c("f","f","f","m","m","f")
)
L2
```

Multilevel analyses require a high number of Level 2 units. The exact number depends on the complexity of the analyses, the size of the effects, the number of level 1 units, and the variability of the residuals. But surely we need at least about 30 level 2 units. In a single-case design that is, we need at least 30 single-cases (subjects) within the study. After setting the level 2 data frame we can merge it to the *scdf* with the `add_l2()` function (alternatively, we can use the `data.l2` argument of the `hplm` function). Then we have to specify the regression function using the `update.fixed` argument. The level 2 variables can be added just like any other additional variable. For example, we have added a level 2 data-set with the two variables `sex` and `age`. `update` could be construed of the level 1 piecewise regression model `.~.` plus the additional level 2 variables of interest `+ sex + age`. The complete argument is `update.fixed = .~. + sex + age`. This analyses will estimate a main effect of sex and age on the overall performance. In case we want to analyze an interaction between the intervention effects and for example the sex of the subject we have to add an additional interaction term (a cross-level interaction). An interaction is defined with a colon. So `sex:phase` indicates an interaction of sex and the level effect in the single case study. The complete formula now is `update.fixed = .~. + sex + age + sex:phase`.

*scan* includes an example single-case study with 50 subjects `example50` and an additional level 2 data-set `example50.l2`. Here are the first 10 cases of `example50.l2`.

```{r}
#| echo: false
knitr::kable(head(exampleAB_50.l2, 10))
```

Analyzing the data with `hplm` could look like this:

```{r}
#| label: hplm-l2
exampleAB_50 %>%
  add_l2(exampleAB_50.l2) %>%
  hplm(update.fixed = .~. + sex + age)

# Alternatively:
# hplm(exampleAB_50, data.l2 = exampleAB_50.l2, update.fixed = .~. + sex + age)
```

`sex` is a factor with the levels `f` and `m`. So `sexm` is the effect of being male on the overall performance. `age` does not seem to have any effect. So we drop `age` out of the equation and add an interaction of sex and phase to see whether the `sex` effect is due to a weaker impact of the intervention on males.

```{r}
exampleAB_50 %>%
  add_l2(exampleAB_50.l2) %>%
  hplm(update.fixed = .~. + sex + sex:phaseB)
```

```{r}
#| include: false
res <- hplm(exampleAB_50, data.l2 = exampleAB_50.l2, update.fixed = .~. + sex + sex:phaseB)
res <- res$hplm$coefficients$fixed
```

Now the interaction `phase:sexm` is significant and the main effect is no longer relevant. It looks like the intervention effect is $`r abs(round(res["phaseB:sexm"], 1))`$ points lower for male subjects. While the level-effect is $`r round(res["phaseB"], 1)`$ points for female subjects it is $`r round(res["phaseB"], 1)`$ - $`r abs(round(res["phaseB:sexm"], 1))`$ = $`r round(res["phaseB"], 1) - abs(round(res["phaseB:sexm"], 1))`$ for males.
