# Multilevel plm analyses {#sec-hplm}

::: callout-note
This chapter describes the multilevel data analyses with a frequentist approach. *scan* also provides functions for Bayesian analyses that are described in chapter @sec-bplm

Read @sec-plm before you start with this chapter.
:::

```{r}
#| echo: false
#| results: asis
function_structure("hplm")
```

Multilevel analyses can take the piecewise-regression approach even further. It allows for

-   analyzing the effects between phases for multiple single-cases at once
-   describing variability between subjects regarding these effects, and
-   introducing variables and factors for explaining the differences.

The basic function for applying a multilevel piecewise regression analysis is `hplm` (*hierarchical piecewise linear model*). The `hplm` function is similar to the `plm` function, so I recommend that you get familar with `plm` before applying an `hplm`.

Here is a simple example:

```{r}
#| include: false

model <- hplm(exampleAB_50)
```

```{r}
#| label: hplm-ex-1
hplm(exampleAB_50)
```

The fixed effects describe the overall effect of the respective predictors on the dependent variable, while the random effects describe the variability of the respective predictor between cases. Here, the fixed effect of the intercept is `r round(coef(model) [1, 1], 3)` which estimates the mean value across all cases at the first measurement. The standard deviation of this average between cases is `r nlme::VarCorr(model$hplm)[1,1] |> as.numeric() |> sqrt() |> round(3)`.\
In this analysis, we have a basic random intercept model. That is, only the intercept is assumed to vary between cases, whereas the other predictors (`Trend`, `Level`, and `Slope`) are assumed to be identical for all cases.

### Explained variance by the variability between cases

The intraclass correlation (ICC) represents the proportion of total variance in the model that is explained by differences between cases.

It is calculated by comparing the model fit (likelihood) of two models using a likelihood ratio test. The first model includes both a fixed and a random intercept as predictors of the measurement, while the second model includes only a fixed intercept without a random intercept.

The model above reports `ICC = 0.287; L = 339.0; p = 0.000`. This means that approximately 29% of the total variance in the dependent variable is explained by differences between cases, and this variance is statistically significantly greater than 0%.

### Variation of single predictors between cases

A multilevel model that includes the variation of specific predictors between cases (e.g., the trend effect) is - in the multilevel regression "world" - called a random slope model.

Here is an example inlcuding random slopes by setting the respective function argument for the trend, slope, and level effects to `TRUE`:

```{r}
#| label: hplm-ex-random
hplm(
  exampleAB_50, 
  random_trend = TRUE, 
  random_level = TRUE, 
  random_slope = TRUE
)
```

The random part of this output now provides estimations of the between case variance of all predictors.

Additionally, we get a correlation matrix of the random slope variables. This depicts the interaction effect of the respective variables at the between case level on the dependent variable. For example, the model above shows a correlation of intercept and trend of *r* = 0.23. That is, the higher the intercept of a case (the starting value), the stronger the trend effect (here a medium effect size).

::: callout-caution
Do not be confused by the terminology: In multilevel regression models, a random slope model refers to the variability of a predictor in a regression model. These predictors contribute to the *slope* of a linear regression line.

This terminology is used in general within multilevel models, regardless of whether they are applied to single-case studies or not.

In single-case piecewise regressions, the *slope* effect refers to the difference in the slopes of the regression lines between two phases.

That is, unfortunately, we have a random slope effect of the slope effect!
:::

### Testing the random slope variance

It is possible to test the statistical significance of the random slope variances by comparing the overall fit of the model (likelihood) with the fit of the same model without the respective random slope parameter using a likelihood ratio test. To do this, set the argument `lr.test = TRUE`.

```{r}
#| label: hplm-ex-lr-test

hplm(
  exampleAB_50, 
  random_trend = TRUE, random_level = TRUE, random_slope = TRUE, 
  lr.test = TRUE
)

```

Here, we can see that the random intercept and the random slope effect for *Level phase B* are significant. This means that cases have significantly different starting values at the first measurement, and the level intervention effect is also not the same for all cases.

### Adding additional L2-variables {#sec-add-l2}

```{r}
#| echo: false
#| results: asis
function_structure("add_l2")
```

In some analyses researchers want to investigate whether attributes of the individuals contribute to the effectiveness of an intervention. For example might an intervention on mathematical abilities be less effective for student with a migration background due to too much language related material within the training. Such analyses can also be conducted with *scan*. Therefore, we need to define a new *data frame* including the relevant information of the subjects of the single-case studies we want to analyze. This *data frame* consists of a variable labeled `case` which has to correspond to the case names of the *scfd* and further variables with attributes of the subjects. To build a *data frame* we can use the R function `data.frame`.

```{r}
#| label: hplm-df-l2
L2 <- data.frame(
  case = c("Antonia","Theresa", "Charlotte", "Luis", "Bennett", "Marie"), 
  age = c(16, 13, 13, 10, 5, 14), 
  sex = c("f","f","f","m","m","f")
)
L2
```

Multilevel analyses require a high number of Level 2 units. The exact number depends on the complexity of the analyses, the size of the effects, the number of level 1 units, and the variability of the residuals. But surely we need at least about 30 level 2 units. In a single-case design that is, we need at least 30 single-cases (subjects) within the study. After setting the level 2 data frame we can merge it to the *scdf* with the `add_l2()` function (alternatively, we can use the `data.l2` argument of the `hplm` function). Then we have to specify the regression function using the `update.fixed` argument. The level 2 variables can be added just like any other additional variable. For example, we have added a level 2 data-set with the two variables `sex` and `age`. `update` could be construed of the level 1 piecewise regression model `.~.` plus the additional level 2 variables of interest `+ sex + age`. The complete argument is `update.fixed = .~. + sex + age`. This analyses will estimate a main effect of sex and age on the overall performance. In case we want to analyze an interaction between the intervention effects and for example the sex of the subject we have to add an additional interaction term (a cross-level interaction). An interaction is defined with a colon. So `sex:phase` indicates an interaction of sex and the level effect in the single case study. The complete formula now is `update.fixed = .~. + sex + age + sex:phase`.

*scan* includes an example single-case study with 50 subjects `example50` and an additional level 2 data-set `example50.l2`. Here are the first 10 cases of `example50.l2`.

```{r}
#| echo: false
knitr::kable(head(exampleAB_50.l2, 10))
```

Analyzing the data with `hplm` could look like this:

```{r}
#| label: hplm-l2
exampleAB_50 %>%
  add_l2(exampleAB_50.l2) %>%
  hplm(update.fixed = .~. + sex + age)

# Alternatively:
# hplm(exampleAB_50, data.l2 = exampleAB_50.l2, update.fixed = .~. + sex + age)
```

`sex` is a factor with the levels `f` and `m`. So `sexm` is the effect of being male on the overall performance. `age` does not seem to have any effect. So we drop `age` out of the equation and add an interaction of sex and phase to see whether the `sex` effect is due to a weaker impact of the intervention on males.

```{r}
exampleAB_50 %>%
  add_l2(exampleAB_50.l2) %>%
  hplm(update.fixed = .~. + sex + sex:phaseB)
```

```{r}
#| include: false
res <- hplm(exampleAB_50, data.l2 = exampleAB_50.l2, update.fixed = .~. + sex + sex:phaseB)
res <- res$hplm$coefficients$fixed
```

Now the interaction `phase:sexm` is significant and the main effect is no longer relevant. It looks like the intervention effect is $`r abs(round(res["phaseB:sexm"], 1))`$ points lower for male subjects. While the level-effect is $`r round(res["phaseB"], 1)`$ points for female subjects it is $`r round(res["phaseB"], 1)`$ - $`r abs(round(res["phaseB:sexm"], 1))`$ = $`r round(res["phaseB"], 1) - abs(round(res["phaseB:sexm"], 1))`$ for males.

### Estimations for each case

For a multilevel model, you can estimate the values for each parameter for each case based on the random intercept and slope values.

Use the `casewise` argument to access these estimations.

```{r}
#| label: hplm-casewise

res <- hplm(exampleAB_50[1:10],random.slopes = TRUE)

# retrieve the case estimations for further calculations
cs <- coef(res, casewise = TRUE)

# or print them
print(res, casewise = TRUE)

```

If you have the `scplot` package installed (version 0.4.1 or higher), you can create a forestplot for each parameter of the model with the `scplot()` function. Set the argument "effect" to choose the effect by number or a string ("intercept", "trend", "slope", "level"). The `ci` argument sets the size of the confidence interval (default is `0.95`) and the `mark` argument sets the value for a reference line (default is the mean effect).

```{r}
#| label: hplm-forestplot
library(scplot)
scplot(res, effect = "level")
```
